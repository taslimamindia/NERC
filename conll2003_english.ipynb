{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from keras.utils import to_categorical, pad_sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class define form data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    unique_words = {}\n",
    "    unique_ner_tags = {}\n",
    "    MAX_LENGTH = 200\n",
    "    def __init__(self):\n",
    "        self.sentences = []\n",
    "        self.sentences_num = None\n",
    "        self.ner_tags = []\n",
    "        self.ner_tags_num = None\n",
    "        self.chunk_tags = []\n",
    "        self.pos_tags = []\n",
    "        self.x, self.y = None, None\n",
    "    def word2vec(self, vector_size=100):\n",
    "        word2vec_model = Word2Vec(self.sentences, vector_size=vector_size, window=5, min_count=1, workers=4)\n",
    "        return word2vec_model\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loading():\n",
    "    def __init__(self, data: Data, file):\n",
    "        self.data = data\n",
    "        self.load_sentences(file)\n",
    "        print(\"Loading successfully\")\n",
    "    def load_sentences(self, filepath):\n",
    "        tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                if (line == ('-DOCSTART- -X- -X- O\\n') or line == '\\n'):\n",
    "                    if len(tokens) > 0:\n",
    "                        self.data.sentences.append(tokens)\n",
    "                        self.data.pos_tags.append(pos_tags)\n",
    "                        self.data.chunk_tags.append(chunk_tags)\n",
    "                        self.data.ner_tags.append(ner_tags)\n",
    "                        tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
    "                else:\n",
    "                    l = line.split(' ')\n",
    "                    tokens.append(l[0])\n",
    "                    pos_tags.append(l[1])\n",
    "                    chunk_tags.append(l[2])\n",
    "                    ner_tags.append(l[3].strip('\\n'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    def __init__(self, data:Data, text=None, lang=\"english\"):\n",
    "        self.data = data\n",
    "        self.text = text\n",
    "        self.lang = lang\n",
    "    def tokenize(self):\n",
    "        if self.text != None:\n",
    "            sentenses = [word_tokenize(sentence, language=self.lang) for sentence in sent_tokenize(self.text, language=self.lang)]\n",
    "            self.data.sentences = [[token for token in sentence if token not in stopwords.words(self.lang)] for sentence in sentenses]\n",
    "    def lowercasing(self):\n",
    "        self.data.sentences = [[word.lower() for word in sentence] for sentence in self.data.sentences]\n",
    "    def lemmatize(self):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        self.data.sentences = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in self.data.sentences]\n",
    "        self.unicity() # must be changed\n",
    "    def unicity(self):\n",
    "        temp = set() \n",
    "        [temp.update(word) for word in self.data.sentences]\n",
    "        Data.unique_words = dict(zip(list(temp), range(1, len(temp) + 1)))\n",
    "        temp = set() \n",
    "        [temp.update(word) for word in self.data.ner_tags]\n",
    "        Data.unique_ner_tags = dict(zip(list(temp), range(len(temp))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorization():\n",
    "    def __init__(self, data:Data):\n",
    "        self.data = data\n",
    "        # self.data.max_length = max([len(sentence) for sentence in data.sentences])\n",
    "        data.sentences_num = [[Data.unique_words[word] for word in sentence] for sentence in data.sentences]\n",
    "        data.ner_tags_num = [[Data.unique_ner_tags[tag] for tag in tags] for tags in data.ner_tags] \n",
    "    def padding_x(self):\n",
    "        if len(self.data.sentences_num) > 0:\n",
    "            self.data.sentences_num = self.word2vec()\n",
    "            self.data.x = pad_sequences(\n",
    "                sequences=self.data.sentences_num, \n",
    "                maxlen=self.data.MAX_LENGTH, \n",
    "                dtype=\"float32\", \n",
    "                padding=\"post\", \n",
    "                value=0\n",
    "            )\n",
    "    def padding_y(self):\n",
    "        if len(self.data.ner_tags_num) > 0:\n",
    "            self.data.y = pad_sequences(\n",
    "                sequences=self.data.ner_tags_num, \n",
    "                maxlen=self.data.MAX_LENGTH, \n",
    "                dtype=\"float32\", \n",
    "                padding=\"post\", \n",
    "                value=self.data.unique_ner_tags.get(\"O\")\n",
    "            )\n",
    "    def word2vec(self, min_count=1, vector_size=100, window=5):\n",
    "        word2vec_model = Word2Vec(self.data.sentences, min_count=min_count, vector_size=vector_size, window=window)\n",
    "        vectors= [[word2vec_model.wv[word] for word in sentence] for sentence in self.data.sentences]\n",
    "        return vectors\n",
    "    def vectorized_x(self):\n",
    "        self.padding_x() \n",
    "        self.data.x = np.array(self.data.x, dtype=\"float32\")\n",
    "    def vectorized_y(self):\n",
    "        self.padding_y()\n",
    "        self.data.y = [[to_categorical(tag, num_classes=len(Data.unique_ner_tags)) for tag in tags] for tags in self.data.y]\n",
    "        self.data.y = np.array(self.data.y, dtype='float32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Data()\n",
    "# test = Data()\n",
    "# valid = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_file = \"conll2003_english/\"\n",
    "Loading(data = train, file=base_file + \"train.txt\")\n",
    "# Loading(data = valid, file=base_file + \"valid.txt\")\n",
    "print(train.sentences[0])\n",
    "print(train.ner_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Preprocessing(data=train)\n",
    "preprocessing.lowercasing()\n",
    "preprocessing.lemmatize()\n",
    "\n",
    "print(train.sentences[0])\n",
    "print(train.ner_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = Vectorization(train)\n",
    "vector.vectorized_x()\n",
    "vector.vectorized_y()\n",
    "\n",
    "print(\"X_train\", train.x.shape)\n",
    "print(\"y_train\", train.y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Data()\n",
    "\n",
    "Loading(data = test, file=base_file + \"test.txt\")\n",
    "\n",
    "preprocessing = Preprocessing(data=test)\n",
    "preprocessing.lowercasing()\n",
    "preprocessing.lemmatize()\n",
    "\n",
    "vector = Vectorization(test)\n",
    "vector.vectorized_x()\n",
    "vector.vectorized_y()\n",
    "\n",
    "print(test.x.shape, test.y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = Data()\n",
    "\n",
    "preprocessing = Preprocessing(data = test_text, text = \"Obama is the president of the United States. I am from Guinea, nice to meet you.\")\n",
    "preprocessing.tokenize()\n",
    "preprocessing.lowercasing()\n",
    "preprocessing.lemmatize()\n",
    "print(test_text.sentences)\n",
    "\n",
    "vector = Vectorization(test_text)\n",
    "vector.vectorized_x()\n",
    "print(test_text.x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 9\n",
    "max_length = train.MAX_LENGTH\n",
    "embedding_dim = 100\n",
    "input_dim = len(train.sentences)\n",
    "print(num_classes, max_length, embedding_dim, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(Data.MAX_LENGTH, embedding_dim), return_sequences=True))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train.x, train.y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "loss, accuracy = model.evaluate(test.x, test.y, batch_size=32)\n",
    "\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras_contrib.layers import CRF\n",
    "\n",
    "# word2vec_model = train.word2vec()\n",
    "\n",
    "# inputs = tf.keras.layers.Input(shape=(max_length, embedding_dim), dtype=tf.float32, name='sequence_input')\n",
    "# conv1D = tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)\n",
    "# maxPooling1D = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1D)\n",
    "# outputs = tf.keras.layers.Dense(num_classes, activation='relu')(maxPooling1D)\n",
    "# base = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# crf_layer = CRF(num_classes, sparse_target=False)\n",
    "# model = crf_layer(base)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#   tf.keras.layers.Input(shape=(max_length, 100), dtype=tf.float32, name='sequence_input'),\n",
    "#   tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "#   tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "#   tf.keras.layers.Dense(num_classes, activation='relu'),\n",
    "#   tf.keras.layers.Flatten(),\n",
    "#   tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(loss=crf_layer.loss_function, optimizer='adam', metrics=[crf_layer.accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trainning\n",
    "# batch_size = 56\n",
    "# num_epochs = 5\n",
    "\n",
    "# model.fit(train.x, train.y, epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "# # # Evaluation\n",
    "# # loss, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "# # print('Test Loss:', loss)\n",
    "# # print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential()\n",
    "# model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(113, 100)))\n",
    "# model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "# model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "# model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "# model.add(tf.keras.layers.Flatten())\n",
    "# model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(units=9, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(train.x, train.y, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Compile the model\n",
    "# # model.compile(optimizer='adam', loss=tfa.losses.SigmoidFocalCrossEntropy(), metrics=[tfa.metrics.F1Score(num_classes=9, threshold=0.5, dtype='float32')])\n",
    "# import tensorflow_addons as tfa\n",
    "# # no need to specify a loss for CRFModel, model will compute crf loss by itself\n",
    "# # model.compile(optimizer=tf.keras.optimizers.Adam(3e-4), metrics=['acc'])\n",
    "# model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(3e-4),\n",
    "#     loss=\"categorical_crossentropy\",\n",
    "#     metrics=[tfa.metrics.MultiLabelConfusionMatrix(num_classes=num_classes)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # vocab_size = len(word2vec_model.wv)\n",
    "        # embedding_dim = 100\n",
    "        # embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "        # for i, vec in enumerate(word2vec_model.wv):\n",
    "        #     embedding_matrix[i] = vec\n",
    "        # embedding_layer = Embedding(\n",
    "        #     input_dim=vocab_size,\n",
    "        #     output_dim=embedding_dim,\n",
    "        #     weights=[embedding_matrix],\n",
    "        #     trainable=False)\n",
    "        # return embedding_layer\n",
    "\n",
    "\n",
    "# class DataSet():\n",
    "#     def __init__(self):\n",
    "#         self.labels = {\"ner_tags\": set(), \"pos_tags\": set(), \"chunk_tags\": set()}\n",
    "#         self.word2vec_model = None\n",
    "#     def unique_values(self, index = \"ner_tags\"):\n",
    "#         if self.labels[index].__len__() > 0:\n",
    "#             return dict(zip(self.labels[index], range(0, len(self.labels[index]))))\n",
    "#         raise KeyError(\"Key does not exist !!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
