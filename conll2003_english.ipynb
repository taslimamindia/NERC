{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "from keras.utils import to_categorical, pad_sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class define form data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    unique_words = {}\n",
    "    unique_ner_tags = {}\n",
    "    MAX_LENGTH = 200\n",
    "    def __init__(self):\n",
    "        self.sentences = []\n",
    "        self.sentences_num = None\n",
    "        self.ner_tags = []\n",
    "        self.ner_tags_num = None\n",
    "        self.chunk_tags = []\n",
    "        self.pos_tags = []\n",
    "        self.x, self.y = None, None\n",
    "    def word2vec(self, vector_size=100):\n",
    "        word2vec_model = Word2Vec(self.sentences, vector_size=vector_size, window=5, min_count=1, workers=4)\n",
    "        return word2vec_model   \n",
    "    def word2idx(self, word):\n",
    "        pass\n",
    "    def idx2word(self, index):\n",
    "        pass\n",
    "    def tag2idx(self, tag):\n",
    "        pass\n",
    "    def idx2tag(self, index):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loading():\n",
    "    def __init__(self, data: Data, file):\n",
    "        self.data = data\n",
    "        self.load_sentences(file)\n",
    "        print(\"Loading successfully\")\n",
    "    def load_sentences(self, filepath):\n",
    "        tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                if (line == ('-DOCSTART- -X- -X- O\\n') or line == '\\n'):\n",
    "                    if len(tokens) > 0:\n",
    "                        self.data.sentences.append(tokens)\n",
    "                        self.data.pos_tags.append(pos_tags)\n",
    "                        self.data.chunk_tags.append(chunk_tags)\n",
    "                        self.data.ner_tags.append(ner_tags)\n",
    "                        tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
    "                else:\n",
    "                    l = line.split(' ')\n",
    "                    tokens.append(l[0])\n",
    "                    pos_tags.append(l[1])\n",
    "                    chunk_tags.append(l[2])\n",
    "                    ner_tags.append(l[3].strip('\\n'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    def __init__(self, data:Data, text=None, lang=\"english\"):\n",
    "        self.data = data\n",
    "        self.text = text\n",
    "        self.lang = lang\n",
    "    def tokenize(self):\n",
    "        if self.text != None:\n",
    "            sentenses = [word_tokenize(sentence, language=self.lang) for sentence in sent_tokenize(self.text, language=self.lang)]\n",
    "            self.data.sentences = [[token for token in sentence if token not in stopwords.words(self.lang)] for sentence in sentenses]\n",
    "    def lowercasing(self):\n",
    "        self.data.sentences = [[word.lower() for word in sentence] for sentence in self.data.sentences]\n",
    "    def lemmatize(self):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        self.data.sentences = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in self.data.sentences]\n",
    "        self.unicity() # must be changed\n",
    "    def unicity(self):\n",
    "        temp = set() \n",
    "        [temp.update(word) for word in self.data.sentences]\n",
    "        Data.unique_words = dict(zip(list(temp), range(1, len(temp) + 1)))\n",
    "        temp = set() \n",
    "        [temp.update(word) for word in self.data.ner_tags]\n",
    "        Data.unique_ner_tags = dict(zip(list(temp), range(len(temp))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorization():\n",
    "    def __init__(self, data:Data):\n",
    "        self.data = data\n",
    "        # self.data.max_length = max([len(sentence) for sentence in data.sentences])\n",
    "        data.sentences_num = [[Data.unique_words[word] for word in sentence] for sentence in data.sentences]\n",
    "        data.ner_tags_num = [[Data.unique_ner_tags[tag] for tag in tags] for tags in data.ner_tags] \n",
    "    def padding_x(self):\n",
    "        if len(self.data.sentences_num) > 0:\n",
    "            self.data.sentences_num = self.word2vec()\n",
    "            self.data.x = pad_sequences(\n",
    "                sequences=self.data.sentences_num, \n",
    "                maxlen=self.data.MAX_LENGTH, \n",
    "                dtype=\"float32\", \n",
    "                padding=\"post\", \n",
    "                value=0\n",
    "            )\n",
    "    def padding_y(self):\n",
    "        if len(self.data.ner_tags_num) > 0:\n",
    "            self.data.y = pad_sequences(\n",
    "                sequences=self.data.ner_tags_num, \n",
    "                maxlen=self.data.MAX_LENGTH, \n",
    "                dtype=\"float32\", \n",
    "                padding=\"post\", \n",
    "                value=self.data.unique_ner_tags.get(\"O\")\n",
    "            )\n",
    "    def word2vec(self, min_count=1, vector_size=100, window=5):\n",
    "        word2vec_model = Word2Vec(self.data.sentences, min_count=min_count, vector_size=vector_size, window=window)\n",
    "        vectors= [[word2vec_model.wv[word] for word in sentence] for sentence in self.data.sentences]\n",
    "        return vectors\n",
    "    def vectorized_x(self):\n",
    "        self.padding_x() \n",
    "        self.data.x = np.array(self.data.x, dtype=\"float32\")\n",
    "    def vectorized_y(self):\n",
    "        self.padding_y()\n",
    "        self.data.y = [[to_categorical(tag, num_classes=len(Data.unique_ner_tags)) for tag in tags] for tags in self.data.y]\n",
    "        self.data.y = np.array(self.data.y, dtype='float32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining for CONLL2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pretraining_CoNLL3(path: str):\n",
    "    data = Data()\n",
    "    base_file = \"conll2003_english/\"\n",
    "    Loading(data = data, file=base_file + path)\n",
    "    preprocessing = Preprocessing(data=data)\n",
    "    preprocessing.lowercasing()\n",
    "    preprocessing.lemmatize()\n",
    "    vector = Vectorization(data=data)\n",
    "    vector.vectorized_x()\n",
    "    vector.vectorized_y()\n",
    "    return data\n",
    "    \n",
    "\n",
    "    # Loading(data = test, file=base_file + \"test.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading successfully\n",
      "['eu', 'reject', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "train = pretraining_CoNLL3(\"train.txt\")\n",
    "print(train.sentences[0])\n",
    "print(train.ner_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eu', 'reject', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(train.sentences[0])\n",
    "print(train.ner_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (14041, 200, 100)\n",
      "y_train (14041, 200, 9)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train\", train.x.shape)\n",
    "print(\"y_train\", train.y.shape)\n",
    "print(type(train.x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading successfully\n",
      "X_test (3453, 200, 100)\n",
      "y_test (3453, 200, 9)\n"
     ]
    }
   ],
   "source": [
    "test = pretraining_CoNLL3(\"test.txt\")\n",
    "print(\"X_test\", test.x.shape)\n",
    "print(\"y_test\", test.y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ValidSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading successfully\n",
      "X_valid (3250, 200, 100)\n",
      "y_valid (3250, 200, 9)\n"
     ]
    }
   ],
   "source": [
    "valid = pretraining_CoNLL3(\"valid.txt\")\n",
    "print(\"X_valid\", valid.x.shape)\n",
    "print(\"y_valid\", valid.y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = Data()\n",
    "\n",
    "preprocessing = Preprocessing(data = test_text, text = \"Obama is the president of the United States. I am from Guinea, nice to meet you.\")\n",
    "preprocessing.tokenize()\n",
    "preprocessing.lowercasing()\n",
    "preprocessing.lemmatize()\n",
    "print(test_text.sentences)\n",
    "\n",
    "vector = Vectorization(test_text)\n",
    "vector.vectorized_x()\n",
    "print(test_text.x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 9 8419 100\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS = len(Data.unique_words)\n",
    "NUM_CLASSES = len(Data.unique_ner_tags)\n",
    "MAX_LENGTH = Data.MAX_LENGTH\n",
    "OUTPUT_DIM = 100\n",
    "print(MAX_LENGTH, NUM_CLASSES, NUM_WORDS, OUTPUT_DIM)\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 100\n",
    "NUM_FILTERS = 256\n",
    "KERNEL_SIZE = 3\n",
    "HIDDEN_DIM = 200\n",
    "DROPOUT_RATE = 0.5\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "439/439 [==============================] - 20s 42ms/step - loss: 0.2733 - accuracy: 0.9851 - val_loss: 0.0704 - val_accuracy: 0.9863\n",
      "Epoch 2/10\n",
      "439/439 [==============================] - 18s 42ms/step - loss: 0.0466 - accuracy: 0.9886 - val_loss: 0.0673 - val_accuracy: 0.9862\n",
      "Epoch 3/10\n",
      "439/439 [==============================] - 18s 40ms/step - loss: 0.0440 - accuracy: 0.9889 - val_loss: 0.0655 - val_accuracy: 0.9864\n",
      "Epoch 4/10\n",
      "439/439 [==============================] - 17s 38ms/step - loss: 0.0426 - accuracy: 0.9890 - val_loss: 0.0630 - val_accuracy: 0.9867\n",
      "Epoch 5/10\n",
      "439/439 [==============================] - 17s 38ms/step - loss: 0.0414 - accuracy: 0.9892 - val_loss: 0.0601 - val_accuracy: 0.9867\n",
      "Epoch 6/10\n",
      "439/439 [==============================] - 17s 38ms/step - loss: 0.0406 - accuracy: 0.9893 - val_loss: 0.0601 - val_accuracy: 0.9866\n",
      "Epoch 7/10\n",
      "439/439 [==============================] - 17s 38ms/step - loss: 0.0398 - accuracy: 0.9894 - val_loss: 0.0586 - val_accuracy: 0.9867\n",
      "Epoch 8/10\n",
      "439/439 [==============================] - 17s 38ms/step - loss: 0.0392 - accuracy: 0.9895 - val_loss: 0.0637 - val_accuracy: 0.9828\n",
      "Epoch 9/10\n",
      "439/439 [==============================] - 17s 38ms/step - loss: 0.0387 - accuracy: 0.9895 - val_loss: 0.0620 - val_accuracy: 0.9847\n",
      "Epoch 10/10\n",
      "439/439 [==============================] - 20s 46ms/step - loss: 0.0384 - accuracy: 0.9896 - val_loss: 0.0602 - val_accuracy: 0.9849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16f47652460>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D\n",
    "\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, KERNEL_SIZE, activation='relu', input_shape=(MAX_LENGTH, EMBEDDING_DIM), padding='same'))\n",
    "# model.add(MaxPooling1D(2, padding='same'))\n",
    "model.add(Dropout(DROPOUT_RATE))\n",
    "model.add(Conv1D(32, KERNEL_SIZE, activation='relu', padding='same'))\n",
    "# model.add(MaxPooling1D(2))\n",
    "# model.add(Dropout(DROPOUT_RATE))\n",
    "# model.add(Dense(HIDDEN_DIM, activation='relu'))\n",
    "model.add(Dropout(DROPOUT_RATE))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "# Train CNN model\n",
    "model.fit(train.x, train.y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(valid.x, valid.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_cnn.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = tf.keras.models.load_model(\"model_cnn.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_30 (Conv1D)          (None, 200, 64)           19264     \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 200, 64)           0         \n",
      "                                                                 \n",
      " conv1d_31 (Conv1D)          (None, 200, 32)           6176      \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 200, 32)           0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 200, 9)            297       \n",
      "                                                                 \n",
      " crf_5 (CRF)                 (None, 200, 9)            189       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,926\n",
      "Trainable params: 25,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras_contrib\\layers\\crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras_contrib\\layers\\crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    }
   ],
   "source": [
    "from keras_contrib.layers import CRF\n",
    "\n",
    "# cnn_model.trainable = False \n",
    "\n",
    "# Build CRF layer\n",
    "crf = CRF(NUM_CLASSES, learn_mode='marginal')\n",
    "cnn_model.add(crf)\n",
    "cnn_model.compile(loss=crf.loss_function, optimizer='adam', metrics=[crf.accuracy])\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "439/439 [==============================] - 17s 39ms/step - loss: 0.0384 - accuracy: 0.9897 - val_loss: 0.0450 - val_accuracy: 0.9882\n",
      "Epoch 2/10\n",
      "439/439 [==============================] - 17s 40ms/step - loss: 0.0383 - accuracy: 0.9896 - val_loss: 0.0452 - val_accuracy: 0.9882\n",
      "Epoch 3/10\n",
      "439/439 [==============================] - 18s 40ms/step - loss: 0.0380 - accuracy: 0.9897 - val_loss: 0.0453 - val_accuracy: 0.9882\n",
      "Epoch 4/10\n",
      "439/439 [==============================] - 18s 41ms/step - loss: 0.0377 - accuracy: 0.9898 - val_loss: 0.0450 - val_accuracy: 0.9883\n",
      "Epoch 5/10\n",
      "439/439 [==============================] - 18s 40ms/step - loss: 0.0375 - accuracy: 0.9898 - val_loss: 0.0451 - val_accuracy: 0.9882\n",
      "Epoch 6/10\n",
      "439/439 [==============================] - 17s 38ms/step - loss: 0.0371 - accuracy: 0.9898 - val_loss: 0.0451 - val_accuracy: 0.9882\n",
      "Epoch 7/10\n",
      "439/439 [==============================] - 17s 38ms/step - loss: 0.0369 - accuracy: 0.9898 - val_loss: 0.0454 - val_accuracy: 0.9883\n",
      "Epoch 8/10\n",
      "439/439 [==============================] - 16s 37ms/step - loss: 0.0369 - accuracy: 0.9898 - val_loss: 0.0447 - val_accuracy: 0.9882\n",
      "Epoch 9/10\n",
      "439/439 [==============================] - 17s 38ms/step - loss: 0.0367 - accuracy: 0.9899 - val_loss: 0.0444 - val_accuracy: 0.9882\n",
      "Epoch 10/10\n",
      "439/439 [==============================] - 17s 38ms/step - loss: 0.0363 - accuracy: 0.9899 - val_loss: 0.0453 - val_accuracy: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16f477bffa0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train.x, train.y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(test.x, test.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_18 (Conv1D)          (None, 200, 64)           19264     \n",
      "                                                                 \n",
      " conv1d_19 (Conv1D)          (None, 200, 32)           6176      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 200, 9)            297       \n",
      "                                                                 \n",
      " crf_9 (CRF)                 (None, 200, 9)            189       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,926\n",
      "Trainable params: 25,926\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import InputLayer, Embedding, Conv1D, GlobalMaxPooling1D, Dense, TimeDistributed, Dropout\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_viterbi_accuracy\n",
    "model = Sequential()\n",
    "\n",
    "# model.add(InputLayer(input_shape=(MAX_LENGTH,)))\n",
    "# model.add(Embedding(input_dim=NUM_WORDS + 1, output_dim=OUTPUT_DIM, input_length=MAX_LENGTH))\n",
    "model.add(Conv1D(64, 3, padding='same', activation='relu', input_shape=(MAX_LENGTH, OUTPUT_DIM)))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Conv1D(32, 3, padding='same', activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(units=NUM_CLASSES))\n",
    "\n",
    "crf_layer = CRF(units=NUM_CLASSES, sparse_target=False)\n",
    "model.add(crf_layer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import SparseCategoricalCrossentropy, Loss\n",
    "class CustomNonPaddingTokenLoss(Loss):\n",
    "    def __init__(self, name=\"custom_ner_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=keras.losses.Reduction.NONE\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "loss = CustomNonPaddingTokenLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1054, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 543, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1173, in apply_gradients\n        grads_and_vars = self.aggregate_gradients(grads_and_vars)\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1139, in aggregate_gradients\n        return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\optimizers\\utils.py\", line 33, in all_reduce_sum_gradients\n        filtered_grads_and_vars = filter_empty_gradients(grads_and_vars)\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\optimizers\\utils.py\", line 77, in filter_empty_gradients\n        raise ValueError(\n\n    ValueError: No gradients provided for any variable: (['conv1d_18/kernel:0', 'conv1d_18/bias:0', 'conv1d_19/kernel:0', 'conv1d_19/bias:0', 'dense_10/kernel:0', 'dense_10/bias:0', 'crf_9/kernel:0', 'crf_9/chain_kernel:0', 'crf_9/bias:0', 'crf_9/left_boundary:0', 'crf_9/right_boundary:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'conv1d_18/kernel:0' shape=(3, 100, 64) dtype=float32>), (None, <tf.Variable 'conv1d_18/bias:0' shape=(64,) dtype=float32>), (None, <tf.Variable 'conv1d_19/kernel:0' shape=(3, 64, 32) dtype=float32>), (None, <tf.Variable 'conv1d_19/bias:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'dense_10/kernel:0' shape=(32, 9) dtype=float32>), (None, <tf.Variable 'dense_10/bias:0' shape=(9,) dtype=float32>), (None, <tf.Variable 'crf_9/kernel:0' shape=(9, 9) dtype=float32>), (None, <tf.Variable 'crf_9/chain_kernel:0' shape=(9, 9) dtype=float32>), (None, <tf.Variable 'crf_9/bias:0' shape=(9,) dtype=float32>), (None, <tf.Variable 'crf_9/left_boundary:0' shape=(9,) dtype=float32>), (None, <tf.Variable 'crf_9/right_boundary:0' shape=(9,) dtype=float32>)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(train\u001b[39m.\u001b[39;49mx, train\u001b[39m.\u001b[39;49my, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n",
      "File \u001b[1;32me:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filele_a8shm.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1054, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 543, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1173, in apply_gradients\n        grads_and_vars = self.aggregate_gradients(grads_and_vars)\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1139, in aggregate_gradients\n        return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\optimizers\\utils.py\", line 33, in all_reduce_sum_gradients\n        filtered_grads_and_vars = filter_empty_gradients(grads_and_vars)\n    File \"e:\\PFE\\CoNLL2003\\NERC\\venv\\lib\\site-packages\\keras\\optimizers\\utils.py\", line 77, in filter_empty_gradients\n        raise ValueError(\n\n    ValueError: No gradients provided for any variable: (['conv1d_18/kernel:0', 'conv1d_18/bias:0', 'conv1d_19/kernel:0', 'conv1d_19/bias:0', 'dense_10/kernel:0', 'dense_10/bias:0', 'crf_9/kernel:0', 'crf_9/chain_kernel:0', 'crf_9/bias:0', 'crf_9/left_boundary:0', 'crf_9/right_boundary:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'conv1d_18/kernel:0' shape=(3, 100, 64) dtype=float32>), (None, <tf.Variable 'conv1d_18/bias:0' shape=(64,) dtype=float32>), (None, <tf.Variable 'conv1d_19/kernel:0' shape=(3, 64, 32) dtype=float32>), (None, <tf.Variable 'conv1d_19/bias:0' shape=(32,) dtype=float32>), (None, <tf.Variable 'dense_10/kernel:0' shape=(32, 9) dtype=float32>), (None, <tf.Variable 'dense_10/bias:0' shape=(9,) dtype=float32>), (None, <tf.Variable 'crf_9/kernel:0' shape=(9, 9) dtype=float32>), (None, <tf.Variable 'crf_9/chain_kernel:0' shape=(9, 9) dtype=float32>), (None, <tf.Variable 'crf_9/bias:0' shape=(9,) dtype=float32>), (None, <tf.Variable 'crf_9/left_boundary:0' shape=(9,) dtype=float32>), (None, <tf.Variable 'crf_9/right_boundary:0' shape=(9,) dtype=float32>)).\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train.x, train.y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CRF in module keras_contrib.layers.crf object:\n",
      "\n",
      "class CRF(keras.engine.base_layer.Layer)\n",
      " |  CRF(units, learn_mode='join', test_mode=None, sparse_target=False, use_boundary=True, use_bias=True, activation='linear', kernel_initializer='glorot_uniform', chain_initializer='orthogonal', bias_initializer='zeros', boundary_initializer='zeros', kernel_regularizer=None, chain_regularizer=None, boundary_regularizer=None, bias_regularizer=None, kernel_constraint=None, chain_constraint=None, boundary_constraint=None, bias_constraint=None, input_dim=None, unroll=False, **kwargs)\n",
      " |  \n",
      " |  An implementation of linear chain conditional random field (CRF).\n",
      " |  \n",
      " |  An linear chain CRF is defined to maximize the following likelihood function:\n",
      " |  \n",
      " |  $$ L(W, U, b; y_1, ..., y_n) := \frac{1}{Z}\n",
      " |  \\sum_{y_1, ..., y_n} \\exp(-a_1' y_1 - a_n' y_n\n",
      " |      - \\sum_{k=1^n}((f(x_k' W + b) y_k) + y_1' U y_2)), $$\n",
      " |  \n",
      " |  where:\n",
      " |      $Z$: normalization constant\n",
      " |      $x_k, y_k$:  inputs and outputs\n",
      " |  \n",
      " |  This implementation has two modes for optimization:\n",
      " |  1. (`join mode`) optimized by maximizing join likelihood,\n",
      " |  which is optimal in theory of statistics.\n",
      " |     Note that in this case, CRF must be the output/last layer.\n",
      " |  2. (`marginal mode`) return marginal probabilities on each time\n",
      " |  step and optimized via composition\n",
      " |     likelihood (product of marginal likelihood), i.e.,\n",
      " |     using `categorical_crossentropy` loss.\n",
      " |     Note that in this case, CRF can be either the last layer or an\n",
      " |     intermediate layer (though not explored).\n",
      " |  \n",
      " |  For prediction (test phrase), one can choose either Viterbi\n",
      " |  best path (class indices) or marginal\n",
      " |  probabilities if probabilities are needed.\n",
      " |  However, if one chooses *join mode* for training,\n",
      " |  Viterbi output is typically better than marginal output,\n",
      " |  but the marginal output will still perform\n",
      " |  reasonably close, while if *marginal mode* is used for training,\n",
      " |  marginal output usually performs\n",
      " |  much better. The default behavior and `metrics.crf_accuracy`\n",
      " |  is set according to this observation.\n",
      " |  \n",
      " |  In addition, this implementation supports masking and accepts either\n",
      " |  onehot or sparse target.\n",
      " |  \n",
      " |  If you open a issue or a pull request about CRF, please\n",
      " |  add 'cc @lzfelix' to notify Luiz Felix.\n",
      " |  \n",
      " |  \n",
      " |  # Examples\n",
      " |  \n",
      " |  ```python\n",
      " |      from keras_contrib.layers import CRF\n",
      " |      from keras_contrib.losses import crf_loss\n",
      " |      from keras_contrib.metrics import crf_viterbi_accuracy\n",
      " |  \n",
      " |      model = Sequential()\n",
      " |      model.add(Embedding(3001, 300, mask_zero=True)(X)\n",
      " |  \n",
      " |      # use learn_mode = 'join', test_mode = 'viterbi',\n",
      " |      # sparse_target = True (label indice output)\n",
      " |      crf = CRF(10, sparse_target=True)\n",
      " |      model.add(crf)\n",
      " |  \n",
      " |      # crf_accuracy is default to Viterbi acc if using join-mode (default).\n",
      " |      # One can add crf.marginal_acc if interested, but may slow down learning\n",
      " |      model.compile('adam', loss=crf_loss, metrics=[crf_viterbi_accuracy])\n",
      " |  \n",
      " |      # y must be label indices (with shape 1 at dim 3) here,\n",
      " |      # since `sparse_target=True`\n",
      " |      model.fit(x, y)\n",
      " |  \n",
      " |      # prediction give onehot representation of Viterbi best path\n",
      " |      y_hat = model.predict(x_test)\n",
      " |  ```\n",
      " |  \n",
      " |  The following snippet shows how to load a persisted\n",
      " |  model that uses the CRF layer:\n",
      " |  \n",
      " |  ```python\n",
      " |      from keras.models import load_model\n",
      " |      from keras_contrib.losses import import crf_loss\n",
      " |      from keras_contrib.metrics import crf_viterbi_accuracy\n",
      " |  \n",
      " |      custom_objects={'CRF': CRF,\n",
      " |                      'crf_loss': crf_loss,\n",
      " |                      'crf_viterbi_accuracy': crf_viterbi_accuracy}\n",
      " |  \n",
      " |      loaded_model = load_model('<path_to_model>',\n",
      " |                                custom_objects=custom_objects)\n",
      " |  ```\n",
      " |  \n",
      " |  # Arguments\n",
      " |      units: Positive integer, dimensionality of the output space.\n",
      " |      learn_mode: Either 'join' or 'marginal'.\n",
      " |          The former train the model by maximizing join likelihood while the latter\n",
      " |          maximize the product of marginal likelihood over all time steps.\n",
      " |          One should use `losses.crf_nll` for 'join' mode\n",
      " |          and `losses.categorical_crossentropy` or\n",
      " |          `losses.sparse_categorical_crossentropy` for\n",
      " |          `marginal` mode.  For convenience, simply\n",
      " |          use `losses.crf_loss`, which will decide the proper loss as described.\n",
      " |      test_mode: Either 'viterbi' or 'marginal'.\n",
      " |          The former is recommended and as default when `learn_mode = 'join'` and\n",
      " |          gives one-hot representation of the best path at test (prediction) time,\n",
      " |          while the latter is recommended and chosen as default\n",
      " |          when `learn_mode = 'marginal'`,\n",
      " |          which produces marginal probabilities for each time step.\n",
      " |          For evaluating metrics, one should\n",
      " |          use `metrics.crf_viterbi_accuracy` for 'viterbi' mode and\n",
      " |          'metrics.crf_marginal_accuracy' for 'marginal' mode, or\n",
      " |          simply use `metrics.crf_accuracy` for\n",
      " |          both which automatically decides it as described.\n",
      " |          One can also use both for evaluation at training.\n",
      " |      sparse_target: Boolean (default False) indicating\n",
      " |          if provided labels are one-hot or\n",
      " |          indices (with shape 1 at dim 3).\n",
      " |      use_boundary: Boolean (default True) indicating if trainable\n",
      " |          start-end chain energies\n",
      " |          should be added to model.\n",
      " |      use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |      kernel_initializer: Initializer for the `kernel` weights matrix,\n",
      " |          used for the linear transformation of the inputs.\n",
      " |          (see [initializers](../initializers.md)).\n",
      " |      chain_initializer: Initializer for the `chain_kernel` weights matrix,\n",
      " |          used for the CRF chain energy.\n",
      " |          (see [initializers](../initializers.md)).\n",
      " |      boundary_initializer: Initializer for the `left_boundary`,\n",
      " |          'right_boundary' weights vectors,\n",
      " |          used for the start/left and end/right boundary energy.\n",
      " |          (see [initializers](../initializers.md)).\n",
      " |      bias_initializer: Initializer for the bias vector\n",
      " |          (see [initializers](../initializers.md)).\n",
      " |      activation: Activation function to use\n",
      " |          (see [activations](../activations.md)).\n",
      " |          If you pass None, no activation is applied\n",
      " |          (ie. \"linear\" activation: `a(x) = x`).\n",
      " |      kernel_regularizer: Regularizer function applied to\n",
      " |          the `kernel` weights matrix\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      chain_regularizer: Regularizer function applied to\n",
      " |          the `chain_kernel` weights matrix\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      boundary_regularizer: Regularizer function applied to\n",
      " |          the 'left_boundary', 'right_boundary' weight vectors\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      bias_regularizer: Regularizer function applied to the bias vector\n",
      " |          (see [regularizer](../regularizers.md)).\n",
      " |      kernel_constraint: Constraint function applied to\n",
      " |          the `kernel` weights matrix\n",
      " |          (see [constraints](../constraints.md)).\n",
      " |      chain_constraint: Constraint function applied to\n",
      " |          the `chain_kernel` weights matrix\n",
      " |          (see [constraints](../constraints.md)).\n",
      " |      boundary_constraint: Constraint function applied to\n",
      " |          the `left_boundary`, `right_boundary` weights vectors\n",
      " |          (see [constraints](../constraints.md)).\n",
      " |      bias_constraint: Constraint function applied to the bias vector\n",
      " |          (see [constraints](../constraints.md)).\n",
      " |      input_dim: dimensionality of the input (integer).\n",
      " |          This argument (or alternatively, the keyword argument `input_shape`)\n",
      " |          is required when using this layer as the first layer in a model.\n",
      " |      unroll: Boolean (default False). If True, the network will be\n",
      " |          unrolled, else a symbolic loop will be used.\n",
      " |          Unrolling can speed-up a RNN, although it tends\n",
      " |          to be more memory-intensive.\n",
      " |          Unrolling is only suitable for short sequences.\n",
      " |  \n",
      " |  # Input shape\n",
      " |      3D tensor with shape `(nb_samples, timesteps, input_dim)`.\n",
      " |  \n",
      " |  # Output shape\n",
      " |      3D tensor with shape `(nb_samples, timesteps, units)`.\n",
      " |  \n",
      " |  # Masking\n",
      " |      This layer supports masking for input data with a variable number\n",
      " |      of timesteps. To introduce masks to your data,\n",
      " |      use an [Embedding](embeddings.md) layer with the `mask_zero` parameter\n",
      " |      set to `True`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CRF\n",
      " |      keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
      " |      tensorflow.python.trackable.base.Trackable\n",
      " |      keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, learn_mode='join', test_mode=None, sparse_target=False, use_boundary=True, use_bias=True, activation='linear', kernel_initializer='glorot_uniform', chain_initializer='orthogonal', bias_initializer='zeros', boundary_initializer='zeros', kernel_regularizer=None, chain_regularizer=None, boundary_regularizer=None, bias_regularizer=None, kernel_constraint=None, chain_constraint=None, boundary_constraint=None, bias_constraint=None, input_dim=None, unroll=False, **kwargs)\n",
      " |  \n",
      " |  add_boundary_energy(self, energy, mask, start, end)\n",
      " |  \n",
      " |  backward_recursion(self, input_energy, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call. It is invoked automatically before\n",
      " |      the first execution of `call()`.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses\n",
      " |      (at the discretion of the subclass implementer).\n",
      " |      \n",
      " |      Args:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, X, mask=None)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      The `call()` method may not create state (except in its first\n",
      " |      invocation, wrapping the creation of variables or other resources in\n",
      " |      `tf.init_scope()`).  It is recommended to create state, including\n",
      " |      `tf.Variable` instances and nested `Layer` instances,\n",
      " |       in `__init__()`, or in the `build()` method that is\n",
      " |      called automatically before `call()` executes for the first time.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor, or dict/list/tuple of input tensors.\n",
      " |          The first positional `inputs` argument is subject to special rules:\n",
      " |          - `inputs` must be explicitly passed. A layer cannot have zero\n",
      " |            arguments, and `inputs` cannot be provided via the default value\n",
      " |            of a keyword argument.\n",
      " |          - NumPy array or Python scalar values in `inputs` get cast as\n",
      " |            tensors.\n",
      " |          - Keras mask metadata is only collected from `inputs`.\n",
      " |          - Layers are built (`build(input_shape)` method)\n",
      " |            using shape info from `inputs` only.\n",
      " |          - `input_spec` compatibility is only checked against `inputs`.\n",
      " |          - Mixed precision input casting is only applied to `inputs`.\n",
      " |            If a layer has tensor arguments in `*args` or `**kwargs`, their\n",
      " |            casting behavior in mixed precision should be handled manually.\n",
      " |          - The SavedModel input specification is generated using `inputs`\n",
      " |            only.\n",
      " |          - Integration with various ecosystem packages like TFMOT, TFLite,\n",
      " |            TF.js, etc is only supported for `inputs` and not for tensors in\n",
      " |            positional and keyword arguments.\n",
      " |        *args: Additional positional arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |        **kwargs: Additional keyword arguments. May contain tensors, although\n",
      " |          this is not recommended, for the reasons above.\n",
      " |          The following optional keyword arguments are reserved:\n",
      " |          - `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          - `mask`: Boolean input mask. If the layer's `call()` method takes a\n",
      " |            `mask` argument, its default value will be set to the mask\n",
      " |            generated for `inputs` by the previous layer (if `input` did come\n",
      " |            from a layer that generated a corresponding mask, i.e. if it came\n",
      " |            from a Keras layer with masking support).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_mask(self, input, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      This method will cause the layer's state to be built, if that has not\n",
      " |      happened before. This requires that the layer will later be used with\n",
      " |      inputs that match the input shape provided here.\n",
      " |      \n",
      " |      Args:\n",
      " |          input_shape: Shape tuple (tuple of integers) or `tf.TensorShape`,\n",
      " |              or structure of shape tuples / `tf.TensorShape` instances\n",
      " |              (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A `tf.TensorShape` instance\n",
      " |          or structure of `tf.TensorShape` instances.\n",
      " |  \n",
      " |  forward_recursion(self, input_energy, **kwargs)\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Note that `get_config()` does not guarantee to return a fresh copy of\n",
      " |      dict every time it is called. The callers should make a copy of the\n",
      " |      returned dict if they want to modify it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  get_energy(self, y_true, input_energy, mask)\n",
      " |      Energy = a1' y1 + u1' y1 + y1' U y2 + u2' y2 + y2' U y3 + u3' y3 + an' y3\n",
      " |  \n",
      " |  get_log_normalization_constant(self, input_energy, mask, **kwargs)\n",
      " |      Compute logarithm of the normalization constant Z, where\n",
      " |      Z = sum exp(-E) -> logZ = log sum exp(-E) =: -nlogZ\n",
      " |  \n",
      " |  get_marginal_prob(self, X, mask=None)\n",
      " |  \n",
      " |  get_negative_log_likelihood(self, y_true, X, mask)\n",
      " |      Compute the loss, i.e., negative log likelihood (normalize by number of time steps)\n",
      " |      likelihood = 1/Z * exp(-E) ->  neg_log_like = - log(1/Z * exp(-E)) = logZ + E\n",
      " |  \n",
      " |  recursion(self, input_energy, mask=None, go_backwards=False, return_sequences=True, return_logZ=True, input_length=None)\n",
      " |      Forward (alpha) or backward (beta) recursion\n",
      " |      \n",
      " |      If `return_logZ = True`, compute the logZ, the normalization constant:\n",
      " |      \n",
      " |      \\[ Z = \\sum_{y1, y2, y3} exp(-E) # energy\n",
      " |        = \\sum_{y1, y2, y3} exp(-(u1' y1 + y1' W y2 + u2' y2 + y2' W y3 + u3' y3))\n",
      " |        = sum_{y2, y3} (exp(-(u2' y2 + y2' W y3 + u3' y3))\n",
      " |        sum_{y1} exp(-(u1' y1' + y1' W y2))) \\]\n",
      " |      \n",
      " |      Denote:\n",
      " |          \\[ S(y2) := sum_{y1} exp(-(u1' y1 + y1' W y2)), \\]\n",
      " |          \\[ Z = sum_{y2, y3} exp(log S(y2) - (u2' y2 + y2' W y3 + u3' y3)) \\]\n",
      " |          \\[ logS(y2) = log S(y2) = log_sum_exp(-(u1' y1' + y1' W y2)) \\]\n",
      " |      Note that:\n",
      " |            yi's are one-hot vectors\n",
      " |            u1, u3: boundary energies have been merged\n",
      " |      \n",
      " |      If `return_logZ = False`, compute the Viterbi's best path lookup table.\n",
      " |  \n",
      " |  step(self, input_energy_t, states, return_logZ=True)\n",
      " |  \n",
      " |  viterbi_decoding(self, X, mask=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  shift_left(x, offset=1)\n",
      " |  \n",
      " |  shift_right(x, offset=1)\n",
      " |  \n",
      " |  softmaxNd(x, axis=-1)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  accuracy\n",
      " |  \n",
      " |  loss_function\n",
      " |  \n",
      " |  marginal_acc\n",
      " |  \n",
      " |  viterbi_acc\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific\n",
      " |          uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |        - If the layer is not built, the method will call `build`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid\n",
      " |          value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the\n",
      " |          constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be\n",
      " |      dependent on the inputs passed when calling a layer. Hence, when reusing\n",
      " |      the same layer on different inputs `a` and `b`, some entries in\n",
      " |      `layer.losses` may be dependent on `a` and some on `b`. This method\n",
      " |      automatically keeps track of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      The same code works in distributed training: the input to `add_loss()`\n",
      " |      is treated like a regularization loss and averaged across replicas\n",
      " |      by the training loop (both built-in `Model.fit()` and compliant custom\n",
      " |      training loops).\n",
      " |      \n",
      " |      The `add_loss` method can also be called directly on a Functional Model\n",
      " |      during construction. In this case, any loss Tensors passed to this Model\n",
      " |      must be symbolic and be able to be traced back to the model's `Input`s.\n",
      " |      These losses become part of the model's topology and are tracked in\n",
      " |      `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss\n",
      " |      references a `Variable` of one of the model's layers), you can wrap your\n",
      " |      loss in a zero-argument lambda. These losses are not tracked as part of\n",
      " |      the model's topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors,\n",
      " |          losses may also be zero-argument callables which create a loss\n",
      " |          tensor.\n",
      " |        **kwargs: Used for backwards compatibility only.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(inputs))\n",
      " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This\n",
      " |      is because we cannot trace the metric result tensor back to the model's\n",
      " |      inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result\n",
      " |          of calling a `keras.Metric` instance, it will be aggregated by\n",
      " |          default using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and\n",
      " |      variance in a BatchNormalization layer) may be dependent on the inputs\n",
      " |      passed when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case,\n",
      " |      variable updates are run on the fly and thus do not need to be tracked\n",
      " |      for later execution).\n",
      " |      \n",
      " |      Args:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregationV2.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        use_resource: Whether to use a `ResourceVariable` or not.\n",
      " |          See [this guide](\n",
      " |          https://www.tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables)\n",
      " |           for more information.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set\n",
      " |          to `AUTO` and the current `DistributionStrategy` chooses when to\n",
      " |          synchronize. If `synchronization` is set to `ON_READ`, `trainable`\n",
      " |          must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The variable created.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as\n",
      " |          `ON_READ`.\n",
      " |  \n",
      " |  build_from_config(self, config)\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects,\n",
      " |          describing how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  finalize_state(self)\n",
      " |      Finalizes the layers state after updating layer weights.\n",
      " |      \n",
      " |      This function can be subclassed in a layer and will be called after\n",
      " |      updating a layer weights. It can be overridden to finalize any\n",
      " |      additional layer state after a weight update.\n",
      " |      \n",
      " |      This function will be called after weights of a layer have been restored\n",
      " |      from a loaded model.\n",
      " |  \n",
      " |  get_build_config(self)\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first input node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first output node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer, as NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with\n",
      " |      this layer as a list of NumPy arrays, which can in turn be used to load\n",
      " |      state into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel\n",
      " |      matrix and the bias vector. These can be used to set the weights of\n",
      " |      another `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of NumPy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function, by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel\n",
      " |      matrix and the bias vector. These can be used to set the weights of\n",
      " |      another `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Args:\n",
      " |        weights: a list of NumPy arrays. The number\n",
      " |          of arrays and their shape must match\n",
      " |          number of the dimensions of the weights\n",
      " |          of the layer (i.e. it should match the\n",
      " |          output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the provided weights list does not match the\n",
      " |          layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the layer's computations.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
      " |      the weights.\n",
      " |      \n",
      " |      Layers automatically cast their inputs to the compute dtype, which\n",
      " |      causes computations and the output to be in the compute dtype as well.\n",
      " |      This is done by the base Layer class in `Layer.__call__`, so you do not\n",
      " |      have to insert these casts if implementing your own layer.\n",
      " |      \n",
      " |      Layers often perform certain internal computations in higher precision\n",
      " |      when `compute_dtype` is float16 or bfloat16 for numeric stability. The\n",
      " |      output will still typically be float16 or bfloat16 in such cases.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The layer's compute dtype.\n",
      " |  \n",
      " |  dtype\n",
      " |      The dtype of the layer weights.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
      " |      dtype of the layer's computations.\n",
      " |  \n",
      " |  dtype_policy\n",
      " |      The dtype policy associated with this layer.\n",
      " |      \n",
      " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Return Functional API nodes upstream of this layer.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is\n",
      " |      accessed, so it is eager safe: accessing `losses` under a\n",
      " |      `tf.GradientTape` will propagate gradients back to the corresponding\n",
      " |      variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> len(model.losses)\n",
      " |      0\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> len(model.losses)\n",
      " |      1\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of `Metric` objects.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are\n",
      " |      expected to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Return Functional API nodes downstream of this layer.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are\n",
      " |      not themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to\n",
      " |      enable the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input\n",
      " |      tensor of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a\n",
      " |      nicely-formatted error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.trackable.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(crf_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myLoss(y_true:tf.Tensor, y_pred):\n",
    "    print(\"ypred\", y_pred.shape)\n",
    "    for i in range(y_true.shape[1]):\n",
    "        for j in range(y_true.shape[2]):\n",
    "            for k in range(0, 9):\n",
    "                print(y_true[i, j, k])\n",
    "    return 0\n",
    "\n",
    "def myAccuracy(y_true, y_pred):\n",
    "    print(y_pred)\n",
    "    print(y_true)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.metrics import Metric\n",
    "from keras.losses import Loss\n",
    "\n",
    "class CRFLoss(Loss):\n",
    "    def __init__(self, num_labels):\n",
    "        super(CRFLoss, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(y_pred, tf.cast(y_true, tf.int32), np.ones([self.num_labels], dtype=np.float32))\n",
    "        loss = tf.reduce_mean(-log_likelihood)\n",
    "        return loss\n",
    "\n",
    "class CRFAccuracy(Metric):\n",
    "    def __init__(self, num_labels):\n",
    "        super(CRFAccuracy, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.total = self.add_weight('total', initializer='zeros')\n",
    "        self.count = self.add_weight('count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(tf.argmax(y_pred, axis=-1), tf.int32)\n",
    "        correct_predictions = tf.cast(tf.equal(y_true, y_pred), tf.float32)\n",
    "        mask = tf.not_equal(y_true, 0)\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        correct_predictions *= mask\n",
    "        self.total.assign_add(tf.reduce_sum(correct_predictions))\n",
    "        self.count.assign_add(tf.reduce_sum(mask))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.total.assign(0)\n",
    "        self.count.assign(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_contrib.losses import crf_loss\n",
    "from keras_contrib.metrics import crf_accuracy\n",
    "\n",
    "model.compile(optimizer='adam', loss=crf_loss, metrics=[crf_accuracy])\n",
    "# model.compile(optimizer='adam', loss=crf_layer.loss_function, metrics=[crf_layer.accuracy])\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss_fn(y_true, y_pred):\n",
    "    # squared_difference = tf.square(y_true - y_pred)\n",
    "    # return tf.reduce_mean(squared_difference, axis=-1)  # Note the `axis=-1`\n",
    "    return 0.0988\n",
    "def my_metric_fn(y_true, y_pred):\n",
    "    # squared_difference = tf.square(y_true - y_pred)\n",
    "    return 0.009  # Note the `axis=-1`\n",
    "model.compile(optimizer='adam', loss=my_loss_fn, metrics=[my_metric_fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 200, 128)          117248    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 200, 64)           49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200, 9)            585       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 167,241\n",
      "Trainable params: 167,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(MAX_LENGTH, OUTPUT_DIM), return_sequences=True))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train.x, train.y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "loss, accuracy = model.evaluate(test.x, test.y, batch_size=32)\n",
    "\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras_contrib.layers import CRF\n",
    "\n",
    "# word2vec_model = train.word2vec()\n",
    "\n",
    "# inputs = tf.keras.layers.Input(shape=(max_length, embedding_dim), dtype=tf.float32, name='sequence_input')\n",
    "# conv1D = tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(inputs)\n",
    "# maxPooling1D = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1D)\n",
    "# outputs = tf.keras.layers.Dense(num_classes, activation='relu')(maxPooling1D)\n",
    "# base = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# crf_layer = CRF(num_classes, sparse_target=False)\n",
    "# model = crf_layer(base)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#   tf.keras.layers.Input(shape=(max_length, 100), dtype=tf.float32, name='sequence_input'),\n",
    "#   tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "#   tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "#   tf.keras.layers.Dense(num_classes, activation='relu'),\n",
    "#   tf.keras.layers.Flatten(),\n",
    "#   tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(loss=crf_layer.loss_function, optimizer='adam', metrics=[crf_layer.accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trainning\n",
    "# batch_size = 56\n",
    "# num_epochs = 5\n",
    "\n",
    "# model.fit(train.x, train.y, epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "# # # Evaluation\n",
    "# # loss, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "# # print('Test Loss:', loss)\n",
    "# # print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential()\n",
    "# model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(113, 100)))\n",
    "# model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "# model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "# model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "# model.add(tf.keras.layers.Flatten())\n",
    "# model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "# model.add(tf.keras.layers.Dense(units=9, activation='softmax'))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(train.x, train.y, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Compile the model\n",
    "# # model.compile(optimizer='adam', loss=tfa.losses.SigmoidFocalCrossEntropy(), metrics=[tfa.metrics.F1Score(num_classes=9, threshold=0.5, dtype='float32')])\n",
    "# import tensorflow_addons as tfa\n",
    "# # no need to specify a loss for CRFModel, model will compute crf loss by itself\n",
    "# # model.compile(optimizer=tf.keras.optimizers.Adam(3e-4), metrics=['acc'])\n",
    "# model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(3e-4),\n",
    "#     loss=\"categorical_crossentropy\",\n",
    "#     metrics=[tfa.metrics.MultiLabelConfusionMatrix(num_classes=num_classes)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # vocab_size = len(word2vec_model.wv)\n",
    "        # embedding_dim = 100\n",
    "        # embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "        # for i, vec in enumerate(word2vec_model.wv):\n",
    "        #     embedding_matrix[i] = vec\n",
    "        # embedding_layer = Embedding(\n",
    "        #     input_dim=vocab_size,\n",
    "        #     output_dim=embedding_dim,\n",
    "        #     weights=[embedding_matrix],\n",
    "        #     trainable=False)\n",
    "        # return embedding_layer\n",
    "\n",
    "\n",
    "# class DataSet():\n",
    "#     def __init__(self):\n",
    "#         self.labels = {\"ner_tags\": set(), \"pos_tags\": set(), \"chunk_tags\": set()}\n",
    "#         self.word2vec_model = None\n",
    "#     def unique_values(self, index = \"ner_tags\"):\n",
    "#         if self.labels[index].__len__() > 0:\n",
    "#             return dict(zip(self.labels[index], range(0, len(self.labels[index]))))\n",
    "#         raise KeyError(\"Key does not exist !!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
