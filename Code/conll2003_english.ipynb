{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWBc46_ZTK5u"
      },
      "source": [
        "# Clone repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ-tKnxA74KU"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/taslimamindia/NERC.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqE9J52MZL3v"
      },
      "outputs": [],
      "source": [
        "# !pip install pyunpack\n",
        "# !pip install patool\n",
        "# !pip install unrar"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3fcvRYZ97fmq"
      },
      "source": [
        "# Importation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tou31yST7fmt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.utils import to_categorical, pad_sequences\n",
        "\n",
        "from keras import Model, Input\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, concatenate\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqqWJefmZL3w"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "from pickle import dump, load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MGeSev0ZL3y"
      },
      "outputs": [],
      "source": [
        "import pyunpack \n",
        "import unrar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ9h38D29wUb",
        "outputId": "babfea22-703f-43c3-b193-98428b0c401a"
      },
      "outputs": [],
      "source": [
        "from nltk import download\n",
        "download('wordnet')\n",
        "download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMfHyYuHTDud"
      },
      "source": [
        "# Base Class "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkK1C7mp7fmu"
      },
      "source": [
        "## Class define form data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsvoLVPY7fmu"
      },
      "outputs": [],
      "source": [
        "class Data(object):\n",
        "    unique_words = {\"<PAD>\": 0}\n",
        "    unique_ner_tags = {\"O\": 0}\n",
        "    unique_chunk_tags = {}\n",
        "    unique_pos_tags = {}\n",
        "    MAX_LENGTH = 50\n",
        "    VOCAB_SIZE = 100\n",
        "    PADDING_SIZE = 10\n",
        "    # Hyperparameters\n",
        "    NUM_FILTERS = 256\n",
        "    KERNEL_SIZE = 3\n",
        "    DROPOUT_RATE = 0.2\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 40\n",
        "\n",
        "    def __init__(self):\n",
        "        self.sentences = []\n",
        "        self.sentences_num = []\n",
        "        self.ner_tags = []\n",
        "        self.ner_tags_num = []\n",
        "        self.chunk_tags = []\n",
        "        self.pos_tags = []\n",
        "        self.features = []\n",
        "        self.x, self.y = None, None\n",
        "\n",
        "    def remove_attributes(self):\n",
        "        self.sentences_num = []\n",
        "        self.ner_tags_num = []\n",
        "        self.chunk_tags = []\n",
        "        self.pos_tags = []\n",
        "        self.features = []\n",
        "        self.x, self.y = None, None\n",
        "\n",
        "    def __add__(self, o):\n",
        "        data = Data()\n",
        "        data.sentences = self.sentences + o.sentences\n",
        "        data.sentences_num = self.sentences_num + o.sentences_num\n",
        "        data.ner_tags = self.ner_tags + o.ner_tags\n",
        "        data.ner_tags_num = self.ner_tags_num + o.ner_tags_num\n",
        "        data.chunk_tags = self.chunk_tags + o.chunk_tags\n",
        "        data.pos_tags = self.pos_tags + o.pos_tags\n",
        "        data.features = self.features + o.features\n",
        "        return data\n",
        "\n",
        "    def word2idx(self, word: str):\n",
        "        return Data.unique_words.get(word, None)\n",
        "\n",
        "    def idx2word(self, index: int):\n",
        "        for word, value in Data.unique_words.items():\n",
        "            if index is value:\n",
        "                return word\n",
        "        return None\n",
        "\n",
        "    def tag2idx(self, tag):\n",
        "        return Data.unique_ner_tags.get(tag, None)\n",
        "\n",
        "    def idx2tag(self, index):\n",
        "        for tag, value in Data.unique_ner_tags.items():\n",
        "            if index == value:\n",
        "                return tag\n",
        "        return None\n",
        "\n",
        "    def __unicity_tag(self, dico: dict, listes: list):\n",
        "        unique_word = set()\n",
        "        [unique_word.update(tags) for tags in listes]\n",
        "        max_index = len(dico)\n",
        "        for word in list(unique_word):\n",
        "            if dico.get(word, None) == None:\n",
        "                dico[word] = max_index\n",
        "                max_index += 1\n",
        "\n",
        "    def unicity(self):\n",
        "        self.__unicity_tag(Data.unique_ner_tags, self.ner_tags_num)\n",
        "        self.__unicity_tag(Data.unique_words, self.sentences_num)\n",
        "        self.__unicity_tag(Data.unique_chunk_tags, self.chunk_tags)\n",
        "        self.__unicity_tag(Data.unique_pos_tags, self.pos_tags)\n",
        "\n",
        "    def features_level(self):\n",
        "        def is_capitalize(word):\n",
        "            return len(word) > 1 and word[0].isupper() and word[1:].islower()\n",
        "        features = [\n",
        "            [\n",
        "                [is_capitalize(word), word.isupper(), word.islower(), word.istitle(), word.isdigit()]\n",
        "                for word in sentence\n",
        "            ]\n",
        "            for sentence in self.sentences\n",
        "        ]\n",
        "        self.features = [[[int(f) for f in feat] for feat in feature] for feature in features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVKzq06h5moN"
      },
      "source": [
        "## Use Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70grOo_p5moO"
      },
      "outputs": [],
      "source": [
        "class Model_Word2Vec:\n",
        "    def __init__(self, sentences):\n",
        "        self.model = Word2Vec(\n",
        "            sentences=sentences, min_count=1, vector_size=Data.VOCAB_SIZE, window=5\n",
        "        )\n",
        "\n",
        "    def wv(self, word):\n",
        "        return self.model.wv[word]\n",
        "\n",
        "def unziprar(path_rar, dest_dir):\n",
        "    pyunpack.Archive(path_rar).extractall(dest_dir, auto_create_dir=True)\n",
        "\n",
        "\n",
        "def serialization(data, path):\n",
        "    with open(path, \"wb\") as outfile:\n",
        "        dump(data, outfile)\n",
        "\n",
        "\n",
        "def deserialization(path):\n",
        "    with open(path, \"rb\") as infile:\n",
        "        data = load(infile)\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_data(data:Data, path, name):\n",
        "    data.x = np.load(file=path + name + \"_x.npy\")\n",
        "    data.features = np.load(file=path + name + \"_features.npy\")\n",
        "    data.y = np.load(file=path + name + \"_y.npy\")\n",
        "\n",
        "\n",
        "def save_data(data: Data, path, name):\n",
        "    np.save(file=path + name + \"_x.npy\", arr=data.x)\n",
        "    np.save(file=path + name + \"_features.npy\", arr=data.features)\n",
        "    np.save(file=path + name + name + \"_y.npy\", arr=data.y)\n",
        "\n",
        "\n",
        "def unformat_for_splitting(data: np.ndarray, initial_size):\n",
        "    x, y, z = data.shape\n",
        "    if initial_size < z:\n",
        "        y_initial = y - 1\n",
        "        X1, X2 = np.zeros(shape=(x, y_initial, z)), np.zeros(shape=(x, initial_size))\n",
        "        for i in range(x):\n",
        "            # X1[i], X2[i] = np.vsplit()\n",
        "            X1[i], X2[i] = data[i][:y_initial], data[i][y_initial][:initial_size]\n",
        "    else:\n",
        "        raise Exception(\"Initial_size must be larger than z.\")\n",
        "    return X1, X2\n",
        "\n",
        "\n",
        "def format_for_splitting(*args):\n",
        "    X1, X2 = args[0], args[1]\n",
        "    x, y, z = X1.shape\n",
        "    n = pad_sequences(X2, maxlen=z, padding=\"post\", value=0)\n",
        "    result = np.zeros(shape=(x, y + 1, z))\n",
        "    for i in range(x):\n",
        "        result[i] = np.vstack((X1[i], n[i]))\n",
        "    return result\n",
        "\n",
        "\n",
        "def string2num(lists, unique_word):\n",
        "    return [unique_word.get(l) for l in lists]\n",
        "\n",
        "\n",
        "def flatting(sentences):\n",
        "    return [word for sentence in sentences for word in sentence]\n",
        "\n",
        "\n",
        "def margin(sentences, batch_size):\n",
        "    \"\"\"Permet d'ajouter du marge sur les bords.\n",
        "\n",
        "    Args:\n",
        "        sentences (list[list]): _description_\n",
        "        batch_size (int): _description_\n",
        "\n",
        "    Returns:\n",
        "        list[list]: _description_\n",
        "\n",
        "    Example:\n",
        "        input: ['Peter', 'Blackburn']\n",
        "        output ['<pad>', 'Blackburn', 'Peter', 'Blackburn', '<pad>']\n",
        "    \"\"\"\n",
        "    batch_size = batch_size + 1\n",
        "    b_size = int(batch_size // 2)\n",
        "    pad = [np.zeros(shape=sentences[0][0].shape)]\n",
        "\n",
        "    def __pad(sentence: list):\n",
        "        n = len(sentence)\n",
        "        if n <= b_size:\n",
        "            sentence = sentence + pad * (b_size - n + 1)\n",
        "            n = len(sentence)\n",
        "        sentence = (\n",
        "            list(reversed(sentence[1 : b_size + 1]))\n",
        "            + sentence\n",
        "            + list(reversed(sentence[n - b_size - 1 : n - 1]))\n",
        "        )\n",
        "        n = len(sentence)\n",
        "        Sentences = []\n",
        "        for i in range(b_size, n - b_size):\n",
        "            Sentences.append(\n",
        "                np.array(sentence[i - b_size : i + b_size + 1][1:], dtype=\"float32\")\n",
        "            )\n",
        "\n",
        "        return Sentences\n",
        "\n",
        "    Sentences = [__pad(sentence.copy()) for sentence in sentences]\n",
        "    Sentences = [\n",
        "        [Sentences[i][j] for j in range(len(sentences[i]))]\n",
        "        for i in range(len(sentences))\n",
        "    ]\n",
        "    return Sentences\n",
        "\n",
        "\n",
        "def zip_2D(*args):\n",
        "    zipdata = list(zip(args[0], args[1], args[2], args[3], args[4]))\n",
        "    zipdata = [\n",
        "        list(zip(data[0], data[1], data[2], data[3], data[4])) for data in zipdata\n",
        "    ]\n",
        "    return zipdata\n",
        "\n",
        "\n",
        "def unzip_2D(args):\n",
        "    words, ners, chunks, poss, features = [], [], [], [], []\n",
        "    for arg in args:\n",
        "        word, ner, chunk, pos, feature = [], [], [], [], []\n",
        "        for triple in arg:\n",
        "            word.append(triple[0])\n",
        "            ner.append(triple[1])\n",
        "            chunk.append(triple[2])\n",
        "            pos.append(triple[3])\n",
        "            feature.append(triple[4])\n",
        "\n",
        "        words.append(word)\n",
        "        ners.append(ner)\n",
        "        chunks.append(chunk)\n",
        "        poss.append(pos)\n",
        "        features.append(feature)\n",
        "    return words, ners, chunks, poss, features\n",
        "\n",
        "\n",
        "def padding(data: Data):\n",
        "    data.flatten()\n",
        "    data.x = data.sentences_num\n",
        "    data.y = data.ner_tags\n",
        "    data.gather()\n",
        "    data.sentences_num = data.x\n",
        "    data.ner_tags_num = data.y\n",
        "\n",
        "\n",
        "def evaluation(y_true, y_predict):\n",
        "    true, false, total, predict = 0, 0, 0, 0\n",
        "    x, y = y_true.shape\n",
        "    for i in range(x):\n",
        "        real_tag = np.argmax(y_true[i])\n",
        "        predict_tag = np.argmax(y_predict[i])\n",
        "        if predict_tag == 0:\n",
        "            predict += 1\n",
        "        if real_tag != 0:\n",
        "            total = total + 1\n",
        "            if real_tag == predict_tag:\n",
        "                true = true + 1\n",
        "            else:\n",
        "                false = false + 1\n",
        "    print(\"----------------------- Evaluation -------------------------\")\n",
        "    print(y_true.shape)\n",
        "    print(predict, x)\n",
        "    print(\n",
        "        true, false, total, round(true / total, 3), round(false / total, 3), end=\"\\n\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "def checkDataset(train, test, valid):\n",
        "    if train != None:\n",
        "        print(\"X_train\", train.x.shape, \"y_train\", train.y.shape, \"\\n\")\n",
        "    if test != None:\n",
        "        print(\"X_test\", test.x.shape, \"y_test\", test.y.shape, \"\\n\")\n",
        "    if valid != None:\n",
        "        print(\"X_valid\", valid.x.shape, \"y_valid\", valid.y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxLQFuLO7fmv"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzjCn5tW7fmv"
      },
      "outputs": [],
      "source": [
        "class Loading():\n",
        "    def __init__(self, path):\n",
        "        if os.path.exists(\"../Data/conll2003_english/\"): \n",
        "            base_file = \"../Data/conll2003_english/\"\n",
        "        else:\n",
        "            base_file = \"/content/NERC/Data/conll2003_english/\"        \n",
        "        self.data = Data()\n",
        "        self.load_sentences(base_file + path)\n",
        "    def load_sentences(self, filepath):\n",
        "        tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                if (line == ('-DOCSTART- -X- -X- O\\n') or line == '\\n'):\n",
        "                    if len(tokens) > 0:\n",
        "                        self.data.sentences.append(tokens)\n",
        "                        self.data.pos_tags.append(pos_tags)\n",
        "                        self.data.chunk_tags.append(chunk_tags)\n",
        "                        self.data.ner_tags.append(ner_tags)\n",
        "                        tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
        "                else:\n",
        "                    l = line.split(' ')\n",
        "                    tokens.append(l[0])\n",
        "                    pos_tags.append(l[1])\n",
        "                    chunk_tags.append(l[2])\n",
        "                    ner_tags.append(l[3].strip('\\n'))\n",
        "            self.data.sentences_num = self.data.sentences\n",
        "            self.data.ner_tags_num = self.data.ner_tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9vvnsDR7fmv"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rbc4nVU7fmw"
      },
      "outputs": [],
      "source": [
        "class Preprocessing:\n",
        "    def __init__(self, data: Data, text=None, lang=\"english\"):\n",
        "        self.data = data\n",
        "        self.text = text\n",
        "        self.lang = lang\n",
        "\n",
        "    def tokenize(self):\n",
        "        if self.text != None:\n",
        "            sentenses = [\n",
        "                word_tokenize(sentence, language=self.lang)\n",
        "                for sentence in sent_tokenize(self.text, language=self.lang)\n",
        "            ]\n",
        "            self.data.sentences = [\n",
        "                [token for token in sentence if token not in stopwords.words(self.lang)]\n",
        "                for sentence in sentenses\n",
        "            ]\n",
        "            self.data.sentences_num = self.data.sentences\n",
        "\n",
        "    def lowercasing(self):\n",
        "        self.data.sentences_num = [\n",
        "            [word.lower() for word in sentence] for sentence in self.data.sentences_num\n",
        "        ]\n",
        "\n",
        "    def lemmatize(self):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        self.data.sentences_num = [\n",
        "            [lemmatizer.lemmatize(word) for word in sentence]\n",
        "            for sentence in self.data.sentences_num\n",
        "        ]\n",
        "\n",
        "    def remove_stopword(self):\n",
        "        punctuation = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
        "        punctuations = stopwords.words(self.lang) + punctuation\n",
        "        sentences = zip_2D(\n",
        "            self.data.sentences_num,\n",
        "            self.data.ner_tags_num,\n",
        "            self.data.chunk_tags,\n",
        "            self.data.pos_tags,\n",
        "            self.data.features\n",
        "        )\n",
        "        sentences = [\n",
        "            [\n",
        "                triple\n",
        "                for triple in sentence\n",
        "                if triple[0] not in punctuations or triple[1] != \"O\"\n",
        "            ]\n",
        "            for sentence in sentences\n",
        "        ]\n",
        "        (\n",
        "            self.data.sentences_num,\n",
        "            self.data.ner_tags_num,\n",
        "            self.data.chunk_tags,\n",
        "            self.data.pos_tags,\n",
        "            self.data.features\n",
        "        ) = unzip_2D(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0yl2FQm7fmw"
      },
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arjdv32N7fmw"
      },
      "outputs": [],
      "source": [
        "class Vectorization:\n",
        "    # path = \"E:\\\\word2vec\\\\gensim-data\\\\word2vec-google-news-300\\\\GoogleNews-vectors-negative300.bin\"\n",
        "    # word2vec_model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
        "\n",
        "    def __init__(self, data: Data, word2vec_model: Model_Word2Vec):\n",
        "        self.data = data\n",
        "        self.word2vec_model = word2vec_model\n",
        "        self.features = Data()\n",
        "\n",
        "    def word2vec(self):\n",
        "        Sentences = []\n",
        "        for sentence in self.data.sentences_num:\n",
        "            Sentence = []\n",
        "            for word in sentence:\n",
        "                try:\n",
        "                    # Sentence.append(self.word2vec_model.get_vector(word))\n",
        "                    Sentence.append(self.word2vec_model.wv(word))\n",
        "                except Exception as e:\n",
        "                    Sentence.append(self.word2vec_model.wv(word))\n",
        "            Sentences.append(Sentence)\n",
        "        self.data.sentences_num = Sentences\n",
        "\n",
        "    def padding_x(self):\n",
        "        self.data.x = pad_sequences(\n",
        "            sequences=self.data.sentences_num,\n",
        "            maxlen=self.data.MAX_LENGTH,\n",
        "            dtype=\"float32\",\n",
        "            padding=\"post\",\n",
        "            value=np.zeros((Data.VOCAB_SIZE,), dtype=\"float32\"),\n",
        "        )\n",
        "\n",
        "    def vectorized_x(self):\n",
        "        self.word2vec()\n",
        "        self.data.sentences_num = margin(\n",
        "            self.data.sentences_num, batch_size=Data.BATCH_SIZE\n",
        "        )\n",
        "        self.data.x = np.array(flatting(self.data.sentences_num), dtype=\"float32\")\n",
        "        # self.padding_x()\n",
        "\n",
        "    def tag2num(self):\n",
        "        self.data.ner_tags_num = [\n",
        "            [Data.unique_ner_tags.get(tag) for tag in tags]\n",
        "            for tags in self.data.ner_tags_num\n",
        "        ]\n",
        "\n",
        "    def num2oneHotEncoding(self):\n",
        "        NUM_CLASSES = 9\n",
        "        self.data.ner_tags_num = [\n",
        "            to_categorical(tags, num_classes=NUM_CLASSES)\n",
        "            for tags in self.data.ner_tags_num\n",
        "        ]\n",
        "\n",
        "    def padding_y(self):\n",
        "        self.data.y = pad_sequences(\n",
        "            sequences=self.data.ner_tags_num,\n",
        "            maxlen=self.data.MAX_LENGTH,\n",
        "            padding=\"post\",\n",
        "            dtype=\"float32\",\n",
        "            value=to_categorical(\n",
        "                Data.unique_ner_tags.get(\"O\"), num_classes=9\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def vectorized_y(self):\n",
        "        self.tag2num()\n",
        "        self.num2oneHotEncoding()\n",
        "        self.data.y = np.array(flatting(self.data.ner_tags_num), dtype=\"float32\")\n",
        "        # self.padding_y()\n",
        "\n",
        "    def __scaled(self, df: pd.DataFrame):\n",
        "        # copy the data\n",
        "        df_min_max_scaled = df.copy()\n",
        "        # apply normalization techniques\n",
        "        for column in df_min_max_scaled.columns:\n",
        "            df_min_max_scaled[column] = (\n",
        "                df_min_max_scaled[column] - df_min_max_scaled[column].min()\n",
        "            ) / (df_min_max_scaled[column].max() - df_min_max_scaled[column].min())\n",
        "        return df_min_max_scaled\n",
        "\n",
        "    def get_additional_features(self):\n",
        "        chunks = string2num(flatting(self.data.chunk_tags), Data.unique_chunk_tags)\n",
        "        poss = string2num(flatting(self.data.pos_tags), Data.unique_pos_tags)\n",
        "        features = flatting(self.data.features)\n",
        "        df_tags = pd.DataFrame({\"chunk_tags\": chunks, \"pos_tags\": poss})\n",
        "        df_features = pd.DataFrame(\n",
        "            data=features,\n",
        "            columns=[\"is_capitalize\", \"isupper\", \"islower\", \"istitle\", \"isdigit\"],\n",
        "        )\n",
        "        df = pd.concat((df_tags, df_features), axis=1)\n",
        "        self.data.features = self.__scaled(df).to_numpy(dtype=\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMdxz6YR7fmx"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-2jPEd6TZ6I"
      },
      "source": [
        "## Visalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPCrQ6VwmV03"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGCDK7Nxmb3f"
      },
      "outputs": [],
      "source": [
        "train = Loading(\"train.txt\").data\n",
        "test = Loading(\"test.txt\").data\n",
        "valid = Loading(\"valid.txt\").data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVvfZZfsmfIv"
      },
      "outputs": [],
      "source": [
        "class Visualisation:\n",
        "    def __init__(self, train, test=None, valid=None, pos: int=0, dim: int = 0):\n",
        "        X, y = None, None\n",
        "        if dim == 0:\n",
        "            if pos == 0:\n",
        "                Sentences = train.sentences + test.sentences + valid.sentences\n",
        "                Tags = train.ner_tags + test.ner_tags + valid.ner_tags\n",
        "            elif pos == 1:\n",
        "                Sentences = train.sentences_num + test.sentences_num + valid.sentences_num\n",
        "                Tags = train.ner_tags_num + test.ner_tags_num + valid.ner_tags_num\n",
        "            X = [word for sentence in Sentences for word in sentence]\n",
        "            y = [tag for tags in Tags for tag in tags]\n",
        "            self.df = pd.DataFrame({\"word\": X, \"label\": y})\n",
        "        else:\n",
        "            if pos == 0:\n",
        "                X, y = train[0], train[1]\n",
        "                self.df = pd.DataFrame({\"word\": np.zeros((X.shape[0])), \"label\": y})\n",
        "            elif pos == 1:\n",
        "                X, y = flatting(train[0]), flatting(train[1])\n",
        "                self.df = pd.DataFrame({\"word\": X, \"label\": y})\n",
        "    def classNumber(self):\n",
        "        hist = self.df.groupby(\"label\").count()\n",
        "        return hist\n",
        "\n",
        "    def classNumberHistogram(self):\n",
        "        hist = self.df.groupby(\"label\").count()\n",
        "        hist = pd.DataFrame({\"class\":hist.index, \"count\": hist[\"word\"]})\n",
        "        hist = hist.to_numpy().tolist()\n",
        "        cl = [t[0] for t in hist]\n",
        "        nbr = [t[1] for t in hist]\n",
        "        sns.barplot(data=pd.DataFrame({\"class\":cl, \"count\":nbr}), x=\"class\", y=\"count\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ1XVZjL0Fbx"
      },
      "outputs": [],
      "source": [
        "visualisation = Visualisation(train, test, valid, pos=1, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnUKp4IhONN7"
      },
      "outputs": [],
      "source": [
        "visualisation.classNumber()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7hnKPtIpVbQ"
      },
      "outputs": [],
      "source": [
        "visualisation.classNumberHistogram()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TEuNuFyqMrZ"
      },
      "outputs": [],
      "source": [
        "visualisation.df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOfYvGsH_C0H"
      },
      "outputs": [],
      "source": [
        "data = train + test + valid\n",
        "preprocess = Preprocessing(data=data)\n",
        "preprocess.remove_stopword()\n",
        "visualisation1 = Visualisation((data.sentences_num, data.ner_tags_num), pos=1, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYt8v5oi_C0H"
      },
      "outputs": [],
      "source": [
        "visualisation1.classNumber()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2YysjPc_C0J"
      },
      "outputs": [],
      "source": [
        "# visualisation1.classNumberHistogram()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NFpti3H7fm0"
      },
      "source": [
        "### New input text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jGe-JDa7fm1"
      },
      "outputs": [],
      "source": [
        "# test_text = Data()\n",
        "\n",
        "# preprocessing = Preprocessing(data = test_text, text = \"Obama is the president of the United States. I am from Guinea, nice to meet you.\")\n",
        "# preprocessing.tokenize()\n",
        "# preprocessing.lowercasing()\n",
        "# preprocessing.lemmatize()\n",
        "# print(test_text.sentences)\n",
        "\n",
        "# vector = Vectorization(test_text)\n",
        "# vector.vectorized_x()\n",
        "# print(test_text.x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npyM4czHULp-"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sikZydI_C0S"
      },
      "source": [
        "### Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_o-tb4i_C0T"
      },
      "outputs": [],
      "source": [
        "class Base_Model:\n",
        "    def __init__(self, train=True, test=True, valid=True):\n",
        "        if train:\n",
        "            self.train = Loading(\"train.txt\").data\n",
        "        else:\n",
        "            self.train = None\n",
        "        if test:\n",
        "            self.test = Loading(\"test.txt\").data\n",
        "        else:\n",
        "            self.test = None\n",
        "        if valid:\n",
        "            self.valid = Loading(\"valid.txt\").data\n",
        "        else:\n",
        "            self.valid = None\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def train_test_split(self):\n",
        "        initial_size = self.train.features.shape[1]\n",
        "        data = format_for_splitting(self.train.x, self.train.features)\n",
        "        x_train, x_test, self.train.y, self.test.y = train_test_split(\n",
        "            data, self.train.y, test_size=0.2\n",
        "        )\n",
        "        # x_train, x_valid, self.train.y, self.valid.y = test_test_split(x_train, self.train.y, test_size=0.2)\n",
        "        self.train.x, self.train.features = unformat_for_splitting(\n",
        "            x_train, initial_size=initial_size\n",
        "        )\n",
        "        # self.valid.x, self.valid.features = unformat_for_splitting(x_valid, initial_size=initial_size)\n",
        "        self.test.x, self.test.features = unformat_for_splitting(\n",
        "            x_test, initial_size=initial_size\n",
        "        )\n",
        "\n",
        "    def compress(self):\n",
        "        self.train = self.train + self.test + self.valid\n",
        "\n",
        "    def change(self, max_length=None, vocab_size=None, padding_size=None):\n",
        "        if max_length != None:\n",
        "            Data.MAX_LENGTH = max_length\n",
        "        if vocab_size != None:\n",
        "            Data.VOCAB_SIZE = vocab_size\n",
        "        if padding_size != None:\n",
        "            Data.PADDING_SIZE = padding_size\n",
        "\n",
        "    def preprocess(self, data: Data):\n",
        "        preprocessing = Preprocessing(data=data)\n",
        "        # preprocessing.lowercasing()\n",
        "        # preprocessing.lemmatize()\n",
        "        preprocessing.remove_stopword()\n",
        "        data.unicity()\n",
        "\n",
        "    def preprocessing(self):\n",
        "        if self.train != None:\n",
        "            self.preprocess(self.train)\n",
        "        # if self.test != None: self.preprocess(self.test)\n",
        "        # if self.valid != None: self.preprocess(self.valid)\n",
        "\n",
        "    def vectorize(self, data: Data, model_wv: Model_Word2Vec):\n",
        "        vector = Vectorization(data=data, word2vec_model=model_wv)\n",
        "        vector.vectorized_x()\n",
        "        vector.vectorized_y()\n",
        "        vector.get_additional_features()\n",
        "\n",
        "    def vectorization(self, model_wv: Model_Word2Vec):\n",
        "        if self.train != None:\n",
        "            self.vectorize(self.train, model_wv)\n",
        "        # if self.test != None: self.vectorize(self.test)\n",
        "        # if self.valid != None: self.vectorize(self.valid)\n",
        "\n",
        "    def summary(self):\n",
        "        self.model.summary()\n",
        "\n",
        "    def trainning(\n",
        "        self, optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        "    ):\n",
        "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "        if self.valid == None:\n",
        "            self.history = self.model.fit(\n",
        "                self.train.x,\n",
        "                self.train.y,\n",
        "                batch_size=Data.BATCH_SIZE,\n",
        "                epochs=Data.EPOCHS,\n",
        "            )\n",
        "        else:\n",
        "            self.history = self.model.fit(\n",
        "                self.train.x,\n",
        "                self.train.y,\n",
        "                batch_size=Data.BATCH_SIZE,\n",
        "                epochs=Data.EPOCHS,\n",
        "                validation_data=(self.valid.x, self.valid.y),\n",
        "            )\n",
        "\n",
        "    def testing(self):\n",
        "        return self.model.evaluate(self.test.x, self.test.y)\n",
        "\n",
        "    def predicting(self):\n",
        "        y_predict = self.model.predict(self.test.x, batch_size=Data.BATCH_SIZE)\n",
        "        evaluation(self.test.y, y_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train, test = Data(), Data()\n",
        "train = deserialization(\"Dataset/train.pickle\")\n",
        "test = deserialization(\"Dataset/test.pickle\")\n",
        "load_data(train, path=\"Dataset/\", name=\"train\")\n",
        "load_data(test, path=\"Dataset/\", name=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkmKgM_qZL4Y"
      },
      "outputs": [],
      "source": [
        "# train = deepcopy(cnn.train)\n",
        "# test = deepcopy(cnn.test)\n",
        "# train.remove_attributes()\n",
        "# test.remove_attributes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G_ckQb-euXy"
      },
      "outputs": [],
      "source": [
        "# unziprar(path_rar=\"train_x.rar\", dest_dir=\"Dataset\")\n",
        "# unziprar(path_rar=\"test_x.rar\", dest_dir=\"Dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1KlZ-wsZL4b"
      },
      "outputs": [],
      "source": [
        "# save_data(cnn.train, \"train\")\n",
        "# save_data(cnn.test, \"test\")\n",
        "# save_data(cnn.valid, \"valid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6IUaT76g8Ja"
      },
      "source": [
        "### CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-azndnf3Z0Ci"
      },
      "source": [
        "##### Class model CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z44LAt-5moQ"
      },
      "outputs": [],
      "source": [
        "class Model_CNN(Base_Model):\n",
        "    def __init__(self, train=True, test=True, valid=True):\n",
        "        super().__init__(train=train, test=test, valid=valid)\n",
        "        \n",
        "    def architecture(self, activation='sigmoid'):\n",
        "        # Define the model architecture\n",
        "        self.model = Sequential()\n",
        "        self.model.add(\n",
        "            Conv1D(\n",
        "                128,\n",
        "                Data.KERNEL_SIZE,\n",
        "                activation=\"relu\",\n",
        "                input_shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE),\n",
        "                padding=\"same\",\n",
        "            )\n",
        "        )\n",
        "        self.model.add(Dropout(Data.DROPOUT_RATE))\n",
        "        self.model.add(Conv1D(64, Data.KERNEL_SIZE, activation=\"relu\", padding=\"same\"))\n",
        "        self.model.add(Dropout(Data.DROPOUT_RATE))\n",
        "        self.model.add(Conv1D(32, Data.KERNEL_SIZE, activation=\"relu\", padding=\"same\"))\n",
        "        self.model.add(Dropout(Data.DROPOUT_RATE))\n",
        "        self.model.add(Dense(9, activation=activation)) #len(Data.unique_ner_tags)\n",
        "\n",
        "    def architecture_1(self, activation='sigmoid'):\n",
        "        # Define the model architecture\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Conv1D(32, 3, activation='relu', input_shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE)))\n",
        "        self.model.add(MaxPooling1D(2))\n",
        "        self.model.add(Conv1D(16, 3, activation='relu'))\n",
        "        self.model.add(MaxPooling1D(2))\n",
        "        # Flatten layer\n",
        "        self.model.add(Flatten())\n",
        "        # Dense layers\n",
        "        self.model.add(Dense(128, activation='relu'))\n",
        "        self.model.add(Dense(64, activation='relu'))\n",
        "        self.model.add(Dense(32, activation='relu'))\n",
        "        self.model.add(Dense(9, activation=activation))\n",
        "\n",
        "    def architecture_2(self, activation='sigmoid'):\n",
        "        input1 = tf.keras.Input(shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE), name=\"input_1\")\n",
        "        output1 = Conv1D(32, 3, activation='relu', input_shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE))(input1)\n",
        "        output1 = MaxPooling1D(2)(output1)\n",
        "        output1 = Conv1D(16, 3, activation='relu')(output1)\n",
        "        output1 = MaxPooling1D(2)(output1)\n",
        "        # Flatten layer\n",
        "        output1 = Flatten()(output1)\n",
        "        # Dense layers\n",
        "        output1 = Dense(128, activation='relu')(output1)\n",
        "        output1 = Dense(64, activation='relu')(output1)\n",
        "        output1 = Dense(32, activation='relu')(output1)\n",
        "        output1 = Dense(9, activation=activation)(output1)\n",
        "        self.model = Model(inputs=input1, outputs=output1)\n",
        "    \n",
        "    def architecture_3(self, activation='sigmoid'):\n",
        "        input1 = tf.keras.Input(shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE), name=\"input_1\")\n",
        "        output1 = Conv1D(64, 3, padding=\"same\", activation='relu')(input1)\n",
        "        output1 = MaxPooling1D(2, padding=\"same\")(output1)\n",
        "        output1 = Dropout(Data.DROPOUT_RATE)(output1)\n",
        "        output1 = Conv1D(32, 3, padding=\"same\", activation='relu')(output1)\n",
        "        output1 = MaxPooling1D(2, padding=\"same\")(output1)\n",
        "        output1 = Dropout(Data.DROPOUT_RATE)(output1)\n",
        "        # Flatten layer\n",
        "        output1 = Flatten()(output1)\n",
        "        # Dense layers\n",
        "        # output1 = Dense(128, activation='relu')(output1)\n",
        "        output1 = Dense(64, activation='relu')(output1)\n",
        "        output1 = Dense(32, activation='relu')(output1)\n",
        "        output1 = Dense(9, activation=activation)(output1)\n",
        "        self.model = Model(inputs=input1, outputs=output1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otbm1PdTZ40R"
      },
      "source": [
        "##### Test model CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsOYiFC_5moR"
      },
      "outputs": [],
      "source": [
        "cnn = Model_CNN()\n",
        "cnn.change(max_length=50,vocab_size=300, padding_size=10)\n",
        "cnn.compress()\n",
        "model_wv = Model_Word2Vec(cnn.train.sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMrqmWrKZL4X"
      },
      "outputs": [],
      "source": [
        "cnn.architecture_2()\n",
        "cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.architecture_3()\n",
        "cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHK-A4bn_C1B"
      },
      "outputs": [],
      "source": [
        "cnn.train.x = train.x\n",
        "cnn.train.y = train.y\n",
        "cnn.train.features = train.features\n",
        "cnn.test.x = test.x\n",
        "cnn.test.y = test.y\n",
        "cnn.test.features = test.features\n",
        "print(cnn.train.x.shape, cnn.train.features.shape, cnn.train.y.shape)\n",
        "print(cnn.test.x.shape, cnn.test.features.shape, cnn.test.y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yb2uHzaZL4X"
      },
      "outputs": [],
      "source": [
        "# cnn.train.features_level()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_leSeTIQZL4Y"
      },
      "outputs": [],
      "source": [
        "# cnn.preprocessing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7u14mEOZL4Y"
      },
      "outputs": [],
      "source": [
        "# serialization(train, \"Dataset\\\\train.pickle\")\n",
        "# serialization(test, \"Dataset\\\\test.pickle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj6aP1Z1ZL4Z"
      },
      "outputs": [],
      "source": [
        "# cnn.vectorization(model_wv=model_wv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzfg_Wzq_C1D"
      },
      "outputs": [],
      "source": [
        "# cnn.train_test_split()\n",
        "# checkDataset(cnn.train, cnn.test, None)\n",
        "# print(cnn.train.features.shape, cnn.test.features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9otrWbpJ_C1F"
      },
      "outputs": [],
      "source": [
        "Data.EPOCHS = 40\n",
        "cnn.trainning()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "hist = cnn.history\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('Pourcentage')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['accuracy', 'loss'], loc='upper left')\n",
        "plt.show()\n",
        "hist=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.testing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.predicting()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEctuVg7HirO"
      },
      "source": [
        "### Model LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbhO6LKDbEhf"
      },
      "source": [
        "#### Class model LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW0Culvw7fm5"
      },
      "outputs": [],
      "source": [
        "class Model_LSTM(Base_Model):\n",
        "    def __init__(self, train=True, test=True, valid=True):\n",
        "        super().__init__(train=train, test=test, valid=valid)\n",
        "    \n",
        "    def architecture(self):\n",
        "        # Define the model architecture\n",
        "        self.model = Sequential()\n",
        "        self.model.add(\n",
        "            LSTM(\n",
        "                256,\n",
        "                input_shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE),\n",
        "                return_sequences=True,\n",
        "                dropout=Data.DROPOUT_RATE,\n",
        "            )\n",
        "        )\n",
        "        self.model.add(LSTM(128, return_sequences=True, dropout=Data.DROPOUT_RATE))\n",
        "        self.model.add(LSTM(64, return_sequences=True, dropout=Data.DROPOUT_RATE))\n",
        "        self.model.add(LSTM(32, return_sequences=True, dropout=Data.DROPOUT_RATE))\n",
        "        self.model.add(Dense(9, activation=\"sigmoid\"))\n",
        "\n",
        "    # def model_1D(self):\n",
        "    #     # self.model.add(\n",
        "    #     #     LSTM(\n",
        "    #     #         256,\n",
        "    #     #         input_shape=(Data.VOCAB_SIZE,), return_state=True\n",
        "    #     #     )\n",
        "    #     # )\n",
        "    #     # # self.model.add(LSTM(128))\n",
        "    #     # # self.model.add(LSTM(64))\n",
        "    #     # # self.model.add(LSTM(32))\n",
        "    #     # self.model.add(Dense(len(Data.unique_ner_tags), activation=\"sigmoid\"))\n",
        "\n",
        "    #     input = tf.keras.Input(shape=(Data.VOCAB_SIZE,))\n",
        "    #     # model = Dropout(0.5)(model)\n",
        "    #     model = tf.keras.layers.Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(input)\n",
        "    #     out = tf.keras.layers.TimeDistributed(Dense(9, activation=\"softmax\"))(model)  # softmax output layer\n",
        "    #     self.model = tf.keras.Model(input, out)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma8R8LJybJpF"
      },
      "source": [
        "#### Test model LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwNkMj2a_C1V"
      },
      "outputs": [],
      "source": [
        "lstm = Model_LSTM()\n",
        "lstm.change(max_length=50,vocab_size=300, padding_size=10)\n",
        "lstm.compress()\n",
        "# model_wv = Model_Word2Vec(lstm.train.sentences)\n",
        "# lstm.train.features_level()\n",
        "# lstm.preprocessing()\n",
        "# lstm.vectorization(model_wv=model_wv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lstm.train = deserialization(\"Dataset/train.pickle\")\n",
        "lstm.test = deserialization(\"Dataset/test.pickle\")\n",
        "load_data(lstm.train, path=\"Dataset/\", name=\"train\")\n",
        "load_data(lstm.test, path=\"Dataset/\", name=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_lstm = Input(shape=(10, 300)) #(Data.PADDING_SIZE, Data.VOCAB_SIZE)\n",
        "\n",
        "output_lstm = LSTM(256, return_sequences=True, dropout=Data.DROPOUT_RATE)(input_lstm)\n",
        "output_lstm = LSTM(128, return_sequences=True, dropout=Data.DROPOUT_RATE)(output_lstm)\n",
        "output_lstm = LSTM(64, return_sequences=True, dropout=Data.DROPOUT_RATE)(output_lstm)\n",
        "output_lstm = LSTM(32, return_sequences=True, dropout=Data.DROPOUT_RATE)(output_lstm)\n",
        "\n",
        "output_lstm = Flatten()(output_lstm)\n",
        "\n",
        "output_lstm = Dense(64, activation=\"relu\")(output_lstm)\n",
        "output_lstm = Dense(32, activation=\"relu\")(output_lstm)\n",
        "output_lstm = Dense(9, activation=\"sigmoid\")(output_lstm)\n",
        "\n",
        "model_lstm = Model(inputs=input_lstm, outputs=output_lstm)\n",
        "\n",
        "model_lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_lstm.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "history_lstm = model_lstm.fit(lstm.train.x, lstm.train.y, batch_size=Data.BATCH_SIZE, epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_predict = model_lstm.predict(lstm.test.x)\n",
        "evaluation(lstm.test.y, y_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_lstm.save(\"Model/model_lstm.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7f-DWh8_C1V"
      },
      "outputs": [],
      "source": [
        "print(lstm.train.x.shape, lstm.train.y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geUEXfA2_C1W"
      },
      "outputs": [],
      "source": [
        "lstm.architecture()\n",
        "lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HizEJd4x5YIL"
      },
      "outputs": [],
      "source": [
        "# from keras.optimizers import Adam\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# def balanceData(data):\n",
        "#     x_train = np.array(data.sentences_num, dtype=\"float32\")\n",
        "#     y_train = np.array(data.ner_tags_num, dtype=\"float32\")\n",
        "#     print(x_train.shape, y_train.shape)\n",
        "#     input_dim = x_train.shape[1]\n",
        "\n",
        "#     # Encoder\n",
        "#     encoder_input = tf.keras.Input(shape=(input_dim,))\n",
        "#     encoder_dense = tf.keras.layers.Dense(400, activation=\"relu\")(encoder_input)\n",
        "#     encoder_output = tf.keras.layers.Dense(input_dim, activation=\"relu\")(encoder_dense)\n",
        "\n",
        "#     # Decoder\n",
        "#     decoder_dense = tf.keras.layers.Dense(400, activation=\"relu\")(encoder_output)\n",
        "#     decoder_output = tf.keras.layers.Dense(input_dim, activation=\"sigmoid\")(\n",
        "#         decoder_dense\n",
        "#     )\n",
        "\n",
        "#     # Autoencoder\n",
        "#     autoencoder = tf.keras.Model(encoder_input, decoder_output)\n",
        "#     autoencoder.compile(\n",
        "#         optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"accuracy\"]\n",
        "#     )\n",
        "#     # print(autoencoder.summary())\n",
        "\n",
        "#     # Train the autoencoder\n",
        "#     autoencoder.fit(x_train, x_train, epochs=10, batch_size=32)\n",
        "\n",
        "#     # Extract latent space representation\n",
        "#     encoder = tf.keras.Model(encoder_input, encoder_output)\n",
        "#     x_train_predict = encoder.predict(x_train)\n",
        "\n",
        "#     # Class balancing with SMOTE or any other technique adadelta\n",
        "#     smote = SMOTE()\n",
        "#     x_train_balanced, y_train_balanced = smote.fit_resample(x_train_predict, y_train)\n",
        "#     return x_train_balanced, y_train_balanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxXGAcX1NRL_"
      },
      "source": [
        "##### train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0NAWSgNEhsQ"
      },
      "outputs": [],
      "source": [
        "lstm.preprocess(lstm.train)\n",
        "vector = Vectorization(lstm.train)\n",
        "vector.word2vec()\n",
        "# vector.tag2num()\n",
        "# vector.num2oneHotEncoding()\n",
        "# lstm.train.flatten()\n",
        "# lstm.train.x, lstm.train.y = balanceData(lstm.train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJMaGjd6_C1c"
      },
      "outputs": [],
      "source": [
        "print(len(lstm.train.sentences),len(lstm.train.sentences_num))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv6jjKD6oNDA"
      },
      "outputs": [],
      "source": [
        "lstm.train.gather()\n",
        "lstm.train.x = np.array(lstm.train.x, dtype=\"float32\")\n",
        "lstm.train.ner_tags_num = lstm.train.y\n",
        "lstm.train.y = np.array(lstm.train.ner_tags_num, dtype=\"float32\")\n",
        "print(lstm.train.x.shape, lstm.train.y.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Egxil5CRNaOu"
      },
      "source": [
        "##### valid set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA3YPsOUsGYi"
      },
      "outputs": [],
      "source": [
        "lstm.valid = Loading(\"valid.txt\").data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZgnkYMMHibf"
      },
      "outputs": [],
      "source": [
        "lstm.preprocess(lstm.valid)\n",
        "vector = Vectorization(lstm.valid)\n",
        "vector.word2vec()\n",
        "vector.tag2num()\n",
        "vector.num2oneHotEncoding()\n",
        "lstm.valid.flatten()\n",
        "lstm.valid.x, lstm.valid.y = balanceData(lstm.valid)\n",
        "lstm.valid.gather()\n",
        "lstm.valid.x = np.array(lstm.valid.x, dtype=\"float32\")\n",
        "lstm.valid.ner_tags_num = lstm.valid.y\n",
        "lstm.valid.y = np.array(lstm.valid.ner_tags_num, dtype=\"float32\")\n",
        "print(lstm.valid.x.shape, lstm.valid.y.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1I7afWv3QP8f"
      },
      "source": [
        "##### Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFb1s9A7-0WL"
      },
      "outputs": [],
      "source": [
        "lstm.preprocess(lstm.test)\n",
        "padding(lstm.test)\n",
        "lstm.vectorize(lstm.test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-5K2vy18gf6"
      },
      "outputs": [],
      "source": [
        "# checkDataset(lstm.train, lstm.test, lstm.valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-801HqfMxVs"
      },
      "outputs": [],
      "source": [
        "# lstm.model=Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO4kJy_5_Yp5"
      },
      "outputs": [],
      "source": [
        "lstm.model_2D()\n",
        "lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-CpDHRdA0kS"
      },
      "outputs": [],
      "source": [
        "lstm.valid=None\n",
        "lstm.trainning()\n",
        "lstm.predicting()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd7STF-I8bN5"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from gensim.models import Word2Vec\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, LSTM\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.utils import class_weight\n",
        "\n",
        "# # Step 1: Load and preprocess the data\n",
        "\n",
        "# # Assuming you have a list of sentences and their corresponding labels\n",
        "# sentences = [\"This is sentence 1\", \"Another sentence\", \"Yet another sentence\"]\n",
        "# labels = [1, 0, 1]  # Assuming binary labels (e.g., 0 - non-NER, 1 - NER)\n",
        "\n",
        "# # Step 2: Train Word2Vec model\n",
        "\n",
        "# # Tokenize the sentences\n",
        "# tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
        "\n",
        "# # Train Word2Vec model\n",
        "# word2vec_model = Word2Vec(tokenized_sentences, size=100, window=5, min_count=1)\n",
        "\n",
        "# # Step 3: Encode sentences using Word2Vec embeddings\n",
        "\n",
        "# # Encode sentences into Word2Vec embeddings\n",
        "# encoded_sentences = []\n",
        "# for sentence in tokenized_sentences:\n",
        "#     sentence_encoding = []\n",
        "#     for word in sentence:\n",
        "#         if word in word2vec_model.wv.vocab:\n",
        "#             sentence_encoding.append(word2vec_model.wv[word])\n",
        "#     encoded_sentences.append(sentence_encoding)\n",
        "\n",
        "# # Pad sequences to the same length\n",
        "# padded_sentences = pad_sequences(encoded_sentences, maxlen=max_length, padding='post')\n",
        "\n",
        "# # Step 4: Build and train the autoencoder for imbalanced data\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(padded_sentences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Calculate class weights to address class imbalance\n",
        "# class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
        "\n",
        "# # Build the autoencoder model\n",
        "# autoencoder = Sequential()\n",
        "# autoencoder.add(Dense(64, activation='relu', input_shape=(max_length, 100)))\n",
        "# autoencoder.add(Dense(32, activation='relu'))\n",
        "# autoencoder.add(Dense(64, activation='relu'))\n",
        "# autoencoder.add(Dense(100, activation='relu'))\n",
        "\n",
        "# # Compile and train the autoencoder\n",
        "# autoencoder.compile(optimizer='adam', loss='mse')\n",
        "# autoencoder.fit(X_train, X_train, epochs=10, batch_size=16, class_weight=class_weights)\n",
        "\n",
        "# # Step 5: Extract features using LSTM\n",
        "\n",
        "# # Build the LSTM model\n",
        "# lstm_model = Sequential()\n",
        "# lstm_model.add(LSTM(64, input_shape=(max_length, 100)))\n",
        "# lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# # Compile and train the LSTM model\n",
        "# lstm_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "# lstm_model.fit(X_train, y_train, epochs=10, batch_size=16, class_weight=class_weights)\n",
        "\n",
        "# # Step 6: Make predictions\n",
        "\n",
        "# # Predict on test data\n",
        "# y_pred = lstm_model.predict(X_test)\n",
        "\n",
        "# # Convert predictions to labels\n",
        "# y_pred_labels = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
        "\n",
        "# # Evaluate the model\n",
        "# accuracy = np.mean(np.array(y_pred_labels) == np.array(y_test))\n",
        "# print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTJqiFMqrwxA"
      },
      "outputs": [],
      "source": [
        "visualisation1 = Visualisation(train = (x_train_balanced, y_train_balanced), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-0LaGTsxRvS"
      },
      "outputs": [],
      "source": [
        "visualisation1.classNumber()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31laKiUWggb8"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from keras import layers\n",
        "# from keras.utils import to_categorical\n",
        "\n",
        "# # Define your autoencoder model\n",
        "# input_dim = 200\n",
        "# encoding_dim = 64\n",
        "\n",
        "# autoencoder_input = tf.keras.Input(shape=(input_dim,))\n",
        "# encoded = layers.Dense(encoding_dim, activation='relu')(autoencoder_input)\n",
        "# decoded = layers.Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "# autoencoder = tf.keras.Model(autoencoder_input, decoded)\n",
        "\n",
        "# # Compile the model\n",
        "# autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# # Generate imbalanced training data\n",
        "# x_train = np.random.rand(1000, 200)\n",
        "# y_train = np.random.choice([0, 1], size=(1000,), p=[0.9, 0.1])\n",
        "# y_train = to_categorical(y_train, num_classes=2)  # Convert labels to one-hot encoding\n",
        "\n",
        "# print(autoencoder.summary())\n",
        "# print(\"train\", x_train.shape, y_train.shape)\n",
        "\n",
        "# # Train the autoencoder\n",
        "# autoencoder.fit(x_train, x_train, epochs=10, batch_size=32, shuffle=True)\n",
        "\n",
        "# # Obtain the encoded representations\n",
        "# encoder = tf.keras.Model(autoencoder_input, encoded)\n",
        "# encoded_train = encoder.predict(x_train)\n",
        "# print(encoded_train.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzsoZFW4baPv"
      },
      "source": [
        "#### Class model TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIppRirQ5mpF"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "class TF_IDF:\n",
        "    def __init__(self, train:Data=None, test:Data=None, valid:Data=None):\n",
        "        if train == None:\n",
        "          self.train = Loading(\"train.txt\").data\n",
        "        else:\n",
        "          self.train = train\n",
        "        if test == None:\n",
        "          self.test = Loading(\"test.txt\").data\n",
        "        else:\n",
        "          self.test = test\n",
        "        if valid == None:\n",
        "          self.valid = Loading(\"valid.txt\").data\n",
        "        else:\n",
        "          self.valid = valid\n",
        "\n",
        "    def __preprocess_tfidf(self, data: Data):\n",
        "        preprocessing = Preprocessing(data=data)\n",
        "        preprocessing.lowercasing()\n",
        "        preprocessing.lemmatize()\n",
        "        preprocessing.remove_stopword()\n",
        "        data.unicity()\n",
        "\n",
        "    def preprocessing(self):\n",
        "        self.__preprocess_tfidf(self.train)\n",
        "        self.__preprocess_tfidf(self.test)\n",
        "        self.__preprocess_tfidf(self.valid)\n",
        "\n",
        "    def __getMatrix(self, max_length, tfidf_matrix, data: Data, feature_names):\n",
        "        entities_data = []\n",
        "        for doc_index in range(len(data.sentences_num)):\n",
        "            doc_tfidf_scores = tfidf_matrix[doc_index].toarray().flatten()\n",
        "            top_indices = doc_tfidf_scores.argsort()[: -doc_tfidf_scores.shape[0] - 1 : -1]\n",
        "            top_entities = [feature_names[i] for i in top_indices if feature_names[i].split(\"__\")[1] != \"O\"]\n",
        "            doc_entities = [entitie for entitie in list(top_entities)[:max_length]]\n",
        "            entities_data.append(doc_entities)\n",
        "        data.sentences_num = entities_data\n",
        "\n",
        "    def __formalize_tfidf(self, data: Data):\n",
        "        return [\n",
        "            \" \".join([\"\".join([word, \"__\", tag]) for word, tag in zip(words, tags)])\n",
        "            for words, tags in zip(data.sentences_num, data.ner_tags_num)\n",
        "        ]\n",
        "\n",
        "    def __deformalize_tfidf(self, data: Data):\n",
        "        sentences_tags = data.sentences_num\n",
        "        data.sentences_num = [[sent_tag.split(\"__\")[0] for sent_tag in sentences_tags] for sentences_tags in sentences_tags]\n",
        "        data.ner_tags_num = [[sent_tag.split(\"__\")[1] for sent_tag in sentences_tags] for sentences_tags in sentences_tags]\n",
        "\n",
        "    def vectorization(self):\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            lowercase=False,\n",
        "            analyzer=\"word\",\n",
        "            stop_words=None,\n",
        "            token_pattern=\"[\\S]+\",\n",
        "            tokenizer=None,\n",
        "            preprocessor=None,\n",
        "        )\n",
        "        # formatted\n",
        "        data_train = self.__formalize_tfidf(self.train)\n",
        "        data_test = self.__formalize_tfidf(self.test)\n",
        "        data_valid = self.__formalize_tfidf(self.valid)\n",
        "        # Tf-idf vectorization\n",
        "        tfidf_matrix_train = vectorizer.fit_transform(data_train)\n",
        "        tfidf_matrix_test = vectorizer.transform(data_test)\n",
        "        tfidf_matrix_valid = vectorizer.transform(data_valid)\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        # Generation of the matrix\n",
        "        self.__getMatrix(Data.MAX_LENGTH, tfidf_matrix_train, self.train, feature_names)\n",
        "        self.__getMatrix(Data.MAX_LENGTH, tfidf_matrix_test, self.test, feature_names)\n",
        "        self.__getMatrix(Data.MAX_LENGTH, tfidf_matrix_valid, self.valid, feature_names)\n",
        "        # unformatted\n",
        "        self.__deformalize_tfidf(self.train)\n",
        "        self.__deformalize_tfidf(self.test)\n",
        "        self.__deformalize_tfidf(self.valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIFkBA7S5mpG"
      },
      "outputs": [],
      "source": [
        "tfidf = TF_IDF()\n",
        "tfidf.preprocessing()\n",
        "tfidf.vectorization()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-5xAyf4bgyZ"
      },
      "source": [
        "#### Test model TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO0kqmRDONOZ"
      },
      "outputs": [],
      "source": [
        "# import gensim.downloader as api\n",
        "# path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
        "# print(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPZEvRTT5mpH"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, Activation, Embedding, Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "# # Load data\n",
        "# df = pd.read_csv(\"ner_data.csv\", encoding=\"ISO-8859-1\", error_bad_lines=False)\n",
        "# df = df.fillna(method=\"ffill\")\n",
        "# sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "# tags = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "\n",
        "# # Perform TF-IDF\n",
        "# vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
        "# X_tfidf = vectorizer.fit_transform([\" \".join(sent) for sent in sentences])\n",
        "# tfidf_vocab = vectorizer.vocabulary_\n",
        "# tfidf_vocab_inv = {v:k for k,v in tfidf_vocab.items()}\n",
        "# tfidf_weights = np.asarray(X_tfidf.mean(axis=0)).ravel()\n",
        "\n",
        "# # Tokenize words\n",
        "# MAX_NB_WORDS = 20000\n",
        "# MAX_SEQ_LENGTH = 100\n",
        "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "# tokenizer.fit_on_texts(sentences)\n",
        "# word_index = tokenizer.word_index\n",
        "\n",
        "# # Convert words to sequences\n",
        "# X = tokenizer.texts_to_sequences(sentences)\n",
        "# X = pad_sequences(X, maxlen=MAX_SEQ_LENGTH)\n",
        "\n",
        "# # Convert tags to sequences\n",
        "# tags_index = {\"O\": 0, \"B-LOC\": 1, \"I-LOC\": 2, \"B-PER\": 3, \"I-PER\": 4, \"B-ORG\": 5, \"I-ORG\": 6}\n",
        "# y = [[tags_index[tag] for tag in sent] for sent in tags]\n",
        "# y = pad_sequences(y, maxlen=MAX_SEQ_LENGTH)\n",
        "\n",
        "# # Split data into train and test sets\n",
        "# VALIDATION_SPLIT = 0.2\n",
        "# nb_validation_samples = int(VALIDATION_SPLIT * len(X))\n",
        "# X_train = X[:-nb_validation_samples]\n",
        "# y_train = y[:-nb_validation_samples]\n",
        "# X_test = X[-nb_validation_samples:]\n",
        "# y_test = y[-nb_validation_samples:]\n",
        "\n",
        "# # Define CNN model\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(MAX_NB_WORDS, Data.VOCAB_SIZE, input_length=MAX_SEQ_LENGTH))\n",
        "# model.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "# model.add(GlobalMaxPooling1D())\n",
        "# model.add(Dense(7, activation=\"softmax\"))\n",
        "# model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# # Train model\n",
        "# model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n",
        "\n",
        "# # Predict tags for new sentences\n",
        "# def predict_tags(sentences):\n",
        "#     X = tokenizer.texts_to_sequences(sentences)\n",
        "#     X = pad_sequences(X, maxlen=MAX_SEQ_LENGTH)\n",
        "#     y_pred = model.predict(X)\n",
        "#     return [[tfidf_vocab_inv[np.argmax(tfidf_weights * y)] if np.max(tfidf_weights * y) > 0.2 else \"O\" for y in sent] for sent in y_pred]\n",
        "\n",
        "# # Test predictions\n",
        "# sentences_test = [\"John lives in New York City.\", \"Steve Jobs was the founder of Apple.\"]\n",
        "# tags_pred = predict_tags(sentences_test)\n",
        "# print(tags_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QGhljfiC5gG"
      },
      "outputs": [],
      "source": [
        "# entities = dict(zip(Data.unique_ner_tags.keys(), [0 for i in range(len(Data.unique_ner_tags))]))\n",
        "# for tags in test.ner_tags:\n",
        "#     for tag in tags:\n",
        "#         entities[tag] += 1\n",
        "# is_entities = 0\n",
        "# is_not_entities = 0\n",
        "# for tag, nbr in entities.items():\n",
        "#     if tag != 'O': is_entities += nbr\n",
        "#     else: is_not_entities += nbr\n",
        "# print(entities)\n",
        "# print(is_entities, is_not_entities)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dvATuqZ4_C18"
      },
      "source": [
        "### CNN for Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBL2xyIAZL4t"
      },
      "outputs": [],
      "source": [
        "input2 = Input(shape=cnn.train.features.shape[1:])\n",
        "output2 = Dense(7, activation='relu')(input2)\n",
        "output2 = Dense(10, activation=\"relu\")(output2)\n",
        "output2 = Dense(9, activation=\"softmax\")(output2)\n",
        "model2 =  Model(input2, output2)\n",
        "model2.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idaQo_MpZL4u"
      },
      "outputs": [],
      "source": [
        "history2 = model2.fit(cnn.train.features, cnn.train.y, batch_size=Data.BATCH_SIZE, epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqJH1jKrZL4u"
      },
      "outputs": [],
      "source": [
        "y_predict = model2.predict(cnn.test.features)\n",
        "evaluation(cnn.test.y, y_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "hist = history2\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('Pourcentage')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['accuracy', 'loss'], loc='upper left')\n",
        "plt.show()\n",
        "hist=None"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combinaison CNN for Word2Vec and Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CNN for word2vec\n",
        "input1 = tf.keras.Input(shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE), name=\"input_1\")\n",
        "output1 = Conv1D(32, 3, activation='relu', input_shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE))(input1)\n",
        "output1 = MaxPooling1D(2)(output1)\n",
        "output1 = Conv1D(16, 3, activation='relu')(output1)\n",
        "output1 = MaxPooling1D(2)(output1)\n",
        "# Flatten layer\n",
        "output1 = Flatten()(output1)\n",
        "# Dense layers\n",
        "output1 = Dense(128, activation='relu')(output1)\n",
        "output1 = Dense(64, activation='relu')(output1)\n",
        "output1 = Dense(32, activation='relu')(output1)\n",
        "output1 = Dense(9, activation=\"relu\")(output1)\n",
        "model1 = Model(inputs=input1, outputs=output1)\n",
        "\n",
        "# Fully connected for Features\n",
        "input2 = Input(shape=cnn.train.features.shape[1:], name=\"input_2\")\n",
        "output2 = Dense(16, activation=\"relu\")(input2)\n",
        "output2 = Dense(9, activation=\"relu\")(output2)\n",
        "model2 =  Model(inputs=input2, outputs=output2)\n",
        "\n",
        "# Output model for the concatenation of CNN for word2vec and Fully Connected\n",
        "model1_model2 = concatenate([model1.output, model2.output])\n",
        "output3 = Dense(18, activation=\"relu\")(model1_model2)\n",
        "output3 = Dense(16, activation=\"relu\")(output3)\n",
        "output3 = Dense(9, activation=\"sigmoid\")(output3)\n",
        "model = Model(inputs=[model1.input, model2.input], outputs=output3)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit([cnn.train.x, cnn.train.features], cnn.train.y, batch_size=Data.BATCH_SIZE, epochs=Data.EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_predict = model.predict([cnn.test.x, cnn.test.features])\n",
        "evaluation(cnn.test.y, y_predict)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pydot\n",
        "# tf.keras.utils.model_to_dot(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cnn.model.save(\"Model/cnn_model.keras\")\n",
        "# model1.save(\"Model/model1.keras\")\n",
        "# model.save(\"Model/model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hya4IeXZAIq"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from keras.layers import Input, Dense\n",
        "# from keras.models import Model\n",
        "# from keras.optimizers import Adam\n",
        "\n",
        "# # Generate synthetic balanced multilabel data\n",
        "# num_samples = 1000\n",
        "# num_features = 50\n",
        "# num_labels = 10\n",
        "# data = np.random.rand(num_samples, num_features)\n",
        "# labels = np.random.randint(2, size=(num_samples, num_labels))\n",
        "# labels_balanced = np.column_stack((labels[:, :5], np.logical_not(labels[:, :5])))\n",
        "# print(labels_balanced[0])\n",
        "# # Split the data into training and testing sets\n",
        "# train_data, test_data, train_labels, test_labels = train_test_split(data, labels_balanced, test_size=0.2, random_state=42)\n",
        "# print(train_data.shape, test_data.shape, train_labels.shape, test_labels.shape)\n",
        "# # Autoencoder architecture\n",
        "# input_dim = num_features\n",
        "# encoding_dim = 20\n",
        "\n",
        "# input_layer = Input(shape=(input_dim,))\n",
        "# encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "# decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
        "\n",
        "# # Create the autoencoder model\n",
        "# autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "\n",
        "# # Compile the model\n",
        "# autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy')\n",
        "\n",
        "# # Train the autoencoder\n",
        "# autoencoder.fit(train_data, train_data, epochs=10, batch_size=32, shuffle=True, validation_data=(test_data, test_data))\n",
        "# autoencoder.summary()\n",
        "# # Extract the encoder part for feature representation\n",
        "# encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
        "\n",
        "# # Get the encoded features\n",
        "# encoded_train_features = encoder_model.predict(train_data)\n",
        "# encoded_test_features = encoder_model.predict(test_data)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hWBc46_ZTK5u",
        "YMfHyYuHTDud",
        "3fcvRYZ97fmq",
        "mkK1C7mp7fmu",
        "VxLQFuLO7fmv",
        "I9vvnsDR7fmv",
        "T0yl2FQm7fmw",
        "N-2jPEd6TZ6I",
        "gPCrQ6VwmV03",
        "2NFpti3H7fm0",
        "rVKzq06h5moN",
        "u6IUaT76g8Ja",
        "-azndnf3Z0Ci",
        "AbhO6LKDbEhf",
        "Ma8R8LJybJpF",
        "vzsoZFW4baPv",
        "X-5xAyf4bgyZ",
        "HwS7GyBk5moe"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
