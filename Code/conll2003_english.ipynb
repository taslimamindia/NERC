{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ-tKnxA74KU",
        "outputId": "a3be3012-9198-4684-baf6-d9ea45f58cf3"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/taslimamindia/NERC.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fcvRYZ97fmq"
      },
      "source": [
        "# Importation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "tou31yST7fmt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from nltk import word_tokenize, sent_tokenize, download\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.utils import to_categorical, pad_sequences\n",
        "\n",
        "# import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ9h38D29wUb",
        "outputId": "845fddfd-81a4-4f08-e373-6cb620e91357"
      },
      "outputs": [],
      "source": [
        "download('wordnet') # for google colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkK1C7mp7fmu"
      },
      "source": [
        "# Class define form data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tsvoLVPY7fmu"
      },
      "outputs": [],
      "source": [
        "class Data(object):\n",
        "    unique_words = {\"<PAD>\":0}\n",
        "    unique_ner_tags = {\"O\":0}\n",
        "    MAX_LENGTH = 50\n",
        "    VOCAB_SIZE = 100\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.sentences = []\n",
        "        self.sentences_num = None\n",
        "        self.ner_tags = []\n",
        "        self.ner_tags_num = None\n",
        "        self.chunk_tags = []\n",
        "        self.pos_tags = []\n",
        "        self.x, self.y = None, None\n",
        "    def word2vec(self, vector_size=100):\n",
        "        VOCAB_SIZE = vector_size\n",
        "        word2vec_model = Word2Vec(self.sentences, vector_size=vector_size, window=5, min_count=1, workers=4)\n",
        "        return word2vec_model   \n",
        "    def word2idx(self, word:str):\n",
        "        return Data.unique_words.get(word, None)\n",
        "    def idx2word(self, index:int):\n",
        "        for word, value in Data.unique_words.items():\n",
        "            if index is value: return word\n",
        "        return None    \n",
        "    def tag2idx(self, tag):\n",
        "        return Data.unique_ner_tags.get(tag, None)\n",
        "    def idx2tag(self, index):\n",
        "        for tag, value in Data.unique_ner_tags.items():\n",
        "            if index == value: return tag\n",
        "        return None\n",
        "    def unicity(self):\n",
        "        unique_sent, unique_tag = set(), set()\n",
        "        [unique_tag.update(tags) for tags in self.ner_tags_num]\n",
        "        [unique_sent.update(tags) for tags in self.sentences_num]\n",
        "        max_tags = len(Data.unique_ner_tags)\n",
        "        max_words = len(Data.unique_words)\n",
        "        for word in list(unique_sent):\n",
        "            if Data.unique_words.get(word, None) == None:\n",
        "                Data.unique_words[word] = max_words\n",
        "                max_words += 1\n",
        "        for tag in list(unique_tag):\n",
        "            if Data.unique_ner_tags.get(tag, None) == None:\n",
        "                Data.unique_ner_tags[tag] = max_tags\n",
        "                max_tags += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxLQFuLO7fmv"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "NzjCn5tW7fmv"
      },
      "outputs": [],
      "source": [
        "class Loading():\n",
        "    def __init__(self, data: Data, file):\n",
        "        self.data = data\n",
        "        self.load_sentences(file)\n",
        "    def load_sentences(self, filepath):\n",
        "        tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                if (line == ('-DOCSTART- -X- -X- O\\n') or line == '\\n'):\n",
        "                    if len(tokens) > 0:\n",
        "                        self.data.sentences.append(tokens)\n",
        "                        self.data.pos_tags.append(pos_tags)\n",
        "                        self.data.chunk_tags.append(chunk_tags)\n",
        "                        self.data.ner_tags.append(ner_tags)\n",
        "                        tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
        "                else:\n",
        "                    l = line.split(' ')\n",
        "                    tokens.append(l[0])\n",
        "                    pos_tags.append(l[1])\n",
        "                    chunk_tags.append(l[2])\n",
        "                    ner_tags.append(l[3].strip('\\n'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9vvnsDR7fmv"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "3Rbc4nVU7fmw"
      },
      "outputs": [],
      "source": [
        "class Preprocessing():\n",
        "    def __init__(self, data:Data, text=None, lang=\"english\"):\n",
        "        self.data = data\n",
        "        self.text = text\n",
        "        self.lang = lang\n",
        "        if text == None:\n",
        "            self.data.sentences_num = self.data.sentences\n",
        "            self.data.ner_tags_num = self.data.ner_tags\n",
        "    \n",
        "    def tokenize(self):\n",
        "        if self.text != None:\n",
        "            sentenses = [word_tokenize(sentence, language=self.lang) for sentence in sent_tokenize(self.text, language=self.lang)]\n",
        "            self.data.sentences = [[token for token in sentence if token not in stopwords.words(self.lang)] for sentence in sentenses]\n",
        "            self.data.sentences_num = self.data.sentences\n",
        "        \n",
        "    def lowercasing(self):\n",
        "        self.data.sentences_num = [[word.lower() for word in sentence] for sentence in self.data.sentences_num]\n",
        "    \n",
        "    def lemmatize(self):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        self.data.sentences_num = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in self.data.sentences_num]\n",
        "    \n",
        "    def remove_stopword(self):\n",
        "        punctuation = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
        "        sentences = [[(self.data.sentences_num[i][j], self.data.ner_tags[i][j]) for j in range(len(self.data.sentences_num[i]))] for i in range(len(self.data.sentences_num))]\n",
        "        sentences = [[(token, tag) for token, tag in sentence if token not in stopwords.words(self.lang) + punctuation] for sentence in sentences]\n",
        "        self.data.sentences_num = [[token for token, tag in sentence] for sentence in sentences]\n",
        "        self.data.ner_tags_num = [[tag for token, tag in sentence] for sentence in sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0yl2FQm7fmw"
      },
      "source": [
        "# Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Arjdv32N7fmw"
      },
      "outputs": [],
      "source": [
        "class Vectorization():\n",
        "    \n",
        "    def __init__(self, data:Data):\n",
        "        self.data = data\n",
        "    \n",
        "    def word2vec(self, min_count=1, window=5):\n",
        "        word2vec_model = Word2Vec(self.data.sentences_num, min_count=min_count, vector_size=Data.VOCAB_SIZE, window=window)\n",
        "        self.data.sentences_num = [[word2vec_model.wv[word] for word in sentence] for sentence in self.data.sentences_num]\n",
        "    \n",
        "    def padding_x(self, value=np.zeros((Data.VOCAB_SIZE,), dtype=\"float32\"), dtype=\"float32\"):\n",
        "        self.data.x = pad_sequences(\n",
        "            sequences=self.data.sentences_num, \n",
        "            maxlen=self.data.MAX_LENGTH, \n",
        "            dtype=dtype, \n",
        "            padding=\"post\", \n",
        "            value=value\n",
        "        )\n",
        "    \n",
        "    def vectorized_x(self):\n",
        "        self.word2vec()\n",
        "        self.padding_x()\n",
        "        \n",
        "    def tag2num(self):\n",
        "        NUM_CLASSES = len(Data.unique_ner_tags)\n",
        "        self.data.ner_tags_num = [[to_categorical(Data.unique_ner_tags.get(tag), num_classes=NUM_CLASSES) for tag in tags] for tags in self.data.ner_tags_num]\n",
        "    \n",
        "    def padding_y(self, value=to_categorical(Data.unique_ner_tags.get(\"O\"), num_classes=NUM_CLASSES)):\n",
        "        self.data.y = pad_sequences(\n",
        "            sequences=self.data.ner_tags_num, \n",
        "            maxlen=self.data.MAX_LENGTH,\n",
        "            padding=\"post\", \n",
        "            dtype=\"float32\",\n",
        "            value=value\n",
        "        )\n",
        "    \n",
        "    def vectorized_y(self):\n",
        "        self.tag2num()\n",
        "        self.padding_y()\n",
        "\n",
        "def load_dataset(path: str):\n",
        "    data = Data()\n",
        "    base_file = \"../Data/conll2003_english/\"\n",
        "    # base_file = \"/content/NERC/Data/conll2003_english/\"\n",
        "    Loading(data = data, file=base_file + path)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMdxz6YR7fmx"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2NFpti3H7fm0"
      },
      "source": [
        "### New input text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jGe-JDa7fm1"
      },
      "outputs": [],
      "source": [
        "# test_text = Data()\n",
        "\n",
        "# preprocessing = Preprocessing(data = test_text, text = \"Obama is the president of the United States. I am from Guinea, nice to meet you.\")\n",
        "# preprocessing.tokenize()\n",
        "# preprocessing.lowercasing()\n",
        "# preprocessing.lemmatize()\n",
        "# print(test_text.sentences)\n",
        "\n",
        "# vector = Vectorization(test_text)\n",
        "# vector.vectorized_x()\n",
        "# print(test_text.x.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F08w-hsB7fm1"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "-M4-ULUI7fm1"
      },
      "outputs": [],
      "source": [
        "NUM_WORDS = len(Data.unique_words)\n",
        "NUM_CLASSES = len(Data.unique_ner_tags)\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 100\n",
        "NUM_FILTERS = 256\n",
        "KERNEL_SIZE = 3\n",
        "DROPOUT_RATE = 0.5\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluation(test:Data, y_predict):\n",
        "  true, false, total, predict = 0, 0, 0, 0\n",
        "  x, y, z = test.y.shape\n",
        "  for i in range(x):\n",
        "    for j in range(y):\n",
        "      real_tag = np.argmax(test.y[i][j]) \n",
        "      predict_tag = np.argmax(y_predict[i][j])\n",
        "      if predict_tag == 0: predict +=1\n",
        "      if real_tag != 0:\n",
        "        total = total + 1\n",
        "        if real_tag == predict_tag: true = true + 1\n",
        "        else: false = false + 1\n",
        "  print(\"----------------------- Evaluation -------------------------\")\n",
        "  print(test.y.shape)\n",
        "  print(predict, x*y)\n",
        "  print(true, false, total, round(true/total, 3), round(false/total, 3), end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "def checkDataset(train, test, valid):    \n",
        "    print(\"X_train\", train.x.shape)\n",
        "    print(\"y_train\", train.y.shape, \"\\n\")\n",
        "    print(\"X_test\", test.x.shape)\n",
        "    print(\"y_test\", test.y.shape, \"\\n\")    \n",
        "    print(\"X_valid\", valid.x.shape)\n",
        "    print(\"y_valid\", valid.y.shape)\n",
        "\n",
        "def main():\n",
        "    train = load_dataset(\"train.txt\")\n",
        "    test = load_dataset(\"test.txt\")\n",
        "    valid = load_dataset(\"valid.txt\")\n",
        "    preprocess_lstm(train)\n",
        "    preprocess_lstm(test)\n",
        "    preprocess_lstm(valid)\n",
        "    vectorize(train)\n",
        "    vectorize(test)\n",
        "    vectorize(valid)\n",
        "    checkDataset(train, test, valid)\n",
        "    return train, test, valid"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u6IUaT76g8Ja"
      },
      "source": [
        "## CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D\n",
        "\n",
        "class Model_CNN:\n",
        "  def __init__(self):\n",
        "    # Define the model architecture\n",
        "    self.model = Sequential()\n",
        "    self.model.add(Conv1D(64, KERNEL_SIZE, activation='relu', input_shape=(Data.MAX_LENGTH, EMBEDDING_DIM), padding='same'))\n",
        "    self.model.add(Dropout(DROPOUT_RATE))\n",
        "    self.model.add(Conv1D(32, KERNEL_SIZE, activation='relu', padding='same'))\n",
        "    self.model.add(Dropout(DROPOUT_RATE))\n",
        "    self.model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "    \n",
        "  def summary(self):\n",
        "    self.model.summary()\n",
        "    \n",
        "  def trainning(self, train:Data, valid:Data=None):\n",
        "    cat_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "    recall = tf.keras.metrics.Recall()\n",
        "    self.model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[cat_accuracy, recall])\n",
        "    if valid == None:\n",
        "      self.model.fit(train.x, train.y, batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
        "    else:\n",
        "      self.model.fit(train.x, train.y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(valid.x, valid.y))\n",
        "      \n",
        "  def testing(self, test:Data):\n",
        "    return self.model.evaluate(test.x, test.y)\n",
        "  \n",
        "  def predicting(self, test:Data):\n",
        "    return self.model.predict(test.x, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main_cnn(param:dict):\n",
        "  dico = {\"params\":[], \"metrics\":[]}\n",
        "  if param.get(\"max_length\", 0) != 0:\n",
        "      max_lengths = param[\"max_length\"]\n",
        "      for max_length in max_lengths:   \n",
        "        Data.MAX_LENGTH = max_length     \n",
        "        train, test, valid = main()\n",
        "        model_cnn = Model_CNN()\n",
        "        model_cnn.trainning(train, valid)\n",
        "        model_cnn.testing(test)\n",
        "        y_predict_cnn = model_cnn.predicting(test)\n",
        "        evaluation(test, y_predict_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train (14041, 50, 100)\n",
            "y_train (14041, 50, 9) \n",
            "\n",
            "X_test (3453, 50, 100)\n",
            "y_test (3453, 50, 9) \n",
            "\n",
            "X_valid (3250, 50, 100)\n",
            "y_valid (3250, 50, 9)\n",
            "Epoch 1/10\n",
            "439/439 [==============================] - 10s 19ms/step - loss: 0.3929 - categorical_accuracy: 0.9510 - recall_4: 0.8289 - val_loss: 0.2289 - val_categorical_accuracy: 0.9478 - val_recall_4: 0.9386\n",
            "Epoch 2/10\n",
            "439/439 [==============================] - 9s 21ms/step - loss: 0.1779 - categorical_accuracy: 0.9558 - recall_4: 0.9379 - val_loss: 0.2396 - val_categorical_accuracy: 0.9478 - val_recall_4: 0.8977\n",
            "Epoch 3/10\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.1678 - categorical_accuracy: 0.9574 - recall_4: 0.9407 - val_loss: 0.2298 - val_categorical_accuracy: 0.9478 - val_recall_4: 0.8857\n",
            "Epoch 4/10\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.1616 - categorical_accuracy: 0.9580 - recall_4: 0.9429 - val_loss: 0.2321 - val_categorical_accuracy: 0.9478 - val_recall_4: 0.9006\n",
            "Epoch 5/10\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.1577 - categorical_accuracy: 0.9583 - recall_4: 0.9438 - val_loss: 0.2318 - val_categorical_accuracy: 0.9480 - val_recall_4: 0.8781\n",
            "Epoch 6/10\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.1552 - categorical_accuracy: 0.9588 - recall_4: 0.9449 - val_loss: 0.2204 - val_categorical_accuracy: 0.9470 - val_recall_4: 0.9054\n",
            "Epoch 7/10\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.1524 - categorical_accuracy: 0.9591 - recall_4: 0.9459 - val_loss: 0.2291 - val_categorical_accuracy: 0.9460 - val_recall_4: 0.8856\n",
            "Epoch 8/10\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.1511 - categorical_accuracy: 0.9593 - recall_4: 0.9468 - val_loss: 0.2128 - val_categorical_accuracy: 0.9463 - val_recall_4: 0.9216\n",
            "Epoch 9/10\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.1485 - categorical_accuracy: 0.9593 - recall_4: 0.9476 - val_loss: 0.2174 - val_categorical_accuracy: 0.9472 - val_recall_4: 0.9147\n",
            "Epoch 10/10\n",
            "439/439 [==============================] - 8s 18ms/step - loss: 0.1473 - categorical_accuracy: 0.9596 - recall_4: 0.9482 - val_loss: 0.2164 - val_categorical_accuracy: 0.9451 - val_recall_4: 0.9081\n",
            "108/108 [==============================] - 1s 7ms/step - loss: 0.1758 - categorical_accuracy: 0.9543 - recall_4: 0.9433\n",
            "108/108 [==============================] - 1s 6ms/step\n",
            "----------------------- Evaluation -------------------------\n",
            "(3453, 50, 9)\n",
            "171968 172650\n",
            "307 7701 8008 0.038 0.962\n",
            "\n"
          ]
        }
      ],
      "source": [
        "main_cnn({\"max_length\":[50]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0eEcWeJRv9G"
      },
      "outputs": [],
      "source": [
        "# # from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from keras.models import Model\n",
        "# from keras.layers import Dense, Conv1D\n",
        "# from tf2crf import CRF, ModelWithCRFLoss\n",
        "# from keras import Input\n",
        "\n",
        "# # Build CNN model\n",
        "# # model = Sequential()\n",
        "# inputs = Input(shape=(MAX_LENGTH, EMBEDDING_DIM))\n",
        "# outputs = Conv1D(64, KERNEL_SIZE, activation='relu', padding='same')(inputs)\n",
        "# # model.add(MaxPooling1D(2, padding='same'))\n",
        "# # outputs = Dropout(DROPOUT_RATE)(inputs)\n",
        "# outputs = Conv1D(32, KERNEL_SIZE, activation='relu', padding='same')(inputs)\n",
        "# # model.add(MaxPooling1D(2))\n",
        "# # model.add(Dropout(DROPOUT_RATE))\n",
        "# # model.add(Dense(HIDDEN_DIM, activation='relu'))\n",
        "# # outputs = Dropout(DROPOUT_RATE)(outputs)\n",
        "# outputs = Dense(NUM_CLASSES, activation='relu')(outputs)\n",
        "# # outputs.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# # outputs.summary()\n",
        "# crf = CRF(units=9)\n",
        "# # cnn_model.add(crf)\n",
        "# output = crf(outputs)\n",
        "# cnn_crf_model = Model(inputs, output)\n",
        "# cnn_crf_model.summary()\n",
        "# # cnn_crf_model = ModelWithCRFLoss(base_model, sparse_target=True)\n",
        "# # cnn_crf_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVZz_xMcRv9H"
      },
      "outputs": [],
      "source": [
        "# cnn_crf_model.compile(optimizer='adam')\n",
        "# cnn_crf_model.fit(train.x, train.y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(valid.x, valid.y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqUvOqaORv9H"
      },
      "outputs": [],
      "source": [
        "# # Evaluation\n",
        "# loss, accuracy = cnn_crf_model.evaluate(test.x, test.y, batch_size=BATCH_SIZE)\n",
        "\n",
        "# print('Test Loss:', loss)\n",
        "# print('Test Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEctuVg7HirO"
      },
      "source": [
        "## Model LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "IgS5eRZ07fmy"
      },
      "outputs": [],
      "source": [
        "def preprocess_lstm(data:Data):\n",
        "    preprocessing = Preprocessing(data=data)\n",
        "    preprocessing.lowercasing()\n",
        "    preprocessing.lemmatize()\n",
        "    preprocessing.remove_stopword()\n",
        "    data.unicity()\n",
        "\n",
        "def vectorize(data:Data):\n",
        "    vector = Vectorization(data=data)\n",
        "    vector.vectorized_x()\n",
        "    vector.vectorized_y()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "hW0Culvw7fm5"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "class Model_LSTM:\n",
        "  def __init__(self):\n",
        "    # Define the model architecture\n",
        "    self.model_LSTM = Sequential()\n",
        "    self.model_LSTM.add(LSTM(256, input_shape=(Data.MAX_LENGTH, Data.VOCAB_SIZE), return_sequences=True, dropout=0.5))\n",
        "    self.model_LSTM.add(LSTM(128, return_sequences=True, dropout=0.5))\n",
        "    self.model_LSTM.add(LSTM(64, return_sequences=True, dropout=0.5))\n",
        "    self.model_LSTM.add(LSTM(32, return_sequences=True, dropout=0.5))\n",
        "    self.model_LSTM.add(Dense(9, activation='softmax'))\n",
        "  def summary(self):\n",
        "    self.model_LSTM.summary()\n",
        "  def trainning(self, train:Data, valid:Data=None):\n",
        "    cat_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "    recall = tf.keras.metrics.Recall()\n",
        "    self.model_LSTM.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[cat_accuracy, recall])\n",
        "    if valid == None:\n",
        "      self.model_LSTM.fit(train.x, train.y, batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
        "    else:\n",
        "      self.model_LSTM.fit(train.x, train.y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(valid.x, valid.y))\n",
        "  def testing(self, test:Data):\n",
        "    return self.model_LSTM.evaluate(test.x, test.y)\n",
        "  def predicting(self, test:Data):\n",
        "    return self.model_LSTM.predict(test.x, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main_lstm(param:dict):\n",
        "    dico = {\"params\":[], \"metrics\":[]}\n",
        "    if param.get(\"max_length\", 0) != 0:\n",
        "        max_lengths = param[\"max_length\"]\n",
        "        for max_length in max_lengths:   \n",
        "            Data.MAX_LENGTH = max_length     \n",
        "            train, test, valid = main()\n",
        "            model_lstm = Model_LSTM()\n",
        "            model_lstm.trainning(train, valid)\n",
        "            model_lstm.testing(test)\n",
        "            y_predict_lstm = model_lstm.predicting(test)\n",
        "            evaluation(test, y_predict_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "chYF5-l2CvEB",
        "outputId": "c1fd05c0-dfd7-43c9-ea5b-53efcd4f363b"
      },
      "outputs": [],
      "source": [
        "main_lstm({\"max_length\":[50]})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7eHuf96nXDm"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tqp60CTqGqzV"
      },
      "outputs": [],
      "source": [
        "# model_LSTM.save(\"../Data/model_lstm.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itvrF6SvFmUg"
      },
      "outputs": [],
      "source": [
        "# model_LSTM = tf.keras.models.load_model(\"../Data/model_lstm.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x5O84rzHU9m"
      },
      "outputs": [],
      "source": [
        "# # np.quantile(sort([1, 2, 3, 8, 7]), 0.50)\n",
        "# dico = {}\n",
        "# for tags in test.sentences + train.sentences + valid.sentences:\n",
        "#   if dico.get(len(tags), None) == None:\n",
        "#     dico[len(tags)] = 1\n",
        "#   dico[len(tags)] += 1\n",
        "# sorted(list(dico.items()), key= lambda x: x[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QGhljfiC5gG"
      },
      "outputs": [],
      "source": [
        "# entities = dict(zip(Data.unique_ner_tags.keys(), [0 for i in range(len(Data.unique_ner_tags))]))\n",
        "# for tags in test.ner_tags:\n",
        "#     for tag in tags:\n",
        "#         entities[tag] += 1\n",
        "# is_entities = 0\n",
        "# is_not_entities = 0\n",
        "# for tag, nbr in entities.items():\n",
        "#     if tag != 'O': is_entities += nbr\n",
        "#     else: is_not_entities += nbr\n",
        "# print(entities)\n",
        "# print(is_entities, is_not_entities)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF CNN-Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D\n",
        "# from keras_contrib.layers import CRF\n",
        "# from keras_contrib.utils import save_load_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_tf_idf():\n",
        "    train = load_dataset(\"train.txt\")\n",
        "    # test = load_dataset(\"test.txt\")\n",
        "    # valid = load_dataset(\"valid.txt\")\n",
        "    # return train, test, valid\n",
        "    return train, None, None\n",
        "\n",
        "def preprocess_tfidf(data:Data):\n",
        "    preprocessing = Preprocessing(data=data)\n",
        "    preprocessing.lowercasing()\n",
        "    preprocessing.lemmatize()\n",
        "    preprocessing.remove_stopword()\n",
        "    data.unicity()\n",
        "    # sentences = [\" \".join(sentence) for sentence in data.sentences_num]\n",
        "    # vectorizer = TfidfVectorizer(max_features=Data.MAX_LENGTH)\n",
        "    # data.x = vectorizer.fit_transform(sentences).toarray()\n",
        "    # y = []\n",
        "    # [[y.append(to_categorical(Data.unique_ner_tags[tag], num_classes=NUM_CLASSES)) for tag in tags] for tags in data.ner_tags]\n",
        "    # data.y = np.array(y, dtype=\"float32\")\n",
        "\n",
        "def vectorize_tf_idf(data:Data):\n",
        "    vectorize = Vectorization(data=data)\n",
        "    print(data.x)\n",
        "    vectorize.padding_x(value=\"<PAD>\", dtype=\"str\")\n",
        "    print(data.x)\n",
        "    vectorize.padding_y(value=\"O\")\n",
        "\n",
        "def formalize_tfidf(data:Data):\n",
        "    data.x = data.sentences_num\n",
        "    data.y = data.ner_tags_num\n",
        "    x = len(data.x)\n",
        "    return [\" \".join([\"\".join([data.x[i][j], \"__\", data.y[i][j]]) for j in range(len(data.x[i]))]) for i in range(x)]\n",
        "\n",
        "def tf(train:Data, test:Data, valid:Data):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    data_train = formalize_tfidf(train)\n",
        "    train.x = vectorizer.fit_transform(data_train).toarray()\n",
        "    print(train.x.shape)\n",
        "    # test.x = vectorizer.transform(test.x).toarray()\n",
        "    # valid.x = vectorizer.transform(valid.x).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(14041, 19159)\n"
          ]
        }
      ],
      "source": [
        "class TF_IDF:\n",
        "  def __init__(self):\n",
        "    self.train, self.test, self.valid = load_tf_idf()\n",
        "    # # Preprocessing\n",
        "    preprocess_tfidf(self.train)\n",
        "    # preprocess_tfidf(self.test)\n",
        "    # preprocess_tfidf(self.valid)\n",
        "    # # Vectorization\n",
        "    # vectorize_tf_idf(self.train)\n",
        "    # vectorize_tf_idf(self.test)\n",
        "    # vectorize_tf_idf(self.valid)\n",
        "    tf(self.train, self.test, self.valid)\n",
        "  def training(self):\n",
        "    pass\n",
        "  def testing(self):\n",
        "    pass\n",
        "  def evaluation(self):\n",
        "    pass\n",
        "    # evaluation(test, y_predict)\n",
        "tfidf = TF_IDF()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
