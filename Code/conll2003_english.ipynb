{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ-tKnxA74KU",
        "outputId": "a3be3012-9198-4684-baf6-d9ea45f58cf3"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/taslimamindia/NERC.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fcvRYZ97fmq"
      },
      "source": [
        "# Importation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "tou31yST7fmt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from nltk import word_tokenize, sent_tokenize, download\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.layers import Embedding\n",
        "from keras.utils import to_categorical, pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ9h38D29wUb",
        "outputId": "845fddfd-81a4-4f08-e373-6cb620e91357"
      },
      "outputs": [],
      "source": [
        "# download('wordnet') # for google colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkK1C7mp7fmu"
      },
      "source": [
        "# Class define form data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "tsvoLVPY7fmu"
      },
      "outputs": [],
      "source": [
        "class Data(object):\n",
        "    unique_words = {\"<PAD>\":0}\n",
        "    unique_ner_tags = {\"O\":0}\n",
        "    MAX_LENGTH = 50\n",
        "    VOCAB_SIZE = 100\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.sentences = []\n",
        "        self.sentences_num = None\n",
        "        self.ner_tags = []\n",
        "        self.ner_tags_num = None\n",
        "        self.chunk_tags = []\n",
        "        self.pos_tags = []\n",
        "        self.x, self.y = None, None\n",
        "    def word2vec(self, vector_size=100):\n",
        "        VOCAB_SIZE = vector_size\n",
        "        word2vec_model = Word2Vec(self.sentences, vector_size=vector_size, window=5, min_count=1, workers=4)\n",
        "        return word2vec_model   \n",
        "    def word2idx(self, word:str):\n",
        "        return Data.unique_words.get(word, None)\n",
        "    def idx2word(self, index:int):\n",
        "        for word, value in Data.unique_words.items():\n",
        "            if index is value: return word\n",
        "        return None    \n",
        "    def tag2idx(self, tag):\n",
        "        return Data.unique_ner_tags.get(tag, None)\n",
        "    def idx2tag(self, index):\n",
        "        for tag, value in Data.unique_ner_tags.items():\n",
        "            if index == value: return tag\n",
        "        return None\n",
        "    def unicity(self):\n",
        "        unique_sent, unique_tag = set(), set()\n",
        "        [unique_tag.update(tags) for tags in self.ner_tags_num]\n",
        "        [unique_sent.update(tags) for tags in self.sentences_num]\n",
        "        max_tags = len(Data.unique_ner_tags)\n",
        "        max_words = len(Data.unique_words)\n",
        "        for word in list(unique_sent):\n",
        "            if Data.unique_words.get(word, None) == None:\n",
        "                Data.unique_words[word] = max_words\n",
        "                max_words += 1\n",
        "        for tag in list(unique_tag):\n",
        "            if Data.unique_ner_tags.get(tag, None) == None:\n",
        "                Data.unique_ner_tags[tag] = max_tags\n",
        "                max_tags += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxLQFuLO7fmv"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "NzjCn5tW7fmv"
      },
      "outputs": [],
      "source": [
        "class Loading():\n",
        "    def __init__(self, data: Data, file):\n",
        "        self.data = data\n",
        "        self.load_sentences(file)\n",
        "    def load_sentences(self, filepath):\n",
        "        tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                if (line == ('-DOCSTART- -X- -X- O\\n') or line == '\\n'):\n",
        "                    if len(tokens) > 0:\n",
        "                        self.data.sentences.append(tokens)\n",
        "                        self.data.pos_tags.append(pos_tags)\n",
        "                        self.data.chunk_tags.append(chunk_tags)\n",
        "                        self.data.ner_tags.append(ner_tags)\n",
        "                        tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
        "                else:\n",
        "                    l = line.split(' ')\n",
        "                    tokens.append(l[0])\n",
        "                    pos_tags.append(l[1])\n",
        "                    chunk_tags.append(l[2])\n",
        "                    ner_tags.append(l[3].strip('\\n'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9vvnsDR7fmv"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "3Rbc4nVU7fmw"
      },
      "outputs": [],
      "source": [
        "class Preprocessing():\n",
        "    def __init__(self, data:Data, text=None, lang=\"english\"):\n",
        "        self.data = data\n",
        "        self.text = text\n",
        "        self.lang = lang\n",
        "        if text == None:\n",
        "          self.data.sentences_num = self.data.sentences\n",
        "          self.data.ner_tags_num = self.data.ner_tags\n",
        "    def tokenize(self):\n",
        "        if self.text != None:\n",
        "            sentenses = [word_tokenize(sentence, language=self.lang) for sentence in sent_tokenize(self.text, language=self.lang)]\n",
        "            self.data.sentences = [[token for token in sentence if token not in stopwords.words(self.lang)] for sentence in sentenses]\n",
        "            self.data.sentences_num = self.data.sentences\n",
        "    def remove_stopword(self):\n",
        "        sentences = []\n",
        "        for i in range(len(self.data.sentences)):\n",
        "            sentence = []\n",
        "            for j in range(len(self.data.sentences[i])):\n",
        "                if self.data.sentences[i][j] not in stopwords.words(self.lang):\n",
        "                    sentence.append((self.data.sentences[i][j], self.data.ner_tags[i][j]))\n",
        "            sentences.append(sentence)\n",
        "        self.data.sentences = [[token[0] for token in sentence ] for sentence in sentences]\n",
        "        self.data.ner_tags = [[tag[1] for tag in tags ] for tags in sentences]\n",
        "        self.data.sentences_num = self.data.sentences\n",
        "        self.data.ner_tags_num = self.data.ner_tags\n",
        "    def lowercasing(self):\n",
        "        self.data.sentences_num = [[word.lower() for word in sentence] for sentence in self.data.sentences_num]\n",
        "    def lemmatize(self):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        self.data.sentences_num = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in self.data.sentences_num]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0yl2FQm7fmw"
      },
      "source": [
        "# Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Arjdv32N7fmw"
      },
      "outputs": [],
      "source": [
        "class Vectorization():\n",
        "    def __init__(self, data:Data):\n",
        "        self.data = data\n",
        "    def word2vec(self, min_count=1, window=5):\n",
        "        word2vec_model = Word2Vec(self.data.sentences_num, min_count=min_count, vector_size=Data.VOCAB_SIZE, window=window)\n",
        "        self.data.sentences_num = [[word2vec_model.wv[word] for word in sentence] for sentence in self.data.sentences_num]\n",
        "    def padding_x(self):\n",
        "        if len(self.data.sentences_num) > 0:\n",
        "            self.data.x = pad_sequences(\n",
        "                sequences=self.data.sentences_num, \n",
        "                maxlen=self.data.MAX_LENGTH, \n",
        "                dtype=\"float32\", \n",
        "                padding=\"post\", \n",
        "                value=np.zeros((Data.VOCAB_SIZE,), dtype=\"float32\")\n",
        "            )\n",
        "    def vectorized_x(self):\n",
        "        self.word2vec()\n",
        "        self.padding_x()\n",
        "        \n",
        "    def padding_y(self):\n",
        "        if len(self.data.ner_tags_num) > 0:\n",
        "            self.data.ner_tags_num = [[Data.unique_ner_tags.get(tag) for tag in tags] for tags in self.data.ner_tags_num]            \n",
        "            self.data.ner_tags_num = pad_sequences(\n",
        "                sequences=self.data.ner_tags_num, \n",
        "                maxlen=self.data.MAX_LENGTH,\n",
        "                padding=\"post\", \n",
        "                dtype=\"str\",\n",
        "                value=Data.unique_ner_tags.get(\"O\")\n",
        "            )\n",
        "    def word2num(self):\n",
        "        x, y = self.data.ner_tags_num.shape\n",
        "        NUM_CLASSES = len(Data.unique_ner_tags)\n",
        "        self.data.y = np.zeros((x, y, NUM_CLASSES))\n",
        "        for i in range(x):\n",
        "            for j in range(y):\n",
        "                self.data.y[i][j] = to_categorical(self.data.ner_tags_num[i][j], num_classes=NUM_CLASSES)\n",
        "    def vectorized_y(self):\n",
        "        self.padding_y()\n",
        "        self.word2num()        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMdxz6YR7fmx"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuBmz4tX7fmx"
      },
      "source": [
        "## Pretraining for CONLL2003"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "IgS5eRZ07fmy"
      },
      "outputs": [],
      "source": [
        "def pretraining_CoNLL3(path: str):\n",
        "    data = Data()\n",
        "    base_file = \"../Data/conll2003_english/\"\n",
        "    # base_file = \"/content/NERC/Data/conll2003_english/\"\n",
        "    Loading(data = data, file=base_file + path)\n",
        "    return data\n",
        "\n",
        "def load(data:Data):\n",
        "    preprocessing = Preprocessing(data=data)\n",
        "    preprocessing.remove_stopword()\n",
        "    preprocessing.lowercasing()\n",
        "    preprocessing.lemmatize()\n",
        "    data.unicity()\n",
        "    \n",
        "def vectorize(data:Data):\n",
        "    vector = Vectorization(data=data)\n",
        "    vector.vectorized_x()\n",
        "    vector.vectorized_y()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Va_j-4a7fmy"
      },
      "source": [
        "## Define Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NFpti3H7fm0"
      },
      "source": [
        "## New input text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "_jGe-JDa7fm1"
      },
      "outputs": [],
      "source": [
        "# test_text = Data()\n",
        "\n",
        "# preprocessing = Preprocessing(data = test_text, text = \"Obama is the president of the United States. I am from Guinea, nice to meet you.\")\n",
        "# preprocessing.tokenize()\n",
        "# preprocessing.lowercasing()\n",
        "# preprocessing.lemmatize()\n",
        "# print(test_text.sentences)\n",
        "\n",
        "# vector = Vectorization(test_text)\n",
        "# vector.vectorized_x()\n",
        "# print(test_text.x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F08w-hsB7fm1"
      },
      "source": [
        "# Trainning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "-M4-ULUI7fm1"
      },
      "outputs": [],
      "source": [
        "NUM_WORDS = len(Data.unique_words)\n",
        "NUM_CLASSES = len(Data.unique_ner_tags)\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 100\n",
        "NUM_FILTERS = 256\n",
        "KERNEL_SIZE = 3\n",
        "DROPOUT_RATE = 0.5\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6IUaT76g8Ja"
      },
      "source": [
        "### CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "BXSF50RS7fm1"
      },
      "outputs": [],
      "source": [
        "# # from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D\n",
        "\n",
        "\n",
        "# # Build CNN model\n",
        "# model = Sequential()\n",
        "# model.add(Conv1D(64, KERNEL_SIZE, activation='relu', input_shape=(MAX_LENGTH, EMBEDDING_DIM), padding='same'))\n",
        "# # model.add(MaxPooling1D(2, padding='same'))\n",
        "# model.add(Dropout(DROPOUT_RATE))\n",
        "# model.add(Conv1D(32, KERNEL_SIZE, activation='relu', padding='same'))\n",
        "# # model.add(MaxPooling1D(2))\n",
        "# # model.add(Dropout(DROPOUT_RATE))\n",
        "# # model.add(Dense(HIDDEN_DIM, activation='relu'))\n",
        "# model.add(Dropout(DROPOUT_RATE))\n",
        "# model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "T1sV-mhtTjfu"
      },
      "outputs": [],
      "source": [
        "# !pip install tf2crf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "c0eEcWeJRv9G"
      },
      "outputs": [],
      "source": [
        "# # from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from keras.models import Model\n",
        "# from keras.layers import Dense, Conv1D\n",
        "# from tf2crf import CRF, ModelWithCRFLoss\n",
        "# from keras import Input\n",
        "\n",
        "# # Build CNN model\n",
        "# # model = Sequential()\n",
        "# inputs = Input(shape=(MAX_LENGTH, EMBEDDING_DIM))\n",
        "# outputs = Conv1D(64, KERNEL_SIZE, activation='relu', padding='same')(inputs)\n",
        "# # model.add(MaxPooling1D(2, padding='same'))\n",
        "# # outputs = Dropout(DROPOUT_RATE)(inputs)\n",
        "# outputs = Conv1D(32, KERNEL_SIZE, activation='relu', padding='same')(inputs)\n",
        "# # model.add(MaxPooling1D(2))\n",
        "# # model.add(Dropout(DROPOUT_RATE))\n",
        "# # model.add(Dense(HIDDEN_DIM, activation='relu'))\n",
        "# # outputs = Dropout(DROPOUT_RATE)(outputs)\n",
        "# outputs = Dense(NUM_CLASSES, activation='relu')(outputs)\n",
        "# # outputs.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# # outputs.summary()\n",
        "# crf = CRF(units=9)\n",
        "# # cnn_model.add(crf)\n",
        "# output = crf(outputs)\n",
        "# cnn_crf_model = Model(inputs, output)\n",
        "# cnn_crf_model.summary()\n",
        "# # cnn_crf_model = ModelWithCRFLoss(base_model, sparse_target=True)\n",
        "# # cnn_crf_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "CVZz_xMcRv9H"
      },
      "outputs": [],
      "source": [
        "# cnn_crf_model.compile(optimizer='adam')\n",
        "# cnn_crf_model.fit(train.x, train.y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(valid.x, valid.y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "HqUvOqaORv9H"
      },
      "outputs": [],
      "source": [
        "# # Evaluation\n",
        "# loss, accuracy = cnn_crf_model.evaluate(test.x, test.y, batch_size=BATCH_SIZE)\n",
        "\n",
        "# print('Test Loss:', loss)\n",
        "# print('Test Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "4KK0VLb6Rv9H"
      },
      "outputs": [],
      "source": [
        "# y_predict_cnn_crf = cnn_crf_model.predict(test.x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "GaJHDwE8Rv9I"
      },
      "outputs": [],
      "source": [
        "# y_predict_cnn_crf.shape\n",
        "# print(len(valid.unique_ner_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "SeA3oDJZ7fm2"
      },
      "outputs": [],
      "source": [
        "# cnn_crf_model.export(\"../data/model_cnn.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "wAddNuYK7fm2"
      },
      "outputs": [],
      "source": [
        "# cnn_model = tf.keras.models.load_model(\"model_cnn.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEctuVg7HirO"
      },
      "source": [
        "## Model LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "hW0Culvw7fm5"
      },
      "outputs": [],
      "source": [
        "from keras.backend import dropout\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "# import tensorflow_addons as tfa\n",
        "\n",
        "class Model_LSTM:\n",
        "  def __init__(self):\n",
        "    # Define the model architecture\n",
        "    self.model_LSTM = Sequential()\n",
        "    self.model_LSTM.add(LSTM(256, input_shape=(Data.MAX_LENGTH, Data.VOCAB_SIZE), return_sequences=True, dropout=0.5))\n",
        "    self.model_LSTM.add(LSTM(128, return_sequences=True, dropout=0.5))\n",
        "    self.model_LSTM.add(LSTM(64, return_sequences=True, dropout=0.5))\n",
        "    self.model_LSTM.add(LSTM(32, return_sequences=True, dropout=0.5))\n",
        "    self.model_LSTM.add(Dense(9, activation='softmax'))\n",
        "  def summary(self):\n",
        "    self.model_LSTM.summary()\n",
        "  def trainning(self, train:Data, valid:Data=None):\n",
        "    cat_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "    recall = tf.keras.metrics.Recall()\n",
        "    self.model_LSTM.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[cat_accuracy, recall])\n",
        "    if valid == None:\n",
        "      self.model_LSTM.fit(train.x, train.y, batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
        "    else:\n",
        "      self.model_LSTM.fit(train.x, train.y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(valid.x, valid.y))\n",
        "  def testing(self, test:Data):\n",
        "    return self.model_LSTM.evaluate(test.x, test.y)\n",
        "  def predicting(self, test:Data):\n",
        "    return self.model_LSTM.predict(test.x, batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "K7eHuf96nXDm"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Tqp60CTqGqzV"
      },
      "outputs": [],
      "source": [
        "# model_LSTM.save(\"../Data/model_lstm.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "itvrF6SvFmUg"
      },
      "outputs": [],
      "source": [
        "# model_LSTM = tf.keras.models.load_model(\"../Data/model_lstm.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "0QGhljfiC5gG"
      },
      "outputs": [],
      "source": [
        "# entities = dict(zip(Data.unique_ner_tags.keys(), [0 for i in range(len(Data.unique_ner_tags))]))\n",
        "# for tags in test.ner_tags:\n",
        "#     for tag in tags:\n",
        "#         entities[tag] += 1\n",
        "# is_entities = 0\n",
        "# is_not_entities = 0\n",
        "# for tag, nbr in entities.items():\n",
        "#     if tag != 'O': is_entities += nbr\n",
        "#     else: is_not_entities += nbr\n",
        "# print(entities)\n",
        "# print(is_entities, is_not_entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "06nR4t-2FmUh"
      },
      "outputs": [],
      "source": [
        "def evaluation(test:Data, y_predict_lstm):\n",
        "  true, false, total, predict = 0, 0, 0, 0\n",
        "  x, y, z = test.y.shape\n",
        "  for i in range(x):\n",
        "    for j in range(y):\n",
        "      real_tag = np.argmax(test.y[i][j]) \n",
        "      predict_tag = np.argmax(y_predict_lstm[i][j])\n",
        "      if predict_tag == 0: predict +=1\n",
        "      if real_tag != 0:\n",
        "        total = total + 1\n",
        "        if real_tag == predict_tag: true = true + 1\n",
        "        else: false = false + 1\n",
        "  print(\"----------------------- Evaluation -------------------------\")\n",
        "  print(test.y.shape)\n",
        "  print(predict, x*y)\n",
        "  print(true, false, total, round(true/total, 3), round(false/total, 3), end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "98BvT-KZ7fmy"
      },
      "outputs": [],
      "source": [
        "def loadData(param:dict):\n",
        "  dico = {\"params\":[], \"metrics\":[]}\n",
        "  if param.get(\"max_length\", 0) != 0:\n",
        "      max_lengths = param[\"max_length\"]\n",
        "      for max_length in max_lengths:   \n",
        "        Data.MAX_LENGTH = max_length     \n",
        "        train = pretraining_CoNLL3(\"train.txt\")\n",
        "        test = pretraining_CoNLL3(\"test.txt\")\n",
        "        valid = pretraining_CoNLL3(\"valid.txt\")\n",
        "        load(train)\n",
        "        load(test)\n",
        "        load(valid)\n",
        "        vectorize(train)\n",
        "        vectorize(test)\n",
        "        vectorize(valid)\n",
        "        model_lstm = Model_LSTM()\n",
        "        model_lstm.trainning(train, valid)\n",
        "        model_lstm.testing(test)\n",
        "        y_predict_lstm = model_lstm.predicting(test)\n",
        "        evaluation(test, y_predict_lstm)\n",
        "        return train, test, valid\n",
        "        \n",
        "\n",
        "def checkDataset(train, test, valid):    \n",
        "    print(\"X_train\", train.x.shape)\n",
        "    print(\"y_train\", train.y.shape, \"\\n\")\n",
        "    print(\"X_test\", test.x.shape)\n",
        "    print(\"y_test\", test.y.shape, \"\\n\")    \n",
        "    print(\"X_valid\", valid.x.shape)\n",
        "    print(\"y_valid\", valid.y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chYF5-l2CvEB",
        "outputId": "ff3fa125-db25-42d6-e727-39a4899e4026"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32me:\\PFE\\CoNLL2003\\NERC\\Code\\conll2003_english.ipynb Cell 39\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train, test, valid \u001b[39m=\u001b[39m loadData({\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m:[\u001b[39m100\u001b[39m]})\n",
            "\u001b[1;32me:\\PFE\\CoNLL2003\\NERC\\Code\\conll2003_english.ipynb Cell 39\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m valid \u001b[39m=\u001b[39m pretraining_CoNLL3(\u001b[39m\"\u001b[39m\u001b[39mvalid.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m load(train)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m load(test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m load(valid)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m vectorize(train)\n",
            "\u001b[1;32me:\\PFE\\CoNLL2003\\NERC\\Code\\conll2003_english.ipynb Cell 39\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(data:Data):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     preprocessing \u001b[39m=\u001b[39m Preprocessing(data\u001b[39m=\u001b[39mdata)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     preprocessing\u001b[39m.\u001b[39;49mremove_stopword()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     preprocessing\u001b[39m.\u001b[39mlowercasing()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     preprocessing\u001b[39m.\u001b[39mlemmatize()\n",
            "\u001b[1;32me:\\PFE\\CoNLL2003\\NERC\\Code\\conll2003_english.ipynb Cell 39\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m sentence \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39msentences[i])):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39msentences[i][j] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlang):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         sentence\u001b[39m.\u001b[39mappend((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39msentences[i][j], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mner_tags[i][j]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/NERC/Code/conll2003_english.ipynb#Y144sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m sentences\u001b[39m.\u001b[39mappend(sentence)\n",
            "File \u001b[1;32mc:\\Pythons\\Python39\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:23\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords\u001b[39m(\u001b[39mself\u001b[39m, fileids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ignore_lines_startswith\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     22\u001b[0m         line\n\u001b[1;32m---> 23\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m line_tokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw(fileids))\n\u001b[0;32m     24\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     25\u001b[0m     ]\n",
            "File \u001b[1;32mc:\\Pythons\\Python39\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:32\u001b[0m, in \u001b[0;36mWordListCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(fileids, \u001b[39mstr\u001b[39m):\n\u001b[0;32m     31\u001b[0m     fileids \u001b[39m=\u001b[39m [fileids]\n\u001b[1;32m---> 32\u001b[0m \u001b[39mreturn\u001b[39;00m concat([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen(f)\u001b[39m.\u001b[39mread() \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fileids])\n",
            "File \u001b[1;32mc:\\Pythons\\Python39\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:32\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(fileids, \u001b[39mstr\u001b[39m):\n\u001b[0;32m     31\u001b[0m     fileids \u001b[39m=\u001b[39m [fileids]\n\u001b[1;32m---> 32\u001b[0m \u001b[39mreturn\u001b[39;00m concat([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopen(f)\u001b[39m.\u001b[39mread() \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fileids])\n",
            "File \u001b[1;32mc:\\Pythons\\Python39\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:208\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding(file)\n\u001b[1;32m--> 208\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_root\u001b[39m.\u001b[39;49mjoin(file)\u001b[39m.\u001b[39;49mopen(encoding)\n\u001b[0;32m    209\u001b[0m \u001b[39mreturn\u001b[39;00m stream\n",
            "File \u001b[1;32mc:\\Pythons\\Python39\\lib\\site-packages\\nltk\\data.py:329\u001b[0m, in \u001b[0;36mFileSystemPathPointer.open\u001b[1;34m(self, encoding)\u001b[0m\n\u001b[0;32m    327\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    328\u001b[0m \u001b[39mif\u001b[39;00m encoding \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 329\u001b[0m     stream \u001b[39m=\u001b[39m SeekableUnicodeStreamReader(stream, encoding)\n\u001b[0;32m    330\u001b[0m \u001b[39mreturn\u001b[39;00m stream\n",
            "File \u001b[1;32mc:\\Pythons\\Python39\\lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decorator\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[39m=\u001b[39m (args[\u001b[39m0\u001b[39m], add_py3_data(args[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m args[\u001b[39m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m init_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Pythons\\Python39\\lib\\site-packages\\nltk\\data.py:1038\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.__init__\u001b[1;34m(self, stream, encoding, errors)\u001b[0m\n\u001b[0;32m   1032\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rewind_numchars \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[39m\"\"\"The number of characters that have been returned since the\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[39m   read that started at ``_rewind_checkpoint``.  This is used,\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[39m   together with ``_rewind_checkpoint``, to backtrack to the\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[39m   beginning of ``linebuffer`` (which is required by ``tell()``).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1038\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bom \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_bom()\n\u001b[0;32m   1039\u001b[0m \u001b[39m\"\"\"The length of the byte order marker at the beginning of\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[39m   the stream (or None for no byte order marker).\"\"\"\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Pythons\\Python39\\lib\\site-packages\\nltk\\data.py:1405\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader._check_bom\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1401\u001b[0m bom_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_BOM_TABLE\u001b[39m.\u001b[39mget(enc)\n\u001b[0;32m   1403\u001b[0m \u001b[39mif\u001b[39;00m bom_info:\n\u001b[0;32m   1404\u001b[0m     \u001b[39m# Read a prefix, to check against the BOM(s)\u001b[39;00m\n\u001b[1;32m-> 1405\u001b[0m     \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mread(\u001b[39m16\u001b[39;49m)\n\u001b[0;32m   1406\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n\u001b[0;32m   1408\u001b[0m     \u001b[39m# Check for each possible BOM.\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train, test, valid = loadData({\"max_length\":[100]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "8x5O84rzHU9m"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(66, 2),\n",
              " (71, 2),\n",
              " (72, 2),\n",
              " (124, 2),\n",
              " (80, 2),\n",
              " (59, 2),\n",
              " (77, 2),\n",
              " (50, 2),\n",
              " (62, 2),\n",
              " (78, 2),\n",
              " (96, 2),\n",
              " (109, 2),\n",
              " (93, 2),\n",
              " (49, 2),\n",
              " (52, 2),\n",
              " (91, 2),\n",
              " (105, 2),\n",
              " (83, 2),\n",
              " (79, 2),\n",
              " (42, 3),\n",
              " (48, 3),\n",
              " (64, 3),\n",
              " (45, 3),\n",
              " (46, 3),\n",
              " (51, 4),\n",
              " (43, 4),\n",
              " (44, 4),\n",
              " (41, 5),\n",
              " (39, 6),\n",
              " (40, 7),\n",
              " (38, 11),\n",
              " (36, 17),\n",
              " (37, 18),\n",
              " (35, 26),\n",
              " (34, 29),\n",
              " (33, 40),\n",
              " (32, 58),\n",
              " (31, 85),\n",
              " (30, 86),\n",
              " (29, 110),\n",
              " (28, 134),\n",
              " (27, 198),\n",
              " (26, 218),\n",
              " (1, 239),\n",
              " (25, 263),\n",
              " (24, 296),\n",
              " (23, 335),\n",
              " (22, 404),\n",
              " (21, 411),\n",
              " (20, 438),\n",
              " (19, 459),\n",
              " (17, 481),\n",
              " (18, 503),\n",
              " (16, 539),\n",
              " (15, 607),\n",
              " (14, 613),\n",
              " (13, 629),\n",
              " (12, 659),\n",
              " (11, 700),\n",
              " (10, 799),\n",
              " (3, 1026),\n",
              " (5, 1122),\n",
              " (6, 1173),\n",
              " (4, 1294),\n",
              " (9, 1324),\n",
              " (2, 1719),\n",
              " (8, 1814),\n",
              " (7, 1852)]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# np.quantile(sort([1, 2, 3, 8, 7]), 0.50)\n",
        "dico = {}\n",
        "for tags in test.sentences + train.sentences + valid.sentences:\n",
        "  if dico.get(len(tags), None) == None:\n",
        "    dico[len(tags)] = 1\n",
        "  dico[len(tags)] += 1\n",
        "sorted(list(dico.items()), key= lambda x: x[1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
