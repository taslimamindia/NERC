{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWBc46_ZTK5u"
      },
      "source": [
        "# Clone repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ-tKnxA74KU"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/taslimamindia/NERC.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqE9J52MZL3v"
      },
      "outputs": [],
      "source": [
        "# !pip install pyunpack\n",
        "# !pip install patool\n",
        "# !pip install unrar"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3fcvRYZ97fmq"
      },
      "source": [
        "# Importation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tou31yST7fmt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.utils import to_categorical, pad_sequences\n",
        "\n",
        "from keras import Model, Input\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, concatenate\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kqqWJefmZL3w"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "from pickle import dump, load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2MGeSev0ZL3y"
      },
      "outputs": [],
      "source": [
        "import pyunpack \n",
        "import unrar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ9h38D29wUb",
        "outputId": "babfea22-703f-43c3-b193-98428b0c401a"
      },
      "outputs": [],
      "source": [
        "# from nltk import download\n",
        "# download('wordnet')\n",
        "# download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMfHyYuHTDud"
      },
      "source": [
        "# Base Class "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkK1C7mp7fmu"
      },
      "source": [
        "## Class define form data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tsvoLVPY7fmu"
      },
      "outputs": [],
      "source": [
        "class Data(object):\n",
        "    unique_words = {\"<PAD>\": 0}\n",
        "    unique_ner_tags = {\"O\": 0}\n",
        "    unique_chunk_tags = {}\n",
        "    unique_pos_tags = {}\n",
        "    MAX_LENGTH = 50\n",
        "    VOCAB_SIZE = 100\n",
        "    PADDING_SIZE = 10\n",
        "    # Hyperparameters\n",
        "    NUM_FILTERS = 256\n",
        "    KERNEL_SIZE = 3\n",
        "    DROPOUT_RATE = 0.2\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 40\n",
        "\n",
        "    def __init__(self):\n",
        "        self.sentences = []\n",
        "        self.sentences_num = []\n",
        "        self.ner_tags = []\n",
        "        self.ner_tags_num = []\n",
        "        self.chunk_tags = []\n",
        "        self.pos_tags = []\n",
        "        self.features = []\n",
        "        self.x, self.y = None, None\n",
        "\n",
        "    def remove_attributes(self):\n",
        "        self.sentences_num = []\n",
        "        self.ner_tags_num = []\n",
        "        self.chunk_tags = []\n",
        "        self.pos_tags = []\n",
        "        self.features = []\n",
        "        self.x, self.y = None, None\n",
        "\n",
        "    def __add__(self, o):\n",
        "        data = Data()\n",
        "        data.sentences = self.sentences + o.sentences\n",
        "        data.sentences_num = self.sentences_num + o.sentences_num\n",
        "        data.ner_tags = self.ner_tags + o.ner_tags\n",
        "        data.ner_tags_num = self.ner_tags_num + o.ner_tags_num\n",
        "        data.chunk_tags = self.chunk_tags + o.chunk_tags\n",
        "        data.pos_tags = self.pos_tags + o.pos_tags\n",
        "        data.features = self.features + o.features\n",
        "        return data\n",
        "\n",
        "    def word2idx(self, word: str):\n",
        "        return Data.unique_words.get(word, None)\n",
        "\n",
        "    def idx2word(self, index: int):\n",
        "        for word, value in Data.unique_words.items():\n",
        "            if index is value:\n",
        "                return word\n",
        "        return None\n",
        "\n",
        "    def tag2idx(self, tag):\n",
        "        return Data.unique_ner_tags.get(tag, None)\n",
        "\n",
        "    def idx2tag(self, index):\n",
        "        for tag, value in Data.unique_ner_tags.items():\n",
        "            if index == value:\n",
        "                return tag\n",
        "        return None\n",
        "\n",
        "    def __unicity_tag(self, dico: dict, listes: list):\n",
        "        unique_word = set()\n",
        "        [unique_word.update(tags) for tags in listes]\n",
        "        max_index = len(dico)\n",
        "        for word in list(unique_word):\n",
        "            if dico.get(word, None) == None:\n",
        "                dico[word] = max_index\n",
        "                max_index += 1\n",
        "\n",
        "    def unicity(self):\n",
        "        self.__unicity_tag(Data.unique_ner_tags, self.ner_tags_num)\n",
        "        self.__unicity_tag(Data.unique_words, self.sentences_num)\n",
        "        self.__unicity_tag(Data.unique_chunk_tags, self.chunk_tags)\n",
        "        self.__unicity_tag(Data.unique_pos_tags, self.pos_tags)\n",
        "\n",
        "    def features_level(self):\n",
        "        def is_capitalize(word):\n",
        "            return len(word) > 1 and word[0].isupper() and word[1:].islower()\n",
        "        features = [\n",
        "            [\n",
        "                [is_capitalize(word), word.isupper(), word.islower(), word.istitle(), word.isdigit()]\n",
        "                for word in sentence\n",
        "            ]\n",
        "            for sentence in self.sentences\n",
        "        ]\n",
        "        self.features = [[[int(f) for f in feat] for feat in feature] for feature in features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVKzq06h5moN"
      },
      "source": [
        "## Use Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70grOo_p5moO"
      },
      "outputs": [],
      "source": [
        "class Model_Word2Vec:\n",
        "    def __init__(self, sentences):\n",
        "        self.model = Word2Vec(\n",
        "            sentences=sentences, min_count=1, vector_size=Data.VOCAB_SIZE, window=5\n",
        "        )\n",
        "\n",
        "    def wv(self, word):\n",
        "        return self.model.wv[word]\n",
        "\n",
        "def unziprar(path_rar, dest_dir):\n",
        "    pyunpack.Archive(path_rar).extractall(dest_dir, auto_create_dir=True)\n",
        "\n",
        "\n",
        "def serialization(data, path):\n",
        "    with open(path, \"wb\") as outfile:\n",
        "        dump(data, outfile)\n",
        "\n",
        "\n",
        "def deserialization(path):\n",
        "    with open(path, \"rb\") as infile:\n",
        "        data = load(infile)\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_data(data:Data, path, name):\n",
        "    data.x = np.load(file=path + name + \"_x.npy\")\n",
        "    data.features = np.load(file=path + name + \"_features.npy\")\n",
        "    data.y = np.load(file=path + name + \"_y.npy\")\n",
        "\n",
        "\n",
        "def save_data(data: Data, path, name):\n",
        "    np.save(file=path + name + \"_x.npy\", arr=data.x)\n",
        "    np.save(file=path + name + \"_features.npy\", arr=data.features)\n",
        "    np.save(file=path + name + name + \"_y.npy\", arr=data.y)\n",
        "\n",
        "\n",
        "def unformat_for_splitting(data: np.ndarray, initial_size):\n",
        "    x, y, z = data.shape\n",
        "    if initial_size < z:\n",
        "        y_initial = y - 1\n",
        "        X1, X2 = np.zeros(shape=(x, y_initial, z)), np.zeros(shape=(x, initial_size))\n",
        "        for i in range(x):\n",
        "            # X1[i], X2[i] = np.vsplit()\n",
        "            X1[i], X2[i] = data[i][:y_initial], data[i][y_initial][:initial_size]\n",
        "    else:\n",
        "        raise Exception(\"Initial_size must be larger than z.\")\n",
        "    return X1, X2\n",
        "\n",
        "\n",
        "def format_for_splitting(*args):\n",
        "    X1, X2 = args[0], args[1]\n",
        "    x, y, z = X1.shape\n",
        "    n = pad_sequences(X2, maxlen=z, padding=\"post\", value=0)\n",
        "    result = np.zeros(shape=(x, y + 1, z))\n",
        "    for i in range(x):\n",
        "        result[i] = np.vstack((X1[i], n[i]))\n",
        "    return result\n",
        "\n",
        "\n",
        "def string2num(lists, unique_word):\n",
        "    return [unique_word.get(l) for l in lists]\n",
        "\n",
        "\n",
        "def flatting(sentences):\n",
        "    return [word for sentence in sentences for word in sentence]\n",
        "\n",
        "\n",
        "def margin(sentences, batch_size):\n",
        "    \"\"\"Permet d'ajouter du marge sur les bords.\n",
        "\n",
        "    Args:\n",
        "        sentences (list[list]): _description_\n",
        "        batch_size (int): _description_\n",
        "\n",
        "    Returns:\n",
        "        list[list]: _description_\n",
        "\n",
        "    Example:\n",
        "        input: ['Peter', 'Blackburn']\n",
        "        output ['<pad>', 'Blackburn', 'Peter', 'Blackburn', '<pad>']\n",
        "    \"\"\"\n",
        "    batch_size = batch_size + 1\n",
        "    b_size = int(batch_size // 2)\n",
        "    pad = [np.zeros(shape=sentences[0][0].shape)]\n",
        "\n",
        "    def __pad(sentence: list):\n",
        "        n = len(sentence)\n",
        "        if n <= b_size:\n",
        "            sentence = sentence + pad * (b_size - n + 1)\n",
        "            n = len(sentence)\n",
        "        sentence = (\n",
        "            list(reversed(sentence[1 : b_size + 1]))\n",
        "            + sentence\n",
        "            + list(reversed(sentence[n - b_size - 1 : n - 1]))\n",
        "        )\n",
        "        n = len(sentence)\n",
        "        Sentences = []\n",
        "        for i in range(b_size, n - b_size):\n",
        "            Sentences.append(\n",
        "                np.array(sentence[i - b_size : i + b_size + 1][1:], dtype=\"float32\")\n",
        "            )\n",
        "\n",
        "        return Sentences\n",
        "\n",
        "    Sentences = [__pad(sentence.copy()) for sentence in sentences]\n",
        "    Sentences = [\n",
        "        [Sentences[i][j] for j in range(len(sentences[i]))]\n",
        "        for i in range(len(sentences))\n",
        "    ]\n",
        "    return Sentences\n",
        "\n",
        "\n",
        "def zip_2D(*args):\n",
        "    zipdata = list(zip(args[0], args[1], args[2], args[3], args[4]))\n",
        "    zipdata = [\n",
        "        list(zip(data[0], data[1], data[2], data[3], data[4])) for data in zipdata\n",
        "    ]\n",
        "    return zipdata\n",
        "\n",
        "\n",
        "def unzip_2D(args):\n",
        "    words, ners, chunks, poss, features = [], [], [], [], []\n",
        "    for arg in args:\n",
        "        word, ner, chunk, pos, feature = [], [], [], [], []\n",
        "        for triple in arg:\n",
        "            word.append(triple[0])\n",
        "            ner.append(triple[1])\n",
        "            chunk.append(triple[2])\n",
        "            pos.append(triple[3])\n",
        "            feature.append(triple[4])\n",
        "\n",
        "        words.append(word)\n",
        "        ners.append(ner)\n",
        "        chunks.append(chunk)\n",
        "        poss.append(pos)\n",
        "        features.append(feature)\n",
        "    return words, ners, chunks, poss, features\n",
        "\n",
        "\n",
        "def padding(data: Data):\n",
        "    data.flatten()\n",
        "    data.x = data.sentences_num\n",
        "    data.y = data.ner_tags\n",
        "    data.gather()\n",
        "    data.sentences_num = data.x\n",
        "    data.ner_tags_num = data.y\n",
        "\n",
        "\n",
        "def evaluation(y_true, y_predict):\n",
        "    true, false, total, predict = 0, 0, 0, 0\n",
        "    x, y = y_true.shape\n",
        "    for i in range(x):\n",
        "        real_tag = np.argmax(y_true[i])\n",
        "        predict_tag = np.argmax(y_predict[i])\n",
        "        if predict_tag == 0:\n",
        "            predict += 1\n",
        "        if real_tag != 0:\n",
        "            total = total + 1\n",
        "            if real_tag == predict_tag:\n",
        "                true = true + 1\n",
        "            else:\n",
        "                false = false + 1\n",
        "    print(\"----------------------- Evaluation -------------------------\")\n",
        "    print(y_true.shape)\n",
        "    print(predict, x)\n",
        "    print(\n",
        "        true, false, total, round(true / total, 3), round(false / total, 3), end=\"\\n\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "def checkDataset(train, test, valid):\n",
        "    if train != None:\n",
        "        print(\"X_train\", train.x.shape, \"y_train\", train.y.shape, \"\\n\")\n",
        "    if test != None:\n",
        "        print(\"X_test\", test.x.shape, \"y_test\", test.y.shape, \"\\n\")\n",
        "    if valid != None:\n",
        "        print(\"X_valid\", valid.x.shape, \"y_valid\", valid.y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxLQFuLO7fmv"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NzjCn5tW7fmv"
      },
      "outputs": [],
      "source": [
        "class Loading():\n",
        "    def __init__(self, path):\n",
        "        if os.path.exists(\"../Data/conll2003_english/\"): \n",
        "            base_file = \"../Data/conll2003_english/\"\n",
        "        else:\n",
        "            base_file = \"/content/NERC/Data/conll2003_english/\"        \n",
        "        self.data = Data()\n",
        "        self.load_sentences(base_file + path)\n",
        "    def load_sentences(self, filepath):\n",
        "        tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                if (line == ('-DOCSTART- -X- -X- O\\n') or line == '\\n'):\n",
        "                    if len(tokens) > 0:\n",
        "                        self.data.sentences.append(tokens)\n",
        "                        self.data.pos_tags.append(pos_tags)\n",
        "                        self.data.chunk_tags.append(chunk_tags)\n",
        "                        self.data.ner_tags.append(ner_tags)\n",
        "                        tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
        "                else:\n",
        "                    l = line.split(' ')\n",
        "                    tokens.append(l[0])\n",
        "                    pos_tags.append(l[1])\n",
        "                    chunk_tags.append(l[2])\n",
        "                    ner_tags.append(l[3].strip('\\n'))\n",
        "            self.data.sentences_num = self.data.sentences\n",
        "            self.data.ner_tags_num = self.data.ner_tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9vvnsDR7fmv"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3Rbc4nVU7fmw"
      },
      "outputs": [],
      "source": [
        "class Preprocessing:\n",
        "    def __init__(self, data: Data, text=None, lang=\"english\"):\n",
        "        self.data = data\n",
        "        self.text = text\n",
        "        self.lang = lang\n",
        "\n",
        "    def tokenize(self):\n",
        "        if self.text != None:\n",
        "            sentenses = [\n",
        "                word_tokenize(sentence, language=self.lang)\n",
        "                for sentence in sent_tokenize(self.text, language=self.lang)\n",
        "            ]\n",
        "            self.data.sentences = [\n",
        "                [token for token in sentence if token not in stopwords.words(self.lang)]\n",
        "                for sentence in sentenses\n",
        "            ]\n",
        "            self.data.sentences_num = self.data.sentences\n",
        "\n",
        "    def lowercasing(self):\n",
        "        self.data.sentences_num = [\n",
        "            [word.lower() for word in sentence] for sentence in self.data.sentences_num\n",
        "        ]\n",
        "\n",
        "    def lemmatize(self):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        self.data.sentences_num = [\n",
        "            [lemmatizer.lemmatize(word) for word in sentence]\n",
        "            for sentence in self.data.sentences_num\n",
        "        ]\n",
        "\n",
        "    def remove_stopword(self):\n",
        "        punctuation = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
        "        punctuations = stopwords.words(self.lang) + punctuation\n",
        "        sentences = zip_2D(\n",
        "            self.data.sentences_num,\n",
        "            self.data.ner_tags_num,\n",
        "            self.data.chunk_tags,\n",
        "            self.data.pos_tags,\n",
        "            self.data.features\n",
        "        )\n",
        "        sentences = [\n",
        "            [\n",
        "                triple\n",
        "                for triple in sentence\n",
        "                if triple[0] not in punctuations or triple[1] != \"O\"\n",
        "            ]\n",
        "            for sentence in sentences\n",
        "        ]\n",
        "        (\n",
        "            self.data.sentences_num,\n",
        "            self.data.ner_tags_num,\n",
        "            self.data.chunk_tags,\n",
        "            self.data.pos_tags,\n",
        "            self.data.features\n",
        "        ) = unzip_2D(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0yl2FQm7fmw"
      },
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Arjdv32N7fmw"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Model_Word2Vec' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mVectorization\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     \u001b[39m# path = \"E:\\\\word2vec\\\\gensim-data\\\\word2vec-google-news-300\\\\GoogleNews-vectors-negative300.bin\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[39m# word2vec_model = KeyedVectors.load_word2vec_format(path, binary=True)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data: Data, word2vec_model: Model_Word2Vec):\n\u001b[0;32m      6\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m data\n",
            "Cell \u001b[1;32mIn[8], line 5\u001b[0m, in \u001b[0;36mVectorization\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mVectorization\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     \u001b[39m# path = \"E:\\\\word2vec\\\\gensim-data\\\\word2vec-google-news-300\\\\GoogleNews-vectors-negative300.bin\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[39m# word2vec_model = KeyedVectors.load_word2vec_format(path, binary=True)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data: Data, word2vec_model: Model_Word2Vec):\n\u001b[0;32m      6\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m data\n\u001b[0;32m      7\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword2vec_model \u001b[39m=\u001b[39m word2vec_model\n",
            "\u001b[1;31mNameError\u001b[0m: name 'Model_Word2Vec' is not defined"
          ]
        }
      ],
      "source": [
        "class Vectorization:\n",
        "    # path = \"E:\\\\word2vec\\\\gensim-data\\\\word2vec-google-news-300\\\\GoogleNews-vectors-negative300.bin\"\n",
        "    # word2vec_model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
        "\n",
        "    def __init__(self, data: Data, word2vec_model: Model_Word2Vec):\n",
        "        self.data = data\n",
        "        self.word2vec_model = word2vec_model\n",
        "        self.features = Data()\n",
        "\n",
        "    def word2vec(self):\n",
        "        Sentences = []\n",
        "        for sentence in self.data.sentences_num:\n",
        "            Sentence = []\n",
        "            for word in sentence:\n",
        "                try:\n",
        "                    # Sentence.append(self.word2vec_model.get_vector(word))\n",
        "                    Sentence.append(self.word2vec_model.wv(word))\n",
        "                except Exception as e:\n",
        "                    Sentence.append(self.word2vec_model.wv(word))\n",
        "            Sentences.append(Sentence)\n",
        "        self.data.sentences_num = Sentences\n",
        "\n",
        "    def padding_x(self):\n",
        "        self.data.x = pad_sequences(\n",
        "            sequences=self.data.sentences_num,\n",
        "            maxlen=self.data.MAX_LENGTH,\n",
        "            dtype=\"float32\",\n",
        "            padding=\"post\",\n",
        "            value=np.zeros((Data.VOCAB_SIZE,), dtype=\"float32\"),\n",
        "        )\n",
        "\n",
        "    def vectorized_x(self):\n",
        "        self.word2vec()\n",
        "        self.data.sentences_num = margin(\n",
        "            self.data.sentences_num, batch_size=Data.BATCH_SIZE\n",
        "        )\n",
        "        self.data.x = np.array(flatting(self.data.sentences_num), dtype=\"float32\")\n",
        "        # self.padding_x()\n",
        "\n",
        "    def tag2num(self):\n",
        "        self.data.ner_tags_num = [\n",
        "            [Data.unique_ner_tags.get(tag) for tag in tags]\n",
        "            for tags in self.data.ner_tags_num\n",
        "        ]\n",
        "\n",
        "    def num2oneHotEncoding(self):\n",
        "        NUM_CLASSES = 9\n",
        "        self.data.ner_tags_num = [\n",
        "            to_categorical(tags, num_classes=NUM_CLASSES)\n",
        "            for tags in self.data.ner_tags_num\n",
        "        ]\n",
        "\n",
        "    def padding_y(self):\n",
        "        self.data.y = pad_sequences(\n",
        "            sequences=self.data.ner_tags_num,\n",
        "            maxlen=self.data.MAX_LENGTH,\n",
        "            padding=\"post\",\n",
        "            dtype=\"float32\",\n",
        "            value=to_categorical(\n",
        "                Data.unique_ner_tags.get(\"O\"), num_classes=9\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def vectorized_y(self):\n",
        "        self.tag2num()\n",
        "        self.num2oneHotEncoding()\n",
        "        self.data.y = np.array(flatting(self.data.ner_tags_num), dtype=\"float32\")\n",
        "        # self.padding_y()\n",
        "\n",
        "    def __scaled(self, df: pd.DataFrame):\n",
        "        # copy the data\n",
        "        df_min_max_scaled = df.copy()\n",
        "        # apply normalization techniques\n",
        "        for column in df_min_max_scaled.columns:\n",
        "            df_min_max_scaled[column] = (\n",
        "                df_min_max_scaled[column] - df_min_max_scaled[column].min()\n",
        "            ) / (df_min_max_scaled[column].max() - df_min_max_scaled[column].min())\n",
        "        return df_min_max_scaled\n",
        "\n",
        "    def get_additional_features(self):\n",
        "        chunks = string2num(flatting(self.data.chunk_tags), Data.unique_chunk_tags)\n",
        "        poss = string2num(flatting(self.data.pos_tags), Data.unique_pos_tags)\n",
        "        features = flatting(self.data.features)\n",
        "        df_tags = pd.DataFrame({\"chunk_tags\": chunks, \"pos_tags\": poss})\n",
        "        df_features = pd.DataFrame(\n",
        "            data=features,\n",
        "            columns=[\"is_capitalize\", \"isupper\", \"islower\", \"istitle\", \"isdigit\"],\n",
        "        )\n",
        "        df = pd.concat((df_tags, df_features), axis=1)\n",
        "        self.data.features = self.__scaled(df).to_numpy(dtype=\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMdxz6YR7fmx"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-2jPEd6TZ6I"
      },
      "source": [
        "## Visalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPCrQ6VwmV03"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SGCDK7Nxmb3f"
      },
      "outputs": [],
      "source": [
        "train = Loading(\"train.txt\").data\n",
        "test = Loading(\"test.txt\").data\n",
        "valid = Loading(\"valid.txt\").data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HVvfZZfsmfIv"
      },
      "outputs": [],
      "source": [
        "class Visualisation:\n",
        "    def __init__(self, train, test=None, valid=None, pos: int=0, dim: int = 0):\n",
        "        X, y = None, None\n",
        "        if dim == 0:\n",
        "            if pos == 0:\n",
        "                Sentences = train.sentences + test.sentences + valid.sentences\n",
        "                Tags = train.ner_tags + test.ner_tags + valid.ner_tags\n",
        "            elif pos == 1:\n",
        "                Sentences = train.sentences_num + test.sentences_num + valid.sentences_num\n",
        "                Tags = train.ner_tags_num + test.ner_tags_num + valid.ner_tags_num\n",
        "            X = [word for sentence in Sentences for word in sentence]\n",
        "            y = [tag for tags in Tags for tag in tags]\n",
        "            self.df = pd.DataFrame({\"word\": X, \"label\": y})\n",
        "        else:\n",
        "            if pos == 0:\n",
        "                X, y = train[0], train[1]\n",
        "                self.df = pd.DataFrame({\"word\": np.zeros((X.shape[0])), \"label\": y})\n",
        "            elif pos == 1:\n",
        "                X, y = flatting(train[0]), flatting(train[1])\n",
        "                self.df = pd.DataFrame({\"word\": X, \"label\": y})\n",
        "    def classNumber(self):\n",
        "        hist = self.df.groupby(\"label\").count()\n",
        "        return hist\n",
        "\n",
        "    def classNumberHistogram(self):\n",
        "        hist = self.df.groupby(\"label\").count()\n",
        "        hist = pd.DataFrame({\"class\":hist.index, \"count\": hist[\"word\"]})\n",
        "        hist = hist.to_numpy().tolist()\n",
        "        cl = [t[0] for t in hist]\n",
        "        nbr = [t[1] for t in hist]\n",
        "        sns.barplot(data=pd.DataFrame({\"class\":cl, \"count\":nbr}), x=\"class\", y=\"count\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VQ1XVZjL0Fbx"
      },
      "outputs": [],
      "source": [
        "visualisation = Visualisation(train, test, valid, pos=1, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NnUKp4IhONN7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>B-LOC</th>\n",
              "      <td>10645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B-MISC</th>\n",
              "      <td>5062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B-ORG</th>\n",
              "      <td>9323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B-PER</th>\n",
              "      <td>10059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-LOC</th>\n",
              "      <td>1671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-MISC</th>\n",
              "      <td>1717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-ORG</th>\n",
              "      <td>5290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I-PER</th>\n",
              "      <td>6991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>O</th>\n",
              "      <td>250660</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          word\n",
              "label         \n",
              "B-LOC    10645\n",
              "B-MISC    5062\n",
              "B-ORG     9323\n",
              "B-PER    10059\n",
              "I-LOC     1671\n",
              "I-MISC    1717\n",
              "I-ORG     5290\n",
              "I-PER     6991\n",
              "O       250660"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "visualisation.classNumber()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "m7hnKPtIpVbQ"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "visualisation.classNumberHistogram()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TEuNuFyqMrZ"
      },
      "outputs": [],
      "source": [
        "visualisation.df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOfYvGsH_C0H"
      },
      "outputs": [],
      "source": [
        "data = train + test + valid\n",
        "preprocess = Preprocessing(data=data)\n",
        "preprocess.remove_stopword()\n",
        "visualisation1 = Visualisation((data.sentences_num, data.ner_tags_num), pos=1, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYt8v5oi_C0H"
      },
      "outputs": [],
      "source": [
        "visualisation1.classNumber()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2YysjPc_C0J"
      },
      "outputs": [],
      "source": [
        "# visualisation1.classNumberHistogram()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NFpti3H7fm0"
      },
      "source": [
        "### New input text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jGe-JDa7fm1"
      },
      "outputs": [],
      "source": [
        "# test_text = Data()\n",
        "\n",
        "# preprocessing = Preprocessing(data = test_text, text = \"Obama is the president of the United States. I am from Guinea, nice to meet you.\")\n",
        "# preprocessing.tokenize()\n",
        "# preprocessing.lowercasing()\n",
        "# preprocessing.lemmatize()\n",
        "# print(test_text.sentences)\n",
        "\n",
        "# vector = Vectorization(test_text)\n",
        "# vector.vectorized_x()\n",
        "# print(test_text.x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npyM4czHULp-"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sikZydI_C0S"
      },
      "source": [
        "### Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_o-tb4i_C0T"
      },
      "outputs": [],
      "source": [
        "class Base_Model:\n",
        "    def __init__(self, train=True, test=True, valid=True):\n",
        "        if train:\n",
        "            self.train = Loading(\"train.txt\").data\n",
        "        else:\n",
        "            self.train = None\n",
        "        if test:\n",
        "            self.test = Loading(\"test.txt\").data\n",
        "        else:\n",
        "            self.test = None\n",
        "        if valid:\n",
        "            self.valid = Loading(\"valid.txt\").data\n",
        "        else:\n",
        "            self.valid = None\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def train_test_split(self):\n",
        "        initial_size = self.train.features.shape[1]\n",
        "        data = format_for_splitting(self.train.x, self.train.features)\n",
        "        x_train, x_test, self.train.y, self.test.y = train_test_split(\n",
        "            data, self.train.y, test_size=0.2\n",
        "        )\n",
        "        # x_train, x_valid, self.train.y, self.valid.y = test_test_split(x_train, self.train.y, test_size=0.2)\n",
        "        self.train.x, self.train.features = unformat_for_splitting(\n",
        "            x_train, initial_size=initial_size\n",
        "        )\n",
        "        # self.valid.x, self.valid.features = unformat_for_splitting(x_valid, initial_size=initial_size)\n",
        "        self.test.x, self.test.features = unformat_for_splitting(\n",
        "            x_test, initial_size=initial_size\n",
        "        )\n",
        "\n",
        "    def compress(self):\n",
        "        self.train = self.train + self.test + self.valid\n",
        "\n",
        "    def change(self, max_length=None, vocab_size=None, padding_size=None):\n",
        "        if max_length != None:\n",
        "            Data.MAX_LENGTH = max_length\n",
        "        if vocab_size != None:\n",
        "            Data.VOCAB_SIZE = vocab_size\n",
        "        if padding_size != None:\n",
        "            Data.PADDING_SIZE = padding_size\n",
        "\n",
        "    def preprocess(self, data: Data):\n",
        "        preprocessing = Preprocessing(data=data)\n",
        "        # preprocessing.lowercasing()\n",
        "        # preprocessing.lemmatize()\n",
        "        preprocessing.remove_stopword()\n",
        "        data.unicity()\n",
        "\n",
        "    def preprocessing(self):\n",
        "        if self.train != None:\n",
        "            self.preprocess(self.train)\n",
        "        # if self.test != None: self.preprocess(self.test)\n",
        "        # if self.valid != None: self.preprocess(self.valid)\n",
        "\n",
        "    def vectorize(self, data: Data, model_wv: Model_Word2Vec):\n",
        "        vector = Vectorization(data=data, word2vec_model=model_wv)\n",
        "        vector.vectorized_x()\n",
        "        vector.vectorized_y()\n",
        "        vector.get_additional_features()\n",
        "\n",
        "    def vectorization(self, model_wv: Model_Word2Vec):\n",
        "        if self.train != None:\n",
        "            self.vectorize(self.train, model_wv)\n",
        "        # if self.test != None: self.vectorize(self.test)\n",
        "        # if self.valid != None: self.vectorize(self.valid)\n",
        "\n",
        "    def summary(self):\n",
        "        self.model.summary()\n",
        "\n",
        "    def trainning(\n",
        "        self, optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        "    ):\n",
        "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "        if self.valid == None:\n",
        "            self.history = self.model.fit(\n",
        "                self.train.x,\n",
        "                self.train.y,\n",
        "                batch_size=Data.BATCH_SIZE,\n",
        "                epochs=Data.EPOCHS,\n",
        "            )\n",
        "        else:\n",
        "            self.history = self.model.fit(\n",
        "                self.train.x,\n",
        "                self.train.y,\n",
        "                batch_size=Data.BATCH_SIZE,\n",
        "                epochs=Data.EPOCHS,\n",
        "                validation_data=(self.valid.x, self.valid.y),\n",
        "            )\n",
        "\n",
        "    def testing(self):\n",
        "        return self.model.evaluate(self.test.x, self.test.y)\n",
        "\n",
        "    def predicting(self):\n",
        "        y_predict = self.model.predict(self.test.x, batch_size=Data.BATCH_SIZE)\n",
        "        evaluation(self.test.y, y_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train, test = Data(), Data()\n",
        "train = deserialization(\"Dataset/train.pickle\")\n",
        "test = deserialization(\"Dataset/test.pickle\")\n",
        "load_data(train, path=\"Dataset/\", name=\"train\")\n",
        "load_data(test, path=\"Dataset/\", name=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkmKgM_qZL4Y"
      },
      "outputs": [],
      "source": [
        "# train = deepcopy(cnn.train)\n",
        "# test = deepcopy(cnn.test)\n",
        "# train.remove_attributes()\n",
        "# test.remove_attributes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G_ckQb-euXy"
      },
      "outputs": [],
      "source": [
        "# unziprar(path_rar=\"train_x.rar\", dest_dir=\"Dataset\")\n",
        "# unziprar(path_rar=\"test_x.rar\", dest_dir=\"Dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1KlZ-wsZL4b"
      },
      "outputs": [],
      "source": [
        "# save_data(cnn.train, \"train\")\n",
        "# save_data(cnn.test, \"test\")\n",
        "# save_data(cnn.valid, \"valid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6IUaT76g8Ja"
      },
      "source": [
        "### CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-azndnf3Z0Ci"
      },
      "source": [
        "##### Class model CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z44LAt-5moQ"
      },
      "outputs": [],
      "source": [
        "class Model_CNN(Base_Model):\n",
        "    def __init__(self, train=True, test=True, valid=True):\n",
        "        super().__init__(train=train, test=test, valid=valid)\n",
        "        \n",
        "    def architecture(self, activation='sigmoid'):\n",
        "        # Define the model architecture\n",
        "        self.model = Sequential()\n",
        "        self.model.add(\n",
        "            Conv1D(\n",
        "                128,\n",
        "                Data.KERNEL_SIZE,\n",
        "                activation=\"relu\",\n",
        "                input_shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE),\n",
        "                padding=\"same\",\n",
        "            )\n",
        "        )\n",
        "        self.model.add(Dropout(Data.DROPOUT_RATE))\n",
        "        self.model.add(Conv1D(64, Data.KERNEL_SIZE, activation=\"relu\", padding=\"same\"))\n",
        "        self.model.add(Dropout(Data.DROPOUT_RATE))\n",
        "        self.model.add(Conv1D(32, Data.KERNEL_SIZE, activation=\"relu\", padding=\"same\"))\n",
        "        self.model.add(Dropout(Data.DROPOUT_RATE))\n",
        "        self.model.add(Dense(9, activation=activation)) #len(Data.unique_ner_tags)\n",
        "\n",
        "    def architecture_1(self, activation='sigmoid'):\n",
        "        # Define the model architecture\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Conv1D(32, 3, activation='relu', input_shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE)))\n",
        "        self.model.add(MaxPooling1D(2))\n",
        "        self.model.add(Conv1D(16, 3, activation='relu'))\n",
        "        self.model.add(MaxPooling1D(2))\n",
        "        # Flatten layer\n",
        "        self.model.add(Flatten())\n",
        "        # Dense layers\n",
        "        self.model.add(Dense(128, activation='relu'))\n",
        "        self.model.add(Dense(64, activation='relu'))\n",
        "        self.model.add(Dense(32, activation='relu'))\n",
        "        self.model.add(Dense(9, activation=activation))\n",
        "\n",
        "    def architecture_2(self, activation='sigmoid'):\n",
        "        input1 = tf.keras.Input(shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE), name=\"input_1\")\n",
        "        output1 = Conv1D(32, 3, activation='relu', input_shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE))(input1)\n",
        "        output1 = MaxPooling1D(2)(output1)\n",
        "        output1 = Conv1D(16, 3, activation='relu')(output1)\n",
        "        output1 = MaxPooling1D(2)(output1)\n",
        "        # Flatten layer\n",
        "        output1 = Flatten()(output1)\n",
        "        # Dense layers\n",
        "        output1 = Dense(128, activation='relu')(output1)\n",
        "        output1 = Dense(64, activation='relu')(output1)\n",
        "        output1 = Dense(32, activation='relu')(output1)\n",
        "        output1 = Dense(9, activation=activation)(output1)\n",
        "        self.model = Model(inputs=input1, outputs=output1)\n",
        "    \n",
        "    def architecture_3(self, activation='sigmoid'):\n",
        "        input1 = tf.keras.Input(shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE), name=\"input_1\")\n",
        "        output1 = Conv1D(64, 3, padding=\"same\", activation='relu')(input1)\n",
        "        output1 = MaxPooling1D(2, padding=\"same\")(output1)\n",
        "        output1 = Dropout(Data.DROPOUT_RATE)(output1)\n",
        "        output1 = Conv1D(32, 3, padding=\"same\", activation='relu')(output1)\n",
        "        output1 = MaxPooling1D(2, padding=\"same\")(output1)\n",
        "        output1 = Dropout(Data.DROPOUT_RATE)(output1)\n",
        "        # Flatten layer\n",
        "        output1 = Flatten()(output1)\n",
        "        # Dense layers\n",
        "        # output1 = Dense(128, activation='relu')(output1)\n",
        "        output1 = Dense(64, activation='relu')(output1)\n",
        "        output1 = Dense(32, activation='relu')(output1)\n",
        "        output1 = Dense(9, activation=activation)(output1)\n",
        "        self.model = Model(inputs=input1, outputs=output1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otbm1PdTZ40R"
      },
      "source": [
        "##### Test model CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsOYiFC_5moR"
      },
      "outputs": [],
      "source": [
        "cnn = Model_CNN()\n",
        "cnn.change(max_length=50,vocab_size=300, padding_size=10)\n",
        "cnn.compress()\n",
        "model_wv = Model_Word2Vec(cnn.train.sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMrqmWrKZL4X"
      },
      "outputs": [],
      "source": [
        "cnn.architecture_2()\n",
        "cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.architecture_3()\n",
        "cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHK-A4bn_C1B"
      },
      "outputs": [],
      "source": [
        "cnn.train.x = train.x\n",
        "cnn.train.y = train.y\n",
        "cnn.train.features = train.features\n",
        "cnn.test.x = test.x\n",
        "cnn.test.y = test.y\n",
        "cnn.test.features = test.features\n",
        "print(cnn.train.x.shape, cnn.train.features.shape, cnn.train.y.shape)\n",
        "print(cnn.test.x.shape, cnn.test.features.shape, cnn.test.y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yb2uHzaZL4X"
      },
      "outputs": [],
      "source": [
        "# cnn.train.features_level()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_leSeTIQZL4Y"
      },
      "outputs": [],
      "source": [
        "# cnn.preprocessing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7u14mEOZL4Y"
      },
      "outputs": [],
      "source": [
        "# serialization(train, \"Dataset\\\\train.pickle\")\n",
        "# serialization(test, \"Dataset\\\\test.pickle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj6aP1Z1ZL4Z"
      },
      "outputs": [],
      "source": [
        "# cnn.vectorization(model_wv=model_wv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzfg_Wzq_C1D"
      },
      "outputs": [],
      "source": [
        "# cnn.train_test_split()\n",
        "# checkDataset(cnn.train, cnn.test, None)\n",
        "# print(cnn.train.features.shape, cnn.test.features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9otrWbpJ_C1F"
      },
      "outputs": [],
      "source": [
        "Data.EPOCHS = 40\n",
        "cnn.trainning()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "hist = cnn.history\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('Pourcentage')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['accuracy', 'loss'], loc='upper left')\n",
        "plt.show()\n",
        "hist=None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.testing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn.predicting()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEctuVg7HirO"
      },
      "source": [
        "### Model LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbhO6LKDbEhf"
      },
      "source": [
        "#### Class model LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW0Culvw7fm5"
      },
      "outputs": [],
      "source": [
        "class Model_LSTM(Base_Model):\n",
        "    def __init__(self, train=True, test=True, valid=True):\n",
        "        super().__init__(train=train, test=test, valid=valid)\n",
        "    \n",
        "    def architecture(self):\n",
        "        # Define the model architecture\n",
        "        self.model = Sequential()\n",
        "        self.model.add(\n",
        "            LSTM(\n",
        "                256,\n",
        "                input_shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE),\n",
        "                return_sequences=True,\n",
        "                dropout=Data.DROPOUT_RATE,\n",
        "            )\n",
        "        )\n",
        "        self.model.add(LSTM(128, return_sequences=True, dropout=Data.DROPOUT_RATE))\n",
        "        self.model.add(LSTM(64, return_sequences=True, dropout=Data.DROPOUT_RATE))\n",
        "        self.model.add(LSTM(32, return_sequences=True, dropout=Data.DROPOUT_RATE))\n",
        "        self.model.add(Dense(9, activation=\"sigmoid\"))\n",
        "\n",
        "    # def model_1D(self):\n",
        "    #     # self.model.add(\n",
        "    #     #     LSTM(\n",
        "    #     #         256,\n",
        "    #     #         input_shape=(Data.VOCAB_SIZE,), return_state=True\n",
        "    #     #     )\n",
        "    #     # )\n",
        "    #     # # self.model.add(LSTM(128))\n",
        "    #     # # self.model.add(LSTM(64))\n",
        "    #     # # self.model.add(LSTM(32))\n",
        "    #     # self.model.add(Dense(len(Data.unique_ner_tags), activation=\"sigmoid\"))\n",
        "\n",
        "    #     input = tf.keras.Input(shape=(Data.VOCAB_SIZE,))\n",
        "    #     # model = Dropout(0.5)(model)\n",
        "    #     model = tf.keras.layers.Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(input)\n",
        "    #     out = tf.keras.layers.TimeDistributed(Dense(9, activation=\"softmax\"))(model)  # softmax output layer\n",
        "    #     self.model = tf.keras.Model(input, out)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma8R8LJybJpF"
      },
      "source": [
        "#### Test model LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwNkMj2a_C1V"
      },
      "outputs": [],
      "source": [
        "lstm = Model_LSTM()\n",
        "lstm.change(max_length=50,vocab_size=300, padding_size=10)\n",
        "lstm.compress()\n",
        "# model_wv = Model_Word2Vec(lstm.train.sentences)\n",
        "# lstm.train.features_level()\n",
        "# lstm.preprocessing()\n",
        "# lstm.vectorization(model_wv=model_wv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lstm.train = deserialization(\"Dataset/train.pickle\")\n",
        "lstm.test = deserialization(\"Dataset/test.pickle\")\n",
        "load_data(lstm.train, path=\"Dataset/\", name=\"train\")\n",
        "load_data(lstm.test, path=\"Dataset/\", name=\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_lstm = Input(shape=(10, 300)) #(Data.PADDING_SIZE, Data.VOCAB_SIZE)\n",
        "\n",
        "output_lstm = LSTM(256, return_sequences=True, dropout=Data.DROPOUT_RATE)(input_lstm)\n",
        "output_lstm = LSTM(128, return_sequences=True, dropout=Data.DROPOUT_RATE)(output_lstm)\n",
        "output_lstm = LSTM(64, return_sequences=True, dropout=Data.DROPOUT_RATE)(output_lstm)\n",
        "output_lstm = LSTM(32, return_sequences=True, dropout=Data.DROPOUT_RATE)(output_lstm)\n",
        "\n",
        "output_lstm = Flatten()(output_lstm)\n",
        "\n",
        "output_lstm = Dense(64, activation=\"relu\")(output_lstm)\n",
        "output_lstm = Dense(32, activation=\"relu\")(output_lstm)\n",
        "output_lstm = Dense(9, activation=\"sigmoid\")(output_lstm)\n",
        "\n",
        "model_lstm = Model(inputs=input_lstm, outputs=output_lstm)\n",
        "\n",
        "model_lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_lstm.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "history_lstm = model_lstm.fit(lstm.train.x, lstm.train.y, batch_size=Data.BATCH_SIZE, epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_predict = model_lstm.predict(lstm.test.x)\n",
        "evaluation(lstm.test.y, y_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_lstm.save(\"Model/model_lstm.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7f-DWh8_C1V"
      },
      "outputs": [],
      "source": [
        "print(lstm.train.x.shape, lstm.train.y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geUEXfA2_C1W"
      },
      "outputs": [],
      "source": [
        "lstm.architecture()\n",
        "lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HizEJd4x5YIL"
      },
      "outputs": [],
      "source": [
        "# from keras.optimizers import Adam\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# def balanceData(data):\n",
        "#     x_train = np.array(data.sentences_num, dtype=\"float32\")\n",
        "#     y_train = np.array(data.ner_tags_num, dtype=\"float32\")\n",
        "#     print(x_train.shape, y_train.shape)\n",
        "#     input_dim = x_train.shape[1]\n",
        "\n",
        "#     # Encoder\n",
        "#     encoder_input = tf.keras.Input(shape=(input_dim,))\n",
        "#     encoder_dense = tf.keras.layers.Dense(400, activation=\"relu\")(encoder_input)\n",
        "#     encoder_output = tf.keras.layers.Dense(input_dim, activation=\"relu\")(encoder_dense)\n",
        "\n",
        "#     # Decoder\n",
        "#     decoder_dense = tf.keras.layers.Dense(400, activation=\"relu\")(encoder_output)\n",
        "#     decoder_output = tf.keras.layers.Dense(input_dim, activation=\"sigmoid\")(\n",
        "#         decoder_dense\n",
        "#     )\n",
        "\n",
        "#     # Autoencoder\n",
        "#     autoencoder = tf.keras.Model(encoder_input, decoder_output)\n",
        "#     autoencoder.compile(\n",
        "#         optimizer=Adam(learning_rate=0.001), loss=\"mse\", metrics=[\"accuracy\"]\n",
        "#     )\n",
        "#     # print(autoencoder.summary())\n",
        "\n",
        "#     # Train the autoencoder\n",
        "#     autoencoder.fit(x_train, x_train, epochs=10, batch_size=32)\n",
        "\n",
        "#     # Extract latent space representation\n",
        "#     encoder = tf.keras.Model(encoder_input, encoder_output)\n",
        "#     x_train_predict = encoder.predict(x_train)\n",
        "\n",
        "#     # Class balancing with SMOTE or any other technique adadelta\n",
        "#     smote = SMOTE()\n",
        "#     x_train_balanced, y_train_balanced = smote.fit_resample(x_train_predict, y_train)\n",
        "#     return x_train_balanced, y_train_balanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxXGAcX1NRL_"
      },
      "source": [
        "##### train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0NAWSgNEhsQ"
      },
      "outputs": [],
      "source": [
        "lstm.preprocess(lstm.train)\n",
        "vector = Vectorization(lstm.train)\n",
        "vector.word2vec()\n",
        "# vector.tag2num()\n",
        "# vector.num2oneHotEncoding()\n",
        "# lstm.train.flatten()\n",
        "# lstm.train.x, lstm.train.y = balanceData(lstm.train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJMaGjd6_C1c"
      },
      "outputs": [],
      "source": [
        "print(len(lstm.train.sentences),len(lstm.train.sentences_num))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv6jjKD6oNDA"
      },
      "outputs": [],
      "source": [
        "lstm.train.gather()\n",
        "lstm.train.x = np.array(lstm.train.x, dtype=\"float32\")\n",
        "lstm.train.ner_tags_num = lstm.train.y\n",
        "lstm.train.y = np.array(lstm.train.ner_tags_num, dtype=\"float32\")\n",
        "print(lstm.train.x.shape, lstm.train.y.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Egxil5CRNaOu"
      },
      "source": [
        "##### valid set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA3YPsOUsGYi"
      },
      "outputs": [],
      "source": [
        "lstm.valid = Loading(\"valid.txt\").data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZgnkYMMHibf"
      },
      "outputs": [],
      "source": [
        "lstm.preprocess(lstm.valid)\n",
        "vector = Vectorization(lstm.valid)\n",
        "vector.word2vec()\n",
        "vector.tag2num()\n",
        "vector.num2oneHotEncoding()\n",
        "lstm.valid.flatten()\n",
        "lstm.valid.x, lstm.valid.y = balanceData(lstm.valid)\n",
        "lstm.valid.gather()\n",
        "lstm.valid.x = np.array(lstm.valid.x, dtype=\"float32\")\n",
        "lstm.valid.ner_tags_num = lstm.valid.y\n",
        "lstm.valid.y = np.array(lstm.valid.ner_tags_num, dtype=\"float32\")\n",
        "print(lstm.valid.x.shape, lstm.valid.y.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1I7afWv3QP8f"
      },
      "source": [
        "##### Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFb1s9A7-0WL"
      },
      "outputs": [],
      "source": [
        "lstm.preprocess(lstm.test)\n",
        "padding(lstm.test)\n",
        "lstm.vectorize(lstm.test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-5K2vy18gf6"
      },
      "outputs": [],
      "source": [
        "# checkDataset(lstm.train, lstm.test, lstm.valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-801HqfMxVs"
      },
      "outputs": [],
      "source": [
        "# lstm.model=Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO4kJy_5_Yp5"
      },
      "outputs": [],
      "source": [
        "lstm.model_2D()\n",
        "lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-CpDHRdA0kS"
      },
      "outputs": [],
      "source": [
        "lstm.valid=None\n",
        "lstm.trainning()\n",
        "lstm.predicting()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd7STF-I8bN5"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from gensim.models import Word2Vec\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, LSTM\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.utils import class_weight\n",
        "\n",
        "# # Step 1: Load and preprocess the data\n",
        "\n",
        "# # Assuming you have a list of sentences and their corresponding labels\n",
        "# sentences = [\"This is sentence 1\", \"Another sentence\", \"Yet another sentence\"]\n",
        "# labels = [1, 0, 1]  # Assuming binary labels (e.g., 0 - non-NER, 1 - NER)\n",
        "\n",
        "# # Step 2: Train Word2Vec model\n",
        "\n",
        "# # Tokenize the sentences\n",
        "# tokenized_sentences = [sentence.lower().split() for sentence in sentences]\n",
        "\n",
        "# # Train Word2Vec model\n",
        "# word2vec_model = Word2Vec(tokenized_sentences, size=100, window=5, min_count=1)\n",
        "\n",
        "# # Step 3: Encode sentences using Word2Vec embeddings\n",
        "\n",
        "# # Encode sentences into Word2Vec embeddings\n",
        "# encoded_sentences = []\n",
        "# for sentence in tokenized_sentences:\n",
        "#     sentence_encoding = []\n",
        "#     for word in sentence:\n",
        "#         if word in word2vec_model.wv.vocab:\n",
        "#             sentence_encoding.append(word2vec_model.wv[word])\n",
        "#     encoded_sentences.append(sentence_encoding)\n",
        "\n",
        "# # Pad sequences to the same length\n",
        "# padded_sentences = pad_sequences(encoded_sentences, maxlen=max_length, padding='post')\n",
        "\n",
        "# # Step 4: Build and train the autoencoder for imbalanced data\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(padded_sentences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Calculate class weights to address class imbalance\n",
        "# class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
        "\n",
        "# # Build the autoencoder model\n",
        "# autoencoder = Sequential()\n",
        "# autoencoder.add(Dense(64, activation='relu', input_shape=(max_length, 100)))\n",
        "# autoencoder.add(Dense(32, activation='relu'))\n",
        "# autoencoder.add(Dense(64, activation='relu'))\n",
        "# autoencoder.add(Dense(100, activation='relu'))\n",
        "\n",
        "# # Compile and train the autoencoder\n",
        "# autoencoder.compile(optimizer='adam', loss='mse')\n",
        "# autoencoder.fit(X_train, X_train, epochs=10, batch_size=16, class_weight=class_weights)\n",
        "\n",
        "# # Step 5: Extract features using LSTM\n",
        "\n",
        "# # Build the LSTM model\n",
        "# lstm_model = Sequential()\n",
        "# lstm_model.add(LSTM(64, input_shape=(max_length, 100)))\n",
        "# lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# # Compile and train the LSTM model\n",
        "# lstm_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "# lstm_model.fit(X_train, y_train, epochs=10, batch_size=16, class_weight=class_weights)\n",
        "\n",
        "# # Step 6: Make predictions\n",
        "\n",
        "# # Predict on test data\n",
        "# y_pred = lstm_model.predict(X_test)\n",
        "\n",
        "# # Convert predictions to labels\n",
        "# y_pred_labels = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
        "\n",
        "# # Evaluate the model\n",
        "# accuracy = np.mean(np.array(y_pred_labels) == np.array(y_test))\n",
        "# print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTJqiFMqrwxA"
      },
      "outputs": [],
      "source": [
        "visualisation1 = Visualisation(train = (x_train_balanced, y_train_balanced), dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-0LaGTsxRvS"
      },
      "outputs": [],
      "source": [
        "visualisation1.classNumber()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31laKiUWggb8"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from keras import layers\n",
        "# from keras.utils import to_categorical\n",
        "\n",
        "# # Define your autoencoder model\n",
        "# input_dim = 200\n",
        "# encoding_dim = 64\n",
        "\n",
        "# autoencoder_input = tf.keras.Input(shape=(input_dim,))\n",
        "# encoded = layers.Dense(encoding_dim, activation='relu')(autoencoder_input)\n",
        "# decoded = layers.Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "# autoencoder = tf.keras.Model(autoencoder_input, decoded)\n",
        "\n",
        "# # Compile the model\n",
        "# autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# # Generate imbalanced training data\n",
        "# x_train = np.random.rand(1000, 200)\n",
        "# y_train = np.random.choice([0, 1], size=(1000,), p=[0.9, 0.1])\n",
        "# y_train = to_categorical(y_train, num_classes=2)  # Convert labels to one-hot encoding\n",
        "\n",
        "# print(autoencoder.summary())\n",
        "# print(\"train\", x_train.shape, y_train.shape)\n",
        "\n",
        "# # Train the autoencoder\n",
        "# autoencoder.fit(x_train, x_train, epochs=10, batch_size=32, shuffle=True)\n",
        "\n",
        "# # Obtain the encoded representations\n",
        "# encoder = tf.keras.Model(autoencoder_input, encoded)\n",
        "# encoded_train = encoder.predict(x_train)\n",
        "# print(encoded_train.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzsoZFW4baPv"
      },
      "source": [
        "#### Class model TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIppRirQ5mpF"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "class TF_IDF:\n",
        "    def __init__(self, train:Data=None, test:Data=None, valid:Data=None):\n",
        "        if train == None:\n",
        "          self.train = Loading(\"train.txt\").data\n",
        "        else:\n",
        "          self.train = train\n",
        "        if test == None:\n",
        "          self.test = Loading(\"test.txt\").data\n",
        "        else:\n",
        "          self.test = test\n",
        "        if valid == None:\n",
        "          self.valid = Loading(\"valid.txt\").data\n",
        "        else:\n",
        "          self.valid = valid\n",
        "\n",
        "    def __preprocess_tfidf(self, data: Data):\n",
        "        preprocessing = Preprocessing(data=data)\n",
        "        preprocessing.lowercasing()\n",
        "        preprocessing.lemmatize()\n",
        "        preprocessing.remove_stopword()\n",
        "        data.unicity()\n",
        "\n",
        "    def preprocessing(self):\n",
        "        self.__preprocess_tfidf(self.train)\n",
        "        self.__preprocess_tfidf(self.test)\n",
        "        self.__preprocess_tfidf(self.valid)\n",
        "\n",
        "    def __getMatrix(self, max_length, tfidf_matrix, data: Data, feature_names):\n",
        "        entities_data = []\n",
        "        for doc_index in range(len(data.sentences_num)):\n",
        "            doc_tfidf_scores = tfidf_matrix[doc_index].toarray().flatten()\n",
        "            top_indices = doc_tfidf_scores.argsort()[: -doc_tfidf_scores.shape[0] - 1 : -1]\n",
        "            top_entities = [feature_names[i] for i in top_indices if feature_names[i].split(\"__\")[1] != \"O\"]\n",
        "            doc_entities = [entitie for entitie in list(top_entities)[:max_length]]\n",
        "            entities_data.append(doc_entities)\n",
        "        data.sentences_num = entities_data\n",
        "\n",
        "    def __formalize_tfidf(self, data: Data):\n",
        "        return [\n",
        "            \" \".join([\"\".join([word, \"__\", tag]) for word, tag in zip(words, tags)])\n",
        "            for words, tags in zip(data.sentences_num, data.ner_tags_num)\n",
        "        ]\n",
        "\n",
        "    def __deformalize_tfidf(self, data: Data):\n",
        "        sentences_tags = data.sentences_num\n",
        "        data.sentences_num = [[sent_tag.split(\"__\")[0] for sent_tag in sentences_tags] for sentences_tags in sentences_tags]\n",
        "        data.ner_tags_num = [[sent_tag.split(\"__\")[1] for sent_tag in sentences_tags] for sentences_tags in sentences_tags]\n",
        "\n",
        "    def vectorization(self):\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            lowercase=False,\n",
        "            analyzer=\"word\",\n",
        "            stop_words=None,\n",
        "            token_pattern=\"[\\S]+\",\n",
        "            tokenizer=None,\n",
        "            preprocessor=None,\n",
        "        )\n",
        "        # formatted\n",
        "        data_train = self.__formalize_tfidf(self.train)\n",
        "        data_test = self.__formalize_tfidf(self.test)\n",
        "        data_valid = self.__formalize_tfidf(self.valid)\n",
        "        # Tf-idf vectorization\n",
        "        tfidf_matrix_train = vectorizer.fit_transform(data_train)\n",
        "        tfidf_matrix_test = vectorizer.transform(data_test)\n",
        "        tfidf_matrix_valid = vectorizer.transform(data_valid)\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "        # Generation of the matrix\n",
        "        self.__getMatrix(Data.MAX_LENGTH, tfidf_matrix_train, self.train, feature_names)\n",
        "        self.__getMatrix(Data.MAX_LENGTH, tfidf_matrix_test, self.test, feature_names)\n",
        "        self.__getMatrix(Data.MAX_LENGTH, tfidf_matrix_valid, self.valid, feature_names)\n",
        "        # unformatted\n",
        "        self.__deformalize_tfidf(self.train)\n",
        "        self.__deformalize_tfidf(self.test)\n",
        "        self.__deformalize_tfidf(self.valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIFkBA7S5mpG"
      },
      "outputs": [],
      "source": [
        "tfidf = TF_IDF()\n",
        "tfidf.preprocessing()\n",
        "tfidf.vectorization()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-5xAyf4bgyZ"
      },
      "source": [
        "#### Test model TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO0kqmRDONOZ"
      },
      "outputs": [],
      "source": [
        "# import gensim.downloader as api\n",
        "# path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
        "# print(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPZEvRTT5mpH"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, Activation, Embedding, Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "# # Load data\n",
        "# df = pd.read_csv(\"ner_data.csv\", encoding=\"ISO-8859-1\", error_bad_lines=False)\n",
        "# df = df.fillna(method=\"ffill\")\n",
        "# sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "# tags = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "\n",
        "# # Perform TF-IDF\n",
        "# vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
        "# X_tfidf = vectorizer.fit_transform([\" \".join(sent) for sent in sentences])\n",
        "# tfidf_vocab = vectorizer.vocabulary_\n",
        "# tfidf_vocab_inv = {v:k for k,v in tfidf_vocab.items()}\n",
        "# tfidf_weights = np.asarray(X_tfidf.mean(axis=0)).ravel()\n",
        "\n",
        "# # Tokenize words\n",
        "# MAX_NB_WORDS = 20000\n",
        "# MAX_SEQ_LENGTH = 100\n",
        "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "# tokenizer.fit_on_texts(sentences)\n",
        "# word_index = tokenizer.word_index\n",
        "\n",
        "# # Convert words to sequences\n",
        "# X = tokenizer.texts_to_sequences(sentences)\n",
        "# X = pad_sequences(X, maxlen=MAX_SEQ_LENGTH)\n",
        "\n",
        "# # Convert tags to sequences\n",
        "# tags_index = {\"O\": 0, \"B-LOC\": 1, \"I-LOC\": 2, \"B-PER\": 3, \"I-PER\": 4, \"B-ORG\": 5, \"I-ORG\": 6}\n",
        "# y = [[tags_index[tag] for tag in sent] for sent in tags]\n",
        "# y = pad_sequences(y, maxlen=MAX_SEQ_LENGTH)\n",
        "\n",
        "# # Split data into train and test sets\n",
        "# VALIDATION_SPLIT = 0.2\n",
        "# nb_validation_samples = int(VALIDATION_SPLIT * len(X))\n",
        "# X_train = X[:-nb_validation_samples]\n",
        "# y_train = y[:-nb_validation_samples]\n",
        "# X_test = X[-nb_validation_samples:]\n",
        "# y_test = y[-nb_validation_samples:]\n",
        "\n",
        "# # Define CNN model\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(MAX_NB_WORDS, Data.VOCAB_SIZE, input_length=MAX_SEQ_LENGTH))\n",
        "# model.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "# model.add(GlobalMaxPooling1D())\n",
        "# model.add(Dense(7, activation=\"softmax\"))\n",
        "# model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# # Train model\n",
        "# model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n",
        "\n",
        "# # Predict tags for new sentences\n",
        "# def predict_tags(sentences):\n",
        "#     X = tokenizer.texts_to_sequences(sentences)\n",
        "#     X = pad_sequences(X, maxlen=MAX_SEQ_LENGTH)\n",
        "#     y_pred = model.predict(X)\n",
        "#     return [[tfidf_vocab_inv[np.argmax(tfidf_weights * y)] if np.max(tfidf_weights * y) > 0.2 else \"O\" for y in sent] for sent in y_pred]\n",
        "\n",
        "# # Test predictions\n",
        "# sentences_test = [\"John lives in New York City.\", \"Steve Jobs was the founder of Apple.\"]\n",
        "# tags_pred = predict_tags(sentences_test)\n",
        "# print(tags_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QGhljfiC5gG"
      },
      "outputs": [],
      "source": [
        "# entities = dict(zip(Data.unique_ner_tags.keys(), [0 for i in range(len(Data.unique_ner_tags))]))\n",
        "# for tags in test.ner_tags:\n",
        "#     for tag in tags:\n",
        "#         entities[tag] += 1\n",
        "# is_entities = 0\n",
        "# is_not_entities = 0\n",
        "# for tag, nbr in entities.items():\n",
        "#     if tag != 'O': is_entities += nbr\n",
        "#     else: is_not_entities += nbr\n",
        "# print(entities)\n",
        "# print(is_entities, is_not_entities)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dvATuqZ4_C18"
      },
      "source": [
        "### CNN for Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBL2xyIAZL4t"
      },
      "outputs": [],
      "source": [
        "input2 = Input(shape=cnn.train.features.shape[1:])\n",
        "output2 = Dense(7, activation='relu')(input2)\n",
        "output2 = Dense(10, activation=\"relu\")(output2)\n",
        "output2 = Dense(9, activation=\"softmax\")(output2)\n",
        "model2 =  Model(input2, output2)\n",
        "model2.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idaQo_MpZL4u"
      },
      "outputs": [],
      "source": [
        "history2 = model2.fit(cnn.train.features, cnn.train.y, batch_size=Data.BATCH_SIZE, epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqJH1jKrZL4u"
      },
      "outputs": [],
      "source": [
        "y_predict = model2.predict(cnn.test.features)\n",
        "evaluation(cnn.test.y, y_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "hist = history2\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('Pourcentage')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['accuracy', 'loss'], loc='upper left')\n",
        "plt.show()\n",
        "hist=None"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combinaison CNN for Word2Vec and Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CNN for word2vec\n",
        "input1 = tf.keras.Input(shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE), name=\"input_1\")\n",
        "output1 = Conv1D(32, 3, activation='relu', input_shape=(Data.PADDING_SIZE, Data.VOCAB_SIZE))(input1)\n",
        "output1 = MaxPooling1D(2)(output1)\n",
        "output1 = Conv1D(16, 3, activation='relu')(output1)\n",
        "output1 = MaxPooling1D(2)(output1)\n",
        "# Flatten layer\n",
        "output1 = Flatten()(output1)\n",
        "# Dense layers\n",
        "output1 = Dense(128, activation='relu')(output1)\n",
        "output1 = Dense(64, activation='relu')(output1)\n",
        "output1 = Dense(32, activation='relu')(output1)\n",
        "output1 = Dense(9, activation=\"relu\")(output1)\n",
        "model1 = Model(inputs=input1, outputs=output1)\n",
        "\n",
        "# Fully connected for Features\n",
        "input2 = Input(shape=cnn.train.features.shape[1:], name=\"input_2\")\n",
        "output2 = Dense(16, activation=\"relu\")(input2)\n",
        "output2 = Dense(9, activation=\"relu\")(output2)\n",
        "model2 =  Model(inputs=input2, outputs=output2)\n",
        "\n",
        "# Output model for the concatenation of CNN for word2vec and Fully Connected\n",
        "model1_model2 = concatenate([model1.output, model2.output])\n",
        "output3 = Dense(18, activation=\"relu\")(model1_model2)\n",
        "output3 = Dense(16, activation=\"relu\")(output3)\n",
        "output3 = Dense(9, activation=\"sigmoid\")(output3)\n",
        "model = Model(inputs=[model1.input, model2.input], outputs=output3)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit([cnn.train.x, cnn.train.features], cnn.train.y, batch_size=Data.BATCH_SIZE, epochs=Data.EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_predict = model.predict([cnn.test.x, cnn.test.features])\n",
        "evaluation(cnn.test.y, y_predict)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pydot\n",
        "# tf.keras.utils.model_to_dot(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cnn.model.save(\"Model/cnn_model.keras\")\n",
        "# model1.save(\"Model/model1.keras\")\n",
        "# model.save(\"Model/model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hya4IeXZAIq"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from keras.layers import Input, Dense\n",
        "# from keras.models import Model\n",
        "# from keras.optimizers import Adam\n",
        "\n",
        "# # Generate synthetic balanced multilabel data\n",
        "# num_samples = 1000\n",
        "# num_features = 50\n",
        "# num_labels = 10\n",
        "# data = np.random.rand(num_samples, num_features)\n",
        "# labels = np.random.randint(2, size=(num_samples, num_labels))\n",
        "# labels_balanced = np.column_stack((labels[:, :5], np.logical_not(labels[:, :5])))\n",
        "# print(labels_balanced[0])\n",
        "# # Split the data into training and testing sets\n",
        "# train_data, test_data, train_labels, test_labels = train_test_split(data, labels_balanced, test_size=0.2, random_state=42)\n",
        "# print(train_data.shape, test_data.shape, train_labels.shape, test_labels.shape)\n",
        "# # Autoencoder architecture\n",
        "# input_dim = num_features\n",
        "# encoding_dim = 20\n",
        "\n",
        "# input_layer = Input(shape=(input_dim,))\n",
        "# encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "# decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
        "\n",
        "# # Create the autoencoder model\n",
        "# autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "\n",
        "# # Compile the model\n",
        "# autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy')\n",
        "\n",
        "# # Train the autoencoder\n",
        "# autoencoder.fit(train_data, train_data, epochs=10, batch_size=32, shuffle=True, validation_data=(test_data, test_data))\n",
        "# autoencoder.summary()\n",
        "# # Extract the encoder part for feature representation\n",
        "# encoder_model = Model(inputs=input_layer, outputs=encoder)\n",
        "\n",
        "# # Get the encoded features\n",
        "# encoded_train_features = encoder_model.predict(train_data)\n",
        "# encoded_test_features = encoder_model.predict(test_data)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hWBc46_ZTK5u",
        "YMfHyYuHTDud",
        "3fcvRYZ97fmq",
        "mkK1C7mp7fmu",
        "VxLQFuLO7fmv",
        "I9vvnsDR7fmv",
        "T0yl2FQm7fmw",
        "N-2jPEd6TZ6I",
        "gPCrQ6VwmV03",
        "2NFpti3H7fm0",
        "rVKzq06h5moN",
        "u6IUaT76g8Ja",
        "-azndnf3Z0Ci",
        "AbhO6LKDbEhf",
        "Ma8R8LJybJpF",
        "vzsoZFW4baPv",
        "X-5xAyf4bgyZ",
        "HwS7GyBk5moe"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
