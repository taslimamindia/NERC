{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ-tKnxA74KU",
        "outputId": "a3be3012-9198-4684-baf6-d9ea45f58cf3"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/taslimamindia/NERC.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fcvRYZ97fmq"
      },
      "source": [
        "# Importation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "tou31yST7fmt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from nltk import word_tokenize, sent_tokenize, download\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.utils import to_categorical, pad_sequences\n",
        "\n",
        "# import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ9h38D29wUb",
        "outputId": "845fddfd-81a4-4f08-e373-6cb620e91357"
      },
      "outputs": [],
      "source": [
        "download('wordnet') # for google colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkK1C7mp7fmu"
      },
      "source": [
        "# Class define form data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "tsvoLVPY7fmu"
      },
      "outputs": [],
      "source": [
        "class Data(object):\n",
        "    unique_words = {\"<PAD>\":0}\n",
        "    unique_ner_tags = {\"O\":0}\n",
        "    MAX_LENGTH = 50\n",
        "    VOCAB_SIZE = 100\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.sentences = []\n",
        "        self.sentences_num = None\n",
        "        self.ner_tags = []\n",
        "        self.ner_tags_num = None\n",
        "        self.chunk_tags = []\n",
        "        self.pos_tags = []\n",
        "        self.x, self.y = None, None\n",
        "    def word2vec(self, vector_size=100):\n",
        "        VOCAB_SIZE = vector_size\n",
        "        word2vec_model = Word2Vec(self.sentences, vector_size=vector_size, window=5, min_count=1, workers=4)\n",
        "        return word2vec_model   \n",
        "    def word2idx(self, word:str):\n",
        "        return Data.unique_words.get(word, None)\n",
        "    def idx2word(self, index:int):\n",
        "        for word, value in Data.unique_words.items():\n",
        "            if index is value: return word\n",
        "        return None    \n",
        "    def tag2idx(self, tag):\n",
        "        return Data.unique_ner_tags.get(tag, None)\n",
        "    def idx2tag(self, index):\n",
        "        for tag, value in Data.unique_ner_tags.items():\n",
        "            if index == value: return tag\n",
        "        return None\n",
        "    def unicity(self):\n",
        "        unique_sent, unique_tag = set(), set()\n",
        "        [unique_tag.update(tags) for tags in self.ner_tags_num]\n",
        "        [unique_sent.update(tags) for tags in self.sentences_num]\n",
        "        max_tags = len(Data.unique_ner_tags)\n",
        "        max_words = len(Data.unique_words)\n",
        "        for word in list(unique_sent):\n",
        "            if Data.unique_words.get(word, None) == None:\n",
        "                Data.unique_words[word] = max_words\n",
        "                max_words += 1\n",
        "        for tag in list(unique_tag):\n",
        "            if Data.unique_ner_tags.get(tag, None) == None:\n",
        "                Data.unique_ner_tags[tag] = max_tags\n",
        "                max_tags += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxLQFuLO7fmv"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "NzjCn5tW7fmv"
      },
      "outputs": [],
      "source": [
        "class Loading():\n",
        "    def __init__(self, data: Data, file):\n",
        "        self.data = data\n",
        "        self.load_sentences(file)\n",
        "    def load_sentences(self, filepath):\n",
        "        tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                if (line == ('-DOCSTART- -X- -X- O\\n') or line == '\\n'):\n",
        "                    if len(tokens) > 0:\n",
        "                        self.data.sentences.append(tokens)\n",
        "                        self.data.pos_tags.append(pos_tags)\n",
        "                        self.data.chunk_tags.append(chunk_tags)\n",
        "                        self.data.ner_tags.append(ner_tags)\n",
        "                        tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
        "                else:\n",
        "                    l = line.split(' ')\n",
        "                    tokens.append(l[0])\n",
        "                    pos_tags.append(l[1])\n",
        "                    chunk_tags.append(l[2])\n",
        "                    ner_tags.append(l[3].strip('\\n'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9vvnsDR7fmv"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3Rbc4nVU7fmw"
      },
      "outputs": [],
      "source": [
        "class Preprocessing():\n",
        "    def __init__(self, data:Data, text=None, lang=\"english\"):\n",
        "        self.data = data\n",
        "        self.text = text\n",
        "        self.lang = lang\n",
        "        if text == None:\n",
        "            self.data.sentences_num = self.data.sentences\n",
        "            self.data.ner_tags_num = self.data.ner_tags\n",
        "    \n",
        "    def tokenize(self):\n",
        "        if self.text != None:\n",
        "            sentenses = [word_tokenize(sentence, language=self.lang) for sentence in sent_tokenize(self.text, language=self.lang)]\n",
        "            self.data.sentences = [[token for token in sentence if token not in stopwords.words(self.lang)] for sentence in sentenses]\n",
        "            self.data.sentences_num = self.data.sentences\n",
        "        \n",
        "    def lowercasing(self):\n",
        "        self.data.sentences_num = [[word.lower() for word in sentence] for sentence in self.data.sentences_num]\n",
        "    \n",
        "    def lemmatize(self):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        self.data.sentences_num = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in self.data.sentences_num]\n",
        "    \n",
        "    def remove_stopword(self):\n",
        "        punctuation = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
        "        sentences = [[(self.data.sentences_num[i][j], self.data.ner_tags[i][j]) for j in range(len(self.data.sentences_num[i]))] for i in range(len(self.data.sentences_num))]\n",
        "        sentences = [[(token, tag) for token, tag in sentence if token not in stopwords.words(self.lang) + punctuation] for sentence in sentences]\n",
        "        self.data.sentences_num = [[token for token, tag in sentence] for sentence in sentences]\n",
        "        self.data.ner_tags_num = [[tag for token, tag in sentence] for sentence in sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0yl2FQm7fmw"
      },
      "source": [
        "# Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Arjdv32N7fmw"
      },
      "outputs": [],
      "source": [
        "class Vectorization():\n",
        "    def __init__(self, data:Data):\n",
        "        self.data = data\n",
        "    \n",
        "    def word2vec(self, min_count=1, window=5):\n",
        "        word2vec_model = Word2Vec(self.data.sentences_num, min_count=min_count, vector_size=Data.VOCAB_SIZE, window=window)\n",
        "        self.data.sentences_num = [[word2vec_model.wv[word] for word in sentence] for sentence in self.data.sentences_num]\n",
        "    \n",
        "    def padding_x(self, value=np.zeros((Data.VOCAB_SIZE,), dtype=\"float32\"), dtype=\"float32\"):\n",
        "        self.data.x = pad_sequences(\n",
        "            sequences=self.data.sentences_num, \n",
        "            maxlen=self.data.MAX_LENGTH, \n",
        "            dtype=dtype, \n",
        "            padding=\"post\", \n",
        "            value=value\n",
        "        )\n",
        "    \n",
        "    def vectorized_x(self):\n",
        "        self.word2vec()\n",
        "        self.padding_x()\n",
        "        \n",
        "    def tag2num(self):\n",
        "        NUM_CLASSES = len(Data.unique_ner_tags)\n",
        "        self.data.ner_tags_num = [[to_categorical(Data.unique_ner_tags.get(tag), num_classes=NUM_CLASSES) for tag in tags] for tags in self.data.ner_tags_num]\n",
        "    \n",
        "    def padding_y(self, value=to_categorical(Data.unique_ner_tags.get(\"O\"), num_classes=len(Data.unique_ner_tags))):\n",
        "        self.data.y = pad_sequences(\n",
        "            sequences=self.data.ner_tags_num, \n",
        "            maxlen=self.data.MAX_LENGTH,\n",
        "            padding=\"post\", \n",
        "            dtype=\"float32\",\n",
        "            value=value\n",
        "        )\n",
        "    \n",
        "    def vectorized_y(self):\n",
        "        self.tag2num()\n",
        "        self.padding_y()\n",
        "\n",
        "def load_dataset(path: str):\n",
        "    data = Data()\n",
        "    base_file = \"../Data/conll2003_english/\"\n",
        "    # base_file = \"/content/NERC/Data/conll2003_english/\"\n",
        "    Loading(data = data, file=base_file + path)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMdxz6YR7fmx"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2NFpti3H7fm0"
      },
      "source": [
        "### New input text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jGe-JDa7fm1"
      },
      "outputs": [],
      "source": [
        "# test_text = Data()\n",
        "\n",
        "# preprocessing = Preprocessing(data = test_text, text = \"Obama is the president of the United States. I am from Guinea, nice to meet you.\")\n",
        "# preprocessing.tokenize()\n",
        "# preprocessing.lowercasing()\n",
        "# preprocessing.lemmatize()\n",
        "# print(test_text.sentences)\n",
        "\n",
        "# vector = Vectorization(test_text)\n",
        "# vector.vectorized_x()\n",
        "# print(test_text.x.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F08w-hsB7fm1"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-M4-ULUI7fm1"
      },
      "outputs": [],
      "source": [
        "# NUM_WORDS = len(Data.unique_words)\n",
        "# NUM_CLASSES = len(Data.unique_ner_tags)\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 100\n",
        "NUM_FILTERS = 256\n",
        "KERNEL_SIZE = 3\n",
        "DROPOUT_RATE = 0.5\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluation(test:Data, y_predict):\n",
        "  true, false, total, predict = 0, 0, 0, 0\n",
        "  x, y, z = test.y.shape\n",
        "  for i in range(x):\n",
        "    for j in range(y):\n",
        "      real_tag = np.argmax(test.y[i][j]) \n",
        "      predict_tag = np.argmax(y_predict[i][j])\n",
        "      if predict_tag == 0: predict +=1\n",
        "      if real_tag != 0:\n",
        "        total = total + 1\n",
        "        if real_tag == predict_tag: true = true + 1\n",
        "        else: false = false + 1\n",
        "  print(\"----------------------- Evaluation -------------------------\")\n",
        "  print(test.y.shape)\n",
        "  print(predict, x*y)\n",
        "  print(true, false, total, round(true/total, 3), round(false/total, 3), end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def checkDataset(train, test, valid):    \n",
        "    print(\"X_train\", train.x.shape)\n",
        "    print(\"y_train\", train.y.shape, \"\\n\")\n",
        "    print(\"X_test\", test.x.shape)\n",
        "    print(\"y_test\", test.y.shape, \"\\n\")    \n",
        "    print(\"X_valid\", valid.x.shape)\n",
        "    print(\"y_valid\", valid.y.shape)\n",
        "\n",
        "def main():\n",
        "    train = load_dataset(\"train.txt\")\n",
        "    test = load_dataset(\"test.txt\")\n",
        "    valid = load_dataset(\"valid.txt\")\n",
        "    preprocess_lstm(train)\n",
        "    preprocess_lstm(test)\n",
        "    preprocess_lstm(valid)\n",
        "    vectorize(train)\n",
        "    vectorize(test)\n",
        "    vectorize(valid)\n",
        "    checkDataset(train, test, valid)\n",
        "    return train, test, valid"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u6IUaT76g8Ja"
      },
      "source": [
        "## CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_17 (Conv1D)          (None, 98, 64)            256       \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 98, 64)            0         \n",
            "                                                                 \n",
            " conv1d_18 (Conv1D)          (None, 96, 32)            6176      \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 96, 32)            0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 96, 9)             297       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,729\n",
            "Trainable params: 6,729\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(64, KERNEL_SIZE, activation='relu', input_shape=(100,1)))\n",
        "model.add(Dropout(DROPOUT_RATE))\n",
        "model.add(Conv1D(32, KERNEL_SIZE, activation='relu'))\n",
        "model.add(Dropout(DROPOUT_RATE))\n",
        "model.add(Dense(9, activation='softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D\n",
        "\n",
        "class Model_CNN:\n",
        "  def __init__(self):\n",
        "    # Define the model architecture\n",
        "    self.model = Sequential()\n",
        "    self.model.add(Conv1D(64, KERNEL_SIZE, activation='relu', input_shape=(Data.MAX_LENGTH, EMBEDDING_DIM), padding='same'))\n",
        "    self.model.add(Dropout(DROPOUT_RATE))\n",
        "    self.model.add(Conv1D(32, KERNEL_SIZE, activation='relu', padding='same'))\n",
        "    self.model.add(Dropout(DROPOUT_RATE))\n",
        "    self.model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "    \n",
        "  def summary(self):\n",
        "    self.model.summary()\n",
        "    \n",
        "  def trainning(self, train:Data, valid:Data=None):\n",
        "    cat_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "    recall = tf.keras.metrics.Recall()\n",
        "    self.model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[cat_accuracy, recall])\n",
        "    if valid == None:\n",
        "      self.model.fit(train.x, train.y, batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
        "    else:\n",
        "      self.model.fit(train.x, train.y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(valid.x, valid.y))\n",
        "      \n",
        "  def testing(self, test:Data):\n",
        "    return self.model.evaluate(test.x, test.y)\n",
        "  \n",
        "  def predicting(self, test:Data):\n",
        "    return self.model.predict(test.x, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main_cnn(param:dict):\n",
        "  dico = {\"params\":[], \"metrics\":[]}\n",
        "  if param.get(\"max_length\", 0) != 0:\n",
        "      max_lengths = param[\"max_length\"]\n",
        "      for max_length in max_lengths:   \n",
        "        Data.MAX_LENGTH = max_length     \n",
        "        train, test, valid = main()\n",
        "        model_cnn = Model_CNN()\n",
        "        model_cnn.trainning(train, valid)\n",
        "        model_cnn.testing(test)\n",
        "        y_predict_cnn = model_cnn.predicting(test)\n",
        "        evaluation(test, y_predict_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main_cnn({\"max_length\":[50]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0eEcWeJRv9G"
      },
      "outputs": [],
      "source": [
        "# # from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from keras.models import Model\n",
        "# from keras.layers import Dense, Conv1D\n",
        "# from tf2crf import CRF, ModelWithCRFLoss\n",
        "# from keras import Input\n",
        "\n",
        "# # Build CNN model\n",
        "# # model = Sequential()\n",
        "# inputs = Input(shape=(MAX_LENGTH, EMBEDDING_DIM))\n",
        "# outputs = Conv1D(64, KERNEL_SIZE, activation='relu', padding='same')(inputs)\n",
        "# # model.add(MaxPooling1D(2, padding='same'))\n",
        "# # outputs = Dropout(DROPOUT_RATE)(inputs)\n",
        "# outputs = Conv1D(32, KERNEL_SIZE, activation='relu', padding='same')(inputs)\n",
        "# # model.add(MaxPooling1D(2))\n",
        "# # model.add(Dropout(DROPOUT_RATE))\n",
        "# # model.add(Dense(HIDDEN_DIM, activation='relu'))\n",
        "# # outputs = Dropout(DROPOUT_RATE)(outputs)\n",
        "# outputs = Dense(NUM_CLASSES, activation='relu')(outputs)\n",
        "# # outputs.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# # outputs.summary()\n",
        "# crf = CRF(units=9)\n",
        "# # cnn_model.add(crf)\n",
        "# output = crf(outputs)\n",
        "# cnn_crf_model = Model(inputs, output)\n",
        "# cnn_crf_model.summary()\n",
        "# # cnn_crf_model = ModelWithCRFLoss(base_model, sparse_target=True)\n",
        "# # cnn_crf_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVZz_xMcRv9H"
      },
      "outputs": [],
      "source": [
        "# cnn_crf_model.compile(optimizer='adam')\n",
        "# cnn_crf_model.fit(train.x, train.y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(valid.x, valid.y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqUvOqaORv9H"
      },
      "outputs": [],
      "source": [
        "# # Evaluation\n",
        "# loss, accuracy = cnn_crf_model.evaluate(test.x, test.y, batch_size=BATCH_SIZE)\n",
        "\n",
        "# print('Test Loss:', loss)\n",
        "# print('Test Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEctuVg7HirO"
      },
      "source": [
        "## Model LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "IgS5eRZ07fmy"
      },
      "outputs": [],
      "source": [
        "def preprocess_lstm(data:Data):\n",
        "    preprocessing = Preprocessing(data=data)\n",
        "    preprocessing.lowercasing()\n",
        "    preprocessing.lemmatize()\n",
        "    preprocessing.remove_stopword()\n",
        "    data.unicity()\n",
        "\n",
        "def vectorize(data:Data):\n",
        "    vector = Vectorization(data=data)\n",
        "    \n",
        "    # # treat sentences\n",
        "    # vector.vectorized_x()\n",
        "    # vector.vectorized_y()\n",
        "    \n",
        "    # # treat words\n",
        "    vector.word2vec()\n",
        "    vector.tag2num()\n",
        "    Sentences, Tags = [], []\n",
        "    [[Sentences.append(word) for word in sentence] for sentence in data.sentences_num]\n",
        "    [[Tags.append(tag) for tag in tags] for tags in data.ner_tags_num]\n",
        "    data.x, data.y = np.array(Sentences, dtype=\"float32\"), np.array(Tags, dtype=\"float32\")\n",
        "    print(data.x.shape, data.y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hW0Culvw7fm5"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "class Model_LSTM:\n",
        "  def __init__(self):\n",
        "    # # Define the model architecture\n",
        "    # self.model_LSTM = Sequential()\n",
        "    # self.model_LSTM.add(LSTM(256, input_shape=(Data.MAX_LENGTH, Data.VOCAB_SIZE), return_sequences=True, dropout=0.5))\n",
        "    # self.model_LSTM.add(LSTM(128, return_sequences=True, dropout=0.5))\n",
        "    # self.model_LSTM.add(LSTM(64, return_sequences=True, dropout=0.5))\n",
        "    # self.model_LSTM.add(LSTM(32, return_sequences=True, dropout=0.5))\n",
        "    # self.model_LSTM.add(Dense(len(Data.unique_ner_tags), activation='softmax'))\n",
        "    \n",
        "    # Define the model architecture\n",
        "    self.model_LSTM = Sequential()\n",
        "    self.model_LSTM.add(LSTM(128, input_shape=(1,Data.VOCAB_SIZE), return_sequences=True, dropout=0.5))\n",
        "    self.model_LSTM.add(LSTM(64, return_sequences=True, dropout=0.5))\n",
        "    self.model_LSTM.add(LSTM(32, return_sequences=True, dropout=0.5))\n",
        "    self.model_LSTM.add(Dense(len(Data.unique_ner_tags), activation='softmax'))\n",
        "\n",
        "  def summary(self):\n",
        "    self.model_LSTM.summary()\n",
        "\n",
        "  def trainning(self, train:Data, valid:Data=None):\n",
        "    cat_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "    recall = tf.keras.metrics.Recall()\n",
        "    self.model_LSTM.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[cat_accuracy, recall])\n",
        "    if valid == None:\n",
        "      self.model_LSTM.fit(train.x, train.y, batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
        "    else:\n",
        "      self.model_LSTM.fit(train.x, train.y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(valid.x, valid.y))\n",
        "\n",
        "  def testing(self, test:Data):\n",
        "    return self.model_LSTM.evaluate(test.x, test.y)\n",
        "\n",
        "  def predicting(self, test:Data):\n",
        "    return self.model_LSTM.predict(test.x, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main_lstm(param:dict):\n",
        "    dico = {\"params\":[], \"metrics\":[]}\n",
        "    if param.get(\"max_length\", 0) != 0:\n",
        "        max_lengths = param[\"max_length\"]\n",
        "        for max_length in max_lengths:   \n",
        "            Data.MAX_LENGTH = max_length     \n",
        "            train, test, valid = main()\n",
        "            model_lstm = Model_LSTM()\n",
        "            model_lstm.summary()\n",
        "            # model_lstm.trainning(train, valid)\n",
        "            # model_lstm.testing(test)\n",
        "            # y_predict_lstm = model_lstm.predicting(test)\n",
        "            # evaluation(test, y_predict_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_39 (LSTM)              (None, 128)               66560     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 9)                 1161      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67,721\n",
            "Trainable params: 67,721\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(50, 1), dropout=0.5))  # assuming you're using a univariate time series\n",
        "# model.add(LSTM(64, activation=\"relu\"))  # assuming you're using a univariate time series\n",
        "model.add(Dense(9, activation='sigmoid'))  # assuming binary classification, adjust for your problem\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "chYF5-l2CvEB",
        "outputId": "c1fd05c0-dfd7-43c9-ea5b-53efcd4f363b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(125907, 100) (125907, 9)\n",
            "(29480, 100) (29480, 9)\n",
            "(31205, 100) (31205, 9)\n",
            "X_train (125907, 100)\n",
            "y_train (125907, 9) \n",
            "\n",
            "X_test (29480, 100)\n",
            "y_test (29480, 9) \n",
            "\n",
            "X_valid (31205, 100)\n",
            "y_valid (31205, 9)\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_29 (LSTM)              (None, 1, 128)            117248    \n",
            "                                                                 \n",
            " lstm_30 (LSTM)              (None, 1, 64)             49408     \n",
            "                                                                 \n",
            " lstm_31 (LSTM)              (None, 1, 32)             12416     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1, 9)              297       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 179,369\n",
            "Trainable params: 179,369\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "main_lstm({\"max_length\":[50]})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7eHuf96nXDm"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tqp60CTqGqzV"
      },
      "outputs": [],
      "source": [
        "# model_LSTM.save(\"../Data/model_lstm.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itvrF6SvFmUg"
      },
      "outputs": [],
      "source": [
        "# model_LSTM = tf.keras.models.load_model(\"../Data/model_lstm.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x5O84rzHU9m"
      },
      "outputs": [],
      "source": [
        "# # np.quantile(sort([1, 2, 3, 8, 7]), 0.50)\n",
        "# dico = {}\n",
        "# for tags in test.sentences + train.sentences + valid.sentences:\n",
        "#   if dico.get(len(tags), None) == None:\n",
        "#     dico[len(tags)] = 1\n",
        "#   dico[len(tags)] += 1\n",
        "# sorted(list(dico.items()), key= lambda x: x[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QGhljfiC5gG"
      },
      "outputs": [],
      "source": [
        "# entities = dict(zip(Data.unique_ner_tags.keys(), [0 for i in range(len(Data.unique_ner_tags))]))\n",
        "# for tags in test.ner_tags:\n",
        "#     for tag in tags:\n",
        "#         entities[tag] += 1\n",
        "# is_entities = 0\n",
        "# is_not_entities = 0\n",
        "# for tag, nbr in entities.items():\n",
        "#     if tag != 'O': is_entities += nbr\n",
        "#     else: is_not_entities += nbr\n",
        "# print(entities)\n",
        "# print(is_entities, is_not_entities)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF CNN-Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D\n",
        "# from keras_contrib.layers import CRF\n",
        "# from keras_contrib.utils import save_load_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_tf_idf():\n",
        "    train = load_dataset(\"train.txt\")\n",
        "    # test = load_dataset(\"test.txt\")\n",
        "    # valid = load_dataset(\"valid.txt\")\n",
        "    # return train, test, valid\n",
        "    return train, None, None\n",
        "\n",
        "def preprocess_tfidf(data:Data):\n",
        "    preprocessing = Preprocessing(data=data)\n",
        "    preprocessing.lowercasing()\n",
        "    preprocessing.lemmatize()\n",
        "    preprocessing.remove_stopword()\n",
        "    data.unicity()\n",
        "    # sentences = [\" \".join(sentence) for sentence in data.sentences_num]\n",
        "    # vectorizer = TfidfVectorizer(max_features=Data.MAX_LENGTH)\n",
        "    # data.x = vectorizer.fit_transform(sentences).toarray()\n",
        "    # y = []\n",
        "    # [[y.append(to_categorical(Data.unique_ner_tags[tag], num_classes=NUM_CLASSES)) for tag in tags] for tags in data.ner_tags]\n",
        "    # data.y = np.array(y, dtype=\"float32\")\n",
        "\n",
        "def vectorize_tf_idf(data:Data):\n",
        "    vectorize = Vectorization(data=data)\n",
        "    print(data.x)\n",
        "    vectorize.padding_x(value=\"<PAD>\", dtype=\"str\")\n",
        "    print(data.x)\n",
        "    vectorize.padding_y(value=\"O\")\n",
        "\n",
        "def formalize_tfidf(data:Data):\n",
        "    data.x = data.sentences_num\n",
        "    data.y = data.ner_tags_num\n",
        "    x = len(data.x)\n",
        "    return [\" \".join([\"\".join([data.x[i][j], \"__\", data.y[i][j]]) for j in range(len(data.x[i]))]) for i in range(x)]\n",
        "\n",
        "def tf(train:Data, test:Data, valid:Data):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    data_train = formalize_tfidf(train)\n",
        "    vectors = vectorizer.fit_transform(data_train)\n",
        "    # vectorizer.get_\n",
        "    train.x = vectors.toarray()\n",
        "    print(vectors)\n",
        "    print(train.x.shape)\n",
        "    # test.x = vectorizer.transform(test.x).toarray()\n",
        "    # valid.x = vectorizer.transform(valid.x).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TF_IDF:\n",
        "  def __init__(self):\n",
        "    self.train, self.test, self.valid = load_tf_idf()\n",
        "    # # Preprocessing\n",
        "    preprocess_tfidf(self.train)\n",
        "    # preprocess_tfidf(self.test)\n",
        "    # preprocess_tfidf(self.valid)\n",
        "    # # Vectorization\n",
        "    # vectorize_tf_idf(self.train)\n",
        "    # vectorize_tf_idf(self.test)\n",
        "    # vectorize_tf_idf(self.valid)\n",
        "    tf(self.train, self.test, self.valid)\n",
        "  def training(self):\n",
        "    pass\n",
        "  def testing(self):\n",
        "    pass\n",
        "  def evaluation(self):\n",
        "    pass\n",
        "    # evaluation(test, y_predict)\n",
        "tfidf = TF_IDF()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Dropout, Activation, Embedding, Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "# # Load data\n",
        "# df = pd.read_csv(\"ner_data.csv\", encoding=\"ISO-8859-1\", error_bad_lines=False)\n",
        "# df = df.fillna(method=\"ffill\")\n",
        "# sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "# tags = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
        "\n",
        "# # Perform TF-IDF\n",
        "# vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
        "# X_tfidf = vectorizer.fit_transform([\" \".join(sent) for sent in sentences])\n",
        "# tfidf_vocab = vectorizer.vocabulary_\n",
        "# tfidf_vocab_inv = {v:k for k,v in tfidf_vocab.items()}\n",
        "# tfidf_weights = np.asarray(X_tfidf.mean(axis=0)).ravel()\n",
        "\n",
        "# # Tokenize words\n",
        "# MAX_NB_WORDS = 20000\n",
        "# MAX_SEQ_LENGTH = 100\n",
        "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "# tokenizer.fit_on_texts(sentences)\n",
        "# word_index = tokenizer.word_index\n",
        "\n",
        "# # Convert words to sequences\n",
        "# X = tokenizer.texts_to_sequences(sentences)\n",
        "# X = pad_sequences(X, maxlen=MAX_SEQ_LENGTH)\n",
        "\n",
        "# # Convert tags to sequences\n",
        "# tags_index = {\"O\": 0, \"B-LOC\": 1, \"I-LOC\": 2, \"B-PER\": 3, \"I-PER\": 4, \"B-ORG\": 5, \"I-ORG\": 6}\n",
        "# y = [[tags_index[tag] for tag in sent] for sent in tags]\n",
        "# y = pad_sequences(y, maxlen=MAX_SEQ_LENGTH)\n",
        "\n",
        "# # Split data into train and test sets\n",
        "# VALIDATION_SPLIT = 0.2\n",
        "# nb_validation_samples = int(VALIDATION_SPLIT * len(X))\n",
        "# X_train = X[:-nb_validation_samples]\n",
        "# y_train = y[:-nb_validation_samples]\n",
        "# X_test = X[-nb_validation_samples:]\n",
        "# y_test = y[-nb_validation_samples:]\n",
        "\n",
        "# # Define CNN model\n",
        "# EMBEDDING_DIM = 100\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQ_LENGTH))\n",
        "# model.add(Conv1D(128, 5, activation=\"relu\"))\n",
        "# model.add(GlobalMaxPooling1D())\n",
        "# model.add(Dense(7, activation=\"softmax\"))\n",
        "# model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# # Train model\n",
        "# model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)\n",
        "\n",
        "# # Predict tags for new sentences\n",
        "# def predict_tags(sentences):\n",
        "#     X = tokenizer.texts_to_sequences(sentences)\n",
        "#     X = pad_sequences(X, maxlen=MAX_SEQ_LENGTH)\n",
        "#     y_pred = model.predict(X)\n",
        "#     return [[tfidf_vocab_inv[np.argmax(tfidf_weights * y)] if np.max(tfidf_weights * y) > 0.2 else \"O\" for y in sent] for sent in y_pred]\n",
        "\n",
        "# # Test predictions\n",
        "# sentences_test = [\"John lives in New York City.\", \"Steve Jobs was the founder of Apple.\"]\n",
        "# tags_pred = predict_tags(sentences_test)\n",
        "# print(tags_pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
