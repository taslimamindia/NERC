{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "# response = urllib.request.urlopen(url)\n",
    "# data = response.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 4, 6, 7, 2], [3, 4, 5, 8, 2], [9, 5, 2], [10, 11, 2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras import Sequential\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a corpus of text data\n",
    "corpus = ['this is a sample sentence', \n",
    "          'this is another example sentence', \n",
    "          'yet another sentence', \n",
    "          'one more sentence']\n",
    "\n",
    "# Define the parameters for the Tokenizer\n",
    "num_words = 10000  # Maximum number of words to keep based on word frequency\n",
    "oov_token = \"<OOV>\"  # Token to use for out-of-vocabulary words\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Define the parameters for the Word2Vec model\n",
    "embedding_dim = 100  # Number of dimensions of the embeddings\n",
    "max_length = 10  # Maximum length of a sentence\n",
    "trunc_type = 'post'  # Truncation strategy for sentences longer than max_length\n",
    "padding_type = 'post'  # Padding strategy for sentences shorter than max_length\n",
    "\n",
    "# Convert the corpus to sequences of indices\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, truncating=trunc_type, padding=padding_type)\n",
    "print(sequences)\n",
    "\n",
    "# # Define the Word2Vec model\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(num_words, embedding_dim, input_length=max_length))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the Word2Vec model\n",
    "# model.fit(padded_sequences, np.array([1, 1, 0, 0]), epochs=50)\n",
    "\n",
    "# # Get the word vector for a given word\n",
    "# word = 'example'\n",
    "# word_index = tokenizer.word_index.get(word, num_words+1)\n",
    "# vector = model.layers[0].get_weights()[0][word_index]\n",
    "# print(f\"Word vector for '{word}': {vector}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "president\n",
      "biden\n",
      "gave\n",
      "speech\n",
      "economy\n",
      "today\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Charger le texte\n",
    "text = \"President Biden gave a speech on the economy today.\"\n",
    "\n",
    "# Tokenizer le texte\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Supprimer les stop words\n",
    "tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "# Lematizer les tokens\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Vectorizer les tokens avec TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(tokens)\n",
    "\n",
    "# Extraire les entités nommées à partir des vecteurs pondérés\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        if X[i, j] > 0.2: # ajuster ce seuil pour extraire plus ou moins d'entités nommées\n",
    "            print(feature_names[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Charger le texte\n",
    "text = \"President Biden gave a speech on the economy today.\"\n",
    "\n",
    "# Tokenizer et nettoyer le texte\n",
    "sentences = sent_tokenize(text)\n",
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sentences = [[word for word in sentence if word.lower() not in stop_words] for sentence in sentences]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentences = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in sentences]\n",
    "\n",
    "# Entraîner un modèle CBOW avec gensim\n",
    "model = Word2Vec(sentences, min_count=1, vector_size=100, workers=3, window=5, sg=0)\n",
    "model.wv[\"President\"].shape\n",
    "# model.wv.most_similar('President')\n",
    "# Extraire les entités nommées à partir du modèle\n",
    "# for word in model.wv.vocab:\n",
    "#     if len(word) > 1 and word.isalpha():\n",
    "#         if model.wv.similarity('president', word) > 0.8 or model.wv.similarity('obama', word) > 0.8:\n",
    "#             print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'conll2003' from 'nltk.corpus' (c:\\Python39\\lib\\site-packages\\nltk\\corpus\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32me:\\PFE\\CoNLL2003\\word2vec.ipynb Cell 5\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/word2vec.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Word2Vec\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/word2vec.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m word_tokenize\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/PFE/CoNLL2003/word2vec.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m conll2003\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'conll2003' from 'nltk.corpus' (c:\\Python39\\lib\\site-packages\\nltk\\corpus\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Embedding, Conv1D, Dense, Dropout, TimeDistributed, Bidirectional, LSTM\n",
    "from keras_contrib.layers import CRF\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import conll2003\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
